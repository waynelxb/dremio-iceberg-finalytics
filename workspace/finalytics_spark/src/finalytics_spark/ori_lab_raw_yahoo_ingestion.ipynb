{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "639ab335-35ed-4d00-9ce9-5d708b21672e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'get_raw_yahooquery' from 'lab_raw_yahoo' (lab_raw_yahoo.ipynb)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlab_spark\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_spark_session\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlab_schema_manager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SchemaManager\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlab_raw_yahoo\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_raw_yahooquery, get_raw_yfinance\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m defaultdict\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'get_raw_yahooquery' from 'lab_raw_yahoo' (lab_raw_yahoo.ipynb)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import yaml\n",
    "import nbimporter\n",
    "from datetime import datetime, date\n",
    "import time\n",
    "import random\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType,  DateType, TimestampType\n",
    "from pyspark.sql.functions import to_date, to_timestamp\n",
    "from lab_database_manager import PgDBManager\n",
    "from lab_spark import create_spark_session\n",
    "from lab_schema_manager import SchemaManager\n",
    "from lab_raw_yahoo import get_raw_yahooquery, get_raw_yfinance\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d1248e-f920-4fe2-85b6-110c03fd84d5",
   "metadata": {},
   "source": [
    "# Create Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "320989cb-efd1-46ef-89b3-745acc98183f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.postgresql#postgresql added as a dependency\n",
      "org.apache.iceberg#iceberg-spark-runtime-3.5_2.12 added as a dependency\n",
      "org.projectnessie.nessie-integrations#nessie-spark-extensions-3.5_2.12 added as a dependency\n",
      "software.amazon.awssdk#bundle added as a dependency\n",
      "software.amazon.awssdk#url-connection-client added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-140d698d-f949-428d-951c-d3b417af3241;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.postgresql#postgresql;42.7.3 in central\n",
      "\tfound org.checkerframework#checker-qual;3.42.0 in central\n",
      "\tfound org.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.5.0 in central\n",
      "\tfound org.projectnessie.nessie-integrations#nessie-spark-extensions-3.5_2.12;0.77.1 in central\n",
      "\tfound software.amazon.awssdk#bundle;2.24.8 in central\n",
      "\tfound software.amazon.awssdk#url-connection-client;2.24.8 in central\n",
      "\tfound software.amazon.awssdk#utils;2.24.8 in central\n",
      "\tfound org.reactivestreams#reactive-streams;1.0.4 in central\n",
      "\tfound software.amazon.awssdk#annotations;2.24.8 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      "\tfound software.amazon.awssdk#http-client-spi;2.24.8 in central\n",
      "\tfound software.amazon.awssdk#metrics-spi;2.24.8 in central\n",
      ":: resolution report :: resolve 353ms :: artifacts dl 16ms\n",
      "\t:: modules in use:\n",
      "\torg.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.5.0 from central in [default]\n",
      "\torg.checkerframework#checker-qual;3.42.0 from central in [default]\n",
      "\torg.postgresql#postgresql;42.7.3 from central in [default]\n",
      "\torg.projectnessie.nessie-integrations#nessie-spark-extensions-3.5_2.12;0.77.1 from central in [default]\n",
      "\torg.reactivestreams#reactive-streams;1.0.4 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\tsoftware.amazon.awssdk#annotations;2.24.8 from central in [default]\n",
      "\tsoftware.amazon.awssdk#bundle;2.24.8 from central in [default]\n",
      "\tsoftware.amazon.awssdk#http-client-spi;2.24.8 from central in [default]\n",
      "\tsoftware.amazon.awssdk#metrics-spi;2.24.8 from central in [default]\n",
      "\tsoftware.amazon.awssdk#url-connection-client;2.24.8 from central in [default]\n",
      "\tsoftware.amazon.awssdk#utils;2.24.8 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   12  |   0   |   0   |   0   ||   12  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-140d698d-f949-428d-951c-d3b417af3241\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 12 already retrieved (0kB/8ms)\n",
      "25/01/10 00:32:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/01/10 00:32:12 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/01/10 00:32:12 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Spark Session\n",
    "connection_config_file=\"cfg_connections.yaml\"\n",
    "spark_app_name=\"raw_yfinance\"\n",
    "spark=create_spark_session(connection_config_file, spark_app_name)\n",
    "\n",
    "# Set logging level to ERROR to reduce verbosity\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "# Optional: You can also adjust Python logging for third-party libraries\n",
    "logging.getLogger(\"py4j\").setLevel(logging.ERROR)\n",
    "spark.sql(\"CREATE NAMESPACEcentral IF NOT EXISTS nessie.raw;\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53196d95-5457-4ce4-a0a4-af774cad7b7b",
   "metadata": {},
   "source": [
    "# Function: Load Iceberg Table Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cae414a-d7b5-41d3-af8f-861d61f07964",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_into_iceberg_table(schema_config_file, spark_source_df, iceberg_sink_table):\n",
    "    try: \n",
    "        schema_manager=SchemaManager(schema_config_file)\n",
    "        schema_struct_type=schema_manager.get_struct_type(\"tables\", iceberg_sink_table)  \n",
    "        \n",
    "        create_table_script = schema_manager.get_create_table_query(\"tables\", iceberg_sink_table)\n",
    "        spark.sql(create_table_script)\n",
    "     \n",
    "        spark_source_df.writeTo(iceberg_sink_table).append()\n",
    "        # source_spark_df.write.mode(\"overwrite\").saveAsTable(iceberg_sink_table) \n",
    "\n",
    "        incremental_count=spark_source_df.count()\n",
    "        total_count=spark.table(iceberg_sink_table).count()\n",
    "\n",
    "        print(f\"{iceberg_sink_table} was loaded with {incremental_count} records, totally {total_count} records.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading lceberg raw table: {e}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28104eba-f0d5-4cc6-a5a5-5a8615cf2359",
   "metadata": {},
   "source": [
    "# Function: Insert Data into PG Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22168854-440f-47aa-9be7-f0c13adef044",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_iceberg_data_into_pg(conn_config_file, iceberg_source_table, pg_database, pg_sink_table, is_pg_truncate_enabled, is_pg_merge_enabled):   \n",
    "    try:    \n",
    "        df_source=spark.read.table(iceberg_source_table)          \n",
    "\n",
    "        pg_db_mgr=PgDBManager(conn_config_file, pg_database)\n",
    "        pg_url=pg_db_mgr.jdbc_url\n",
    "        pg_driver=pg_db_mgr.driver\n",
    "\n",
    "        if is_pg_truncate_enabled == True:\n",
    "            pg_truncate_script=f\"TRUNCATE TABLE {pg_sink_table}\"\n",
    "            pg_db_mgr.execute_sql_script(pg_truncate_script)\n",
    "        \n",
    "        # Write DataFrame to PostgreSQL\n",
    "        df_source.write.jdbc(url=pg_url, table=pg_sink_table, mode=\"append\", properties={\"driver\": pg_driver}) \n",
    "\n",
    "        if is_pg_merge_enabled == True:\n",
    "            pg_merge_script = \"call fin.usp_load_stock_eod();\"\n",
    "            pg_db_mgr.execute_sql_script(pg_merge_script)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading pg finalytics: {e}\")       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc5b821-f483-4e5d-9766-becd93dd3dfb",
   "metadata": {},
   "source": [
    "# Truncate Iceberg Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f190b4f3-4dfd-491d-8a45-ec17875a98cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\n",
      "SLF4J: Defaulting to no-operation (NOP) logger implementation\n",
      "SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iceberg table nessie.raw.stock_eod_yahooquery truncated successfully.\n"
     ]
    }
   ],
   "source": [
    "# Get iceberg table config info\n",
    "schema_config_file='cfg_schemas.yaml'\n",
    "iceberg_raw_stock_eod_table='nessie.raw.stock_eod_yahooquery'\n",
    "\n",
    "# Check if the Iceberg table exists and truncate it if it does\n",
    "if spark.catalog.tableExists(iceberg_raw_stock_eod_table):\n",
    "    spark.sql(f\"TRUNCATE TABLE {iceberg_raw_stock_eod_table}\")\n",
    "    print(f\"Iceberg table {iceberg_raw_stock_eod_table} truncated successfully.\")\n",
    "else:\n",
    "    print(f\"Iceberg table {iceberg_raw_stock_eod_table} does not exist.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1f1bda-a63e-4410-92b8-b3a8be099844",
   "metadata": {},
   "source": [
    "# Get Symbol Group Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "123067ab-f994-4de4-9f63-6cce3afca0df",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'defaultdict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m query_result\u001b[38;5;241m=\u001b[39mpg_db_mgr\u001b[38;5;241m.\u001b[39mget_sql_script_result_list(query)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Initialize a defaultdict to store the symbols for each (group_date, group_id)\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m grouped_symbols \u001b[38;5;241m=\u001b[39m \u001b[43mdefaultdict\u001b[49m(\u001b[38;5;28mlist\u001b[39m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Iterate over the data to group symbols by (group_date, group_id)\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m group_id, group_start_date, symbol \u001b[38;5;129;01min\u001b[39;00m query_result:\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;66;03m# Use a tuple of (group_date, group_id) as the key and append the symbol to the list\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'defaultdict' is not defined"
     ]
    }
   ],
   "source": [
    "# Get finalytics connetion info\n",
    "conn_config_file='cfg_connections.yaml'\n",
    "pg_db=\"finalytics\"\n",
    "pg_db_mgr=PgDBManager(conn_config_file, pg_db)\n",
    "# pg_url=pg_db_mgr.jdbc_url\n",
    "# pg_driver=pg_db_mgr.driver\n",
    "import_time = datetime.now()\n",
    "\n",
    "\n",
    "# Get symbol_start_date_pairs from finalytics\n",
    "query=\"SELECT group_id, group_start_date, symbol from fin.vw_etl_stock_eod_start_date_grouped  WHERE group_start_date <'2025-1-9' Limit 5;\"\n",
    "query_result=pg_db_mgr.get_sql_script_result_list(query)\n",
    "\n",
    "# Initialize a defaultdict to store the symbols for each (group_date, group_id)\n",
    "grouped_symbols = defaultdict(list)\n",
    "\n",
    "# Iterate over the data to group symbols by (group_date, group_id)\n",
    "for group_id, group_start_date, symbol in query_result:\n",
    "    # Use a tuple of (group_date, group_id) as the key and append the symbol to the list\n",
    "    grouped_symbols[(group_id, group_start_date)].append(symbol)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd33642-894e-486b-81a3-5461477d837c",
   "metadata": {},
   "source": [
    "# Loop Groups and Insert Data into Iceberg Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "299981f7-8e88-4070-a0d4-0690f3cb97c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['GTBP', 'ACLS', 'ISDR', 'TDY', 'MMT']\n",
      "<class 'str'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/finalytics_spark/.venv/lib/python3.10/site-packages/yahooquery/utils/__init__.py:1470: FutureWarning: 'S' is deprecated and will be removed in a future version. Please use 's' instead of 'S'.\n",
      "  has_live_indice = index_utc[-1] >= last_trade - pd.Timedelta(2, \"S\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   open   high    low  close  volume  adjclose  dividends\n",
      "symbol date                                                              \n",
      "GTBP   2024-11-15  3.05  3.200  2.910  2.920   12300     2.920        0.0\n",
      "       2024-11-18  2.90  3.046  2.728  2.950   56700     2.950        0.0\n",
      "       2024-11-19  2.95  3.136  2.760  3.136   16700     3.136        0.0\n",
      "       2024-11-20  3.19  3.200  2.930  3.190   15600     3.190        0.0\n",
      "       2024-11-21  3.19  3.190  2.970  3.010    5800     3.010        0.0\n",
      "...                 ...    ...    ...    ...     ...       ...        ...\n",
      "MMT    2025-01-02  4.68  4.720  4.650  4.660   47200     4.660        0.0\n",
      "       2025-01-03  4.66  4.700  4.660  4.680   47500     4.680        0.0\n",
      "       2025-01-06  4.67  4.700  4.670  4.670   84400     4.670        0.0\n",
      "       2025-01-07  4.67  4.670  4.630  4.630   79600     4.630        0.0\n",
      "       2025-01-08  4.62  4.690  4.620  4.640   78600     4.640        0.0\n",
      "\n",
      "[180 rows x 7 columns]\n",
      "    symbol        date  open   high    low  close  volume  adjclose  dividends\n",
      "0     GTBP  2024-11-15  3.05  3.200  2.910  2.920   12300     2.920        0.0\n",
      "1     GTBP  2024-11-18  2.90  3.046  2.728  2.950   56700     2.950        0.0\n",
      "2     GTBP  2024-11-19  2.95  3.136  2.760  3.136   16700     3.136        0.0\n",
      "3     GTBP  2024-11-20  3.19  3.200  2.930  3.190   15600     3.190        0.0\n",
      "4     GTBP  2024-11-21  3.19  3.190  2.970  3.010    5800     3.010        0.0\n",
      "..     ...         ...   ...    ...    ...    ...     ...       ...        ...\n",
      "175    MMT  2025-01-02  4.68  4.720  4.650  4.660   47200     4.660        0.0\n",
      "176    MMT  2025-01-03  4.66  4.700  4.660  4.680   47500     4.680        0.0\n",
      "177    MMT  2025-01-06  4.67  4.700  4.670  4.670   84400     4.670        0.0\n",
      "178    MMT  2025-01-07  4.67  4.670  4.630  4.630   79600     4.630        0.0\n",
      "179    MMT  2025-01-08  4.62  4.690  4.620  4.640   78600     4.640        0.0\n",
      "\n",
      "[180 rows x 9 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/finalytics_spark/.venv/lib/python3.10/site-packages/yahooquery/utils/__init__.py:1470: FutureWarning: 'S' is deprecated and will be removed in a future version. Please use 's' instead of 'S'.\n",
      "  has_live_indice = index_utc[-1] >= last_trade - pd.Timedelta(2, \"S\")\n",
      "/workspace/finalytics_spark/.venv/lib/python3.10/site-packages/yahooquery/utils/__init__.py:1470: FutureWarning: 'S' is deprecated and will be removed in a future version. Please use 's' instead of 'S'.\n",
      "  has_live_indice = index_utc[-1] >= last_trade - pd.Timedelta(2, \"S\")\n",
      "/workspace/finalytics_spark/.venv/lib/python3.10/site-packages/yahooquery/utils/__init__.py:1470: FutureWarning: 'S' is deprecated and will be removed in a future version. Please use 's' instead of 'S'.\n",
      "  has_live_indice = index_utc[-1] >= last_trade - pd.Timedelta(2, \"S\")\n",
      "/workspace/finalytics_spark/.venv/lib/python3.10/site-packages/yahooquery/utils/__init__.py:1470: FutureWarning: 'S' is deprecated and will be removed in a future version. Please use 's' instead of 'S'.\n",
      "  has_live_indice = index_utc[-1] >= last_trade - pd.Timedelta(2, \"S\")\n",
      "/workspace/finalytics_spark/.venv/lib/python3.10/site-packages/yahooquery/ticker.py:1333: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[\"dividends\"].fillna(0, inplace=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>symbol</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>import_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-11-15</td>\n",
       "      <td>GTBP</td>\n",
       "      <td>3.05</td>\n",
       "      <td>3.200</td>\n",
       "      <td>2.910</td>\n",
       "      <td>2.920</td>\n",
       "      <td>12300</td>\n",
       "      <td>2025-01-10 00:44:22.652197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-11-18</td>\n",
       "      <td>GTBP</td>\n",
       "      <td>2.90</td>\n",
       "      <td>3.046</td>\n",
       "      <td>2.728</td>\n",
       "      <td>2.950</td>\n",
       "      <td>56700</td>\n",
       "      <td>2025-01-10 00:44:22.652197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-11-19</td>\n",
       "      <td>GTBP</td>\n",
       "      <td>2.95</td>\n",
       "      <td>3.136</td>\n",
       "      <td>2.760</td>\n",
       "      <td>3.136</td>\n",
       "      <td>16700</td>\n",
       "      <td>2025-01-10 00:44:22.652197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-11-20</td>\n",
       "      <td>GTBP</td>\n",
       "      <td>3.19</td>\n",
       "      <td>3.200</td>\n",
       "      <td>2.930</td>\n",
       "      <td>3.190</td>\n",
       "      <td>15600</td>\n",
       "      <td>2025-01-10 00:44:22.652197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-11-21</td>\n",
       "      <td>GTBP</td>\n",
       "      <td>3.19</td>\n",
       "      <td>3.190</td>\n",
       "      <td>2.970</td>\n",
       "      <td>3.010</td>\n",
       "      <td>5800</td>\n",
       "      <td>2025-01-10 00:44:22.652197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>2025-01-02</td>\n",
       "      <td>MMT</td>\n",
       "      <td>4.68</td>\n",
       "      <td>4.720</td>\n",
       "      <td>4.650</td>\n",
       "      <td>4.660</td>\n",
       "      <td>47200</td>\n",
       "      <td>2025-01-10 00:44:22.652197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>2025-01-03</td>\n",
       "      <td>MMT</td>\n",
       "      <td>4.66</td>\n",
       "      <td>4.700</td>\n",
       "      <td>4.660</td>\n",
       "      <td>4.680</td>\n",
       "      <td>47500</td>\n",
       "      <td>2025-01-10 00:44:22.652197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>2025-01-06</td>\n",
       "      <td>MMT</td>\n",
       "      <td>4.67</td>\n",
       "      <td>4.700</td>\n",
       "      <td>4.670</td>\n",
       "      <td>4.670</td>\n",
       "      <td>84400</td>\n",
       "      <td>2025-01-10 00:44:22.652197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>2025-01-07</td>\n",
       "      <td>MMT</td>\n",
       "      <td>4.67</td>\n",
       "      <td>4.670</td>\n",
       "      <td>4.630</td>\n",
       "      <td>4.630</td>\n",
       "      <td>79600</td>\n",
       "      <td>2025-01-10 00:44:22.652197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>2025-01-08</td>\n",
       "      <td>MMT</td>\n",
       "      <td>4.62</td>\n",
       "      <td>4.690</td>\n",
       "      <td>4.620</td>\n",
       "      <td>4.640</td>\n",
       "      <td>78600</td>\n",
       "      <td>2025-01-10 00:44:22.652197</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>180 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          date symbol  open   high    low  close  volume  \\\n",
       "0   2024-11-15   GTBP  3.05  3.200  2.910  2.920   12300   \n",
       "1   2024-11-18   GTBP  2.90  3.046  2.728  2.950   56700   \n",
       "2   2024-11-19   GTBP  2.95  3.136  2.760  3.136   16700   \n",
       "3   2024-11-20   GTBP  3.19  3.200  2.930  3.190   15600   \n",
       "4   2024-11-21   GTBP  3.19  3.190  2.970  3.010    5800   \n",
       "..         ...    ...   ...    ...    ...    ...     ...   \n",
       "175 2025-01-02    MMT  4.68  4.720  4.650  4.660   47200   \n",
       "176 2025-01-03    MMT  4.66  4.700  4.660  4.680   47500   \n",
       "177 2025-01-06    MMT  4.67  4.700  4.670  4.670   84400   \n",
       "178 2025-01-07    MMT  4.67  4.670  4.630  4.630   79600   \n",
       "179 2025-01-08    MMT  4.62  4.690  4.620  4.640   78600   \n",
       "\n",
       "                   import_time  \n",
       "0   2025-01-10 00:44:22.652197  \n",
       "1   2025-01-10 00:44:22.652197  \n",
       "2   2025-01-10 00:44:22.652197  \n",
       "3   2025-01-10 00:44:22.652197  \n",
       "4   2025-01-10 00:44:22.652197  \n",
       "..                         ...  \n",
       "175 2025-01-10 00:44:22.652197  \n",
       "176 2025-01-10 00:44:22.652197  \n",
       "177 2025-01-10 00:44:22.652197  \n",
       "178 2025-01-10 00:44:22.652197  \n",
       "179 2025-01-10 00:44:22.652197  \n",
       "\n",
       "[180 rows x 8 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "symbol_list=['GTBP', 'ACLS', 'ISDR', 'TDY', 'MMT']\n",
    "get_raw_yahooquery(symbol_list, '2024-11-15', datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47f50bb6-b8ab-401c-b268-5567c62e71b0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'grouped_symbols' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"yahooquery\")\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Display the results\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m group, group_symbols \u001b[38;5;129;01min\u001b[39;00m \u001b[43mgrouped_symbols\u001b[49m\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m      5\u001b[0m     group_id, group_start_date \u001b[38;5;241m=\u001b[39m group\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGroup Date: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgroup_start_date\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Group Number: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgroup_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Symbols: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgroup_symbols\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'grouped_symbols' is not defined"
     ]
    }
   ],
   "source": [
    "# warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"yahooquery\")\n",
    "\n",
    "# Display the results\n",
    "for group, group_symbols in grouped_symbols.items():\n",
    "    group_id, group_start_date = group\n",
    "    print(f\"Group Date: {group_start_date}, Group Number: {group_id}, Symbols: {group_symbols}\")\n",
    "    # hist_data=get_raw_yfinance(group_symbols, group_start_date, import_time)\n",
    "    hist_data=get_raw_yahooquery(group_symbols, group_start_date, import_time)\n",
    "    print(hist_data)\n",
    "    # hist_df = spark.createDataFrame(hist_data)\n",
    "    \n",
    "    # insert_into_iceberg_table(schema_config_file, hist_df, iceberg_raw_stock_eod_table)\n",
    "    time.sleep(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1227020-d4a1-48d5-a5f4-0575f61ba26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hist_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a558a80-e6c2-497c-875e-69cc8a6921a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pg_table='stage.stock_eod_quote_yahoo'\n",
    "is_pg_truncate_enabled=True\n",
    "is_pg_merge_enabled=True\n",
    "insert_iceberg_data_into_pg(conn_config_file, iceberg_raw_stock_eod_table, pg_db, pg_table, is_pg_truncate_enabled, is_pg_merge_enabled)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5edf5c-d96d-4299-8e32-5556cfb93c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('select * from nessie.raw.stock_eod_yahooquery').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70235ec4-8a46-45be-ba7f-3bad304cdc3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Poetry",
   "language": "python",
   "name": "poetry-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
