{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0197c5b6-752d-4719-b7f7-fc60afea8ba5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import nbimporter\n",
    "from zz_table_manager import TableManager\n",
    "from datetime import datetime, date\n",
    "# import pyspark\n",
    "# from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType, DateType, FloatType, TimestampType\n",
    "# from pyspark.sql.functions import to_date, to_timestamp\n",
    "# from lab_table_manager import TableManager\n",
    "import yfinance as yf\n",
    "# from lab_finalytics_database import FinalyticsDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f507395-a75e-4217-b142-1c989471b0e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import yaml\n",
    "# import pyspark\n",
    "# from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType, DateType, FloatType, TimestampType\n",
    "\n",
    "# class TableManager:\n",
    "#     def __init__(self, config_file):\n",
    "#         \"\"\"\n",
    "#         Initialize the TableManager with the path to the YAML configuration file.\n",
    "#         \"\"\"\n",
    "#         self.config_file = config_file\n",
    "#         self.config = self._load_config()\n",
    "\n",
    "#     def _load_config(self):\n",
    "#         \"\"\"\n",
    "#         Load the YAML configuration file.\n",
    "#         \"\"\"\n",
    "#         with open(self.config_file, \"r\") as file:\n",
    "#             return yaml.safe_load(file)\n",
    "\n",
    "#     def get_schema(self, table_name):\n",
    "#         \"\"\"\n",
    "#         Generate a PySpark schema for a given table name from the configuration.\n",
    "        \n",
    "#         :param table_name: Name of the table in the configuration.\n",
    "#         :return: A PySpark StructType schema.\n",
    "#         \"\"\"\n",
    "#         if table_name not in self.config[\"tables\"]:\n",
    "#             raise ValueError(f\"Table '{table_name}' not found in the configuration.\")\n",
    "\n",
    "#         table_config = self.config[\"tables\"][table_name]\n",
    "#         schema = StructType([\n",
    "#             StructField(field[\"name\"], eval(field[\"type\"])(), field[\"nullable\"])\n",
    "#             for field in table_config[\"schema\"]\n",
    "#         ])\n",
    "#         return schema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d18047a-528f-4978-b3d5-5240b4536c0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fetch_stock_data(symbol, start_date, end_date):\n",
    "    stock = yf.Ticker(symbol)\n",
    "    hist = stock.history(start=start_date, end=end_date)\n",
    "    hist.reset_index(inplace=True)\n",
    "\n",
    "    # Rename and transform columns\n",
    "    hist = hist[[\"Date\", \"Open\"]].rename(columns={\"Date\": \"date\", \"Open\": \"open\"})\n",
    "\n",
    "    # Strip timezone from the 'date' column\n",
    "    hist[\"date\"] = hist[\"date\"].dt.date  # Convert to naive date (no timezone)\n",
    "    \n",
    "    hist[\"symbol\"] = symbol\n",
    "    hist[\"import_time\"] = datetime.now()\n",
    "    return hist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd24e262-3e52-4baf-8ce8-398cbbec9889",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/12/25 01:20:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/12/25 01:20:45 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType([StructField('date', DateType(), False), StructField('open', FloatType(), True), StructField('symbol', StringType(), False), StructField('import_time', TimestampType(), False)])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+------+--------------------+\n",
      "|      date|     open|symbol|         import_time|\n",
      "+----------+---------+------+--------------------+\n",
      "|2023-01-03|128.92424|  AAPL|2024-12-25 01:20:...|\n",
      "|2023-01-04|125.56951|  AAPL|2024-12-25 01:20:...|\n",
      "|2023-01-05|125.80702|  AAPL|2024-12-25 01:20:...|\n",
      "|2023-01-06|124.69868|  AAPL|2024-12-25 01:20:...|\n",
      "|2023-01-09|129.11227|  AAPL|2024-12-25 01:20:...|\n",
      "|2023-01-10|128.90446|  AAPL|2024-12-25 01:20:...|\n",
      "|2023-01-11|129.88416|  AAPL|2024-12-25 01:20:...|\n",
      "|2023-01-12|132.48679|  AAPL|2024-12-25 01:20:...|\n",
      "|2023-01-13|130.65602|  AAPL|2024-12-25 01:20:...|\n",
      "|2023-01-17| 133.4269|  AAPL|2024-12-25 01:20:...|\n",
      "|2023-01-18| 135.3962|  AAPL|2024-12-25 01:20:...|\n",
      "|2023-01-19|132.68472|  AAPL|2024-12-25 01:20:...|\n",
      "|2023-01-20|133.87221|  AAPL|2024-12-25 01:20:...|\n",
      "|2023-01-23|136.68265|  AAPL|2024-12-25 01:20:...|\n",
      "|2023-01-24|138.84985|  AAPL|2024-12-25 01:20:...|\n",
      "|2023-01-25|139.42384|  AAPL|2024-12-25 01:20:...|\n",
      "|2023-01-26|141.68011|  AAPL|2024-12-25 01:20:...|\n",
      "|2023-01-27|141.67023|  AAPL|2024-12-25 01:20:...|\n",
      "|2023-01-30|143.45149|  AAPL|2024-12-25 01:20:...|\n",
      "|2023-01-31|  141.215|  AAPL|2024-12-25 01:20:...|\n",
      "+----------+---------+------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"YFinance Data Loader\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Initialize TableManager with the config file path\n",
    "config_file = \"table_config.yaml\"\n",
    "table_manager = TableManager(config_file)\n",
    "\n",
    "# Get schema for a specific table\n",
    "table_name = \"nessie.raw.stock_eod_yfinance\"\n",
    "schema = table_manager.get_schema(table_name)\n",
    "\n",
    "print(schema)\n",
    "\n",
    "symbol = \"AAPL\"\n",
    "start_date = \"2023-01-01\"\n",
    "end_date = \"2023-12-31\"\n",
    "stock_data = fetch_stock_data(symbol, start_date, end_date)\n",
    "\n",
    "stock_df = spark.createDataFrame(stock_data, schema=schema)\n",
    "\n",
    "stock_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2d80b2-7169-4fe0-809f-23cfd844659d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cce35195-8622-40f7-96f2-19c7fc617bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Date        Open        High         Low       Close  Volume  \\\n",
      "0  2024-12-23  551.150024  551.150024  551.150024  551.150024       0   \n",
      "1  2024-12-24  557.229980  557.229980  557.229980  557.229980       0   \n",
      "\n",
      "   Dividends  Stock Splits  Capital Gains  \n",
      "0        0.0           0.0            0.0  \n",
      "1        0.0           0.0            0.0  \n"
     ]
    }
   ],
   "source": [
    "symbol='VFIAX'\n",
    "quote = yf.Ticker(symbol)\n",
    "current_date = date.today()\n",
    "hist = quote.history(start='2024-12-21', end=current_date)\n",
    "\n",
    "# Reset index to include Date as a column and format it\n",
    "hist.reset_index(inplace=True)\n",
    "hist[\"Date\"] = hist[\"Date\"].dt.date\n",
    "\n",
    "print(hist)\n",
    "\n",
    "# limit and stablize the fields of hist\n",
    "hist = hist[['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Dividends', 'Stock Splits']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c41e8201-2d7a-48fa-a492-350890613c16",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Date        Open        High         Low       Close   Volume  \\\n",
      "0  2024-12-23  173.639999  177.570007  170.779999  177.110001  7031100   \n",
      "1  2024-12-24  178.070007  182.750000  177.570007  182.639999  2183200   \n",
      "\n",
      "   Dividends  Stock Splits  \n",
      "0      0.321           0.0  \n",
      "1      0.000           0.0  \n"
     ]
    }
   ],
   "source": [
    "print(hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08c3603-b760-4e0b-b1b3-d20277d6fa59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Poetry",
   "language": "python",
   "name": "poetry-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
