{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc71f1ce-97ce-4e88-a374-747acfc540fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import nbimporter\n",
    "from datetime import datetime, date\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType\n",
    "from raw_yfinance_ingestion import RawYFIngestion  \n",
    "from lab_table_manager import TableManager\n",
    "import yfinance as yf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8e29c97-3bf7-4c3f-8608-65c8a39b910f",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'config_connections.yaml'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconfig_connections.yaml\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m      2\u001b[0m     config\u001b[38;5;241m=\u001b[39myaml\u001b[38;5;241m.\u001b[39msafe_load(file)\n\u001b[1;32m      3\u001b[0m     catalog_uri \u001b[38;5;241m=\u001b[39m config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdocker_env\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcatalog_uri\u001b[39m\u001b[38;5;124m'\u001b[39m] \n",
      "File \u001b[0;32m/workspace/finalytics_spark/.venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'config_connections.yaml'"
     ]
    }
   ],
   "source": [
    "with open(\"config_connections.yaml\",\"r\") as file:\n",
    "    config=yaml.safe_load(file)\n",
    "    catalog_uri = config['docker_env']['catalog_uri'] \n",
    "    warehouse = config['docker_env']['warehouse']     # Minio Address to Write to\n",
    "    storage_uri = config['docker_env']['storage_uri'] # Minio IP address from docker inspec\n",
    "\n",
    "# Configure Spark with necessary packages and Iceberg/Nessie settings\n",
    "conf = (\n",
    "    pyspark.SparkConf()\n",
    "        .setAppName('finalytics_app')\n",
    "        # Include necessary packages\n",
    "        .set('spark.jars.packages',\n",
    "             'org.postgresql:postgresql:42.7.3,'\n",
    "             'org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.0,'\n",
    "             'org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.12:0.77.1,'\n",
    "             'software.amazon.awssdk:bundle:2.24.8,'\n",
    "             'software.amazon.awssdk:url-connection-client:2.24.8')\n",
    "        # Enable Iceberg and Nessie extensions\n",
    "        .set('spark.sql.extensions', \n",
    "             'org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,'\n",
    "             'org.projectnessie.spark.extensions.NessieSparkSessionExtensions')\n",
    "        # Configure Nessie catalog\n",
    "        .set('spark.sql.catalog.nessie', 'org.apache.iceberg.spark.SparkCatalog')\n",
    "        .set('spark.sql.catalog.nessie.uri', catalog_uri)\n",
    "        .set('spark.sql.catalog.nessie.ref', 'main')\n",
    "        .set('spark.sql.catalog.nessie.authentication.type', 'NONE')\n",
    "        .set('spark.sql.catalog.nessie.catalog-impl', 'org.apache.iceberg.nessie.NessieCatalog')\n",
    "        # Set Minio as the S3 endpoint for Iceberg storage\n",
    "        .set('spark.sql.catalog.nessie.s3.endpoint', storage_uri)\n",
    "        .set('spark.sql.catalog.nessie.warehouse', warehouse)\n",
    "        .set('spark.sql.catalog.nessie.io-impl', 'org.apache.iceberg.aws.s3.S3FileIO')       \n",
    ")   \n",
    "\n",
    "# Start Spark session\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()  \n",
    "# Create the \"sales\" namespace\n",
    "spark.sql(\"CREATE NAMESPACE IF NOT EXISTS nessie.raw;\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43f1668e-6c02-40a4-8d8b-4c1e522420fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fetch_yfinance_record(multi_param_pairs):\n",
    "    try:\n",
    "        symbol, start_date = multi_param_pairs\n",
    "        # Fetch stock data using yfinance\n",
    "        quote = yf.Ticker(symbol)\n",
    "        current_date = date.today()\n",
    "        hist = quote.history(start=start_date, end=current_date)\n",
    "\n",
    "        # Reset index to include Date as a column and format it\n",
    "        hist.reset_index(inplace=True)\n",
    "        hist['Date'] = hist['Date'].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "        # Add symbol and import_time to each row\n",
    "        record_list = [\n",
    "            tuple(row) + (symbol, import_time) for row in hist.itertuples(index=False)\n",
    "        ]\n",
    "\n",
    "        return record_list\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data for {symbol}: {e}\")\n",
    "        return []  # Return an empty list on error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7a8628-9f57-478a-9ca5-5e500fceb7a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afab4b13-8598-414f-8e80-14ec3828ed3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to process the records (pass through parameters)\n",
    "def process_yfinance_record(single_param_pair):\n",
    "    # print(f\"Processing {single_param_pair}\")\n",
    "    return fetch_yfinance_record(single_param_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc24f51b-ca74-4232-ae7c-eda5bafa83ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Parallel fetch function\n",
    "def parallel_fetch(multi_param_pairs, column_list):\n",
    "    # Create RDD from the input parameter pairs\n",
    "    record_rdd = spark.sparkContext.parallelize(multi_param_pairs)\n",
    "\n",
    "    # Use flatMap to return a flattened list of records\n",
    "    results_rdd = record_rdd.flatMap(process_yfinance_record)\n",
    "\n",
    "    # Collect the results from the RDD and convert to a list of tuples\n",
    "    # results = results_rdd.collect()        \n",
    "    df = spark.createDataFrame(results_rdd, column_list)   \n",
    "    \n",
    "    return df\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "78b02da3-a584-4dbd-b44b-ccd3d5bbed1a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------+------+------+------+---------+---------+------------+------+--------------------+\n",
      "|               date|  open|  high|   low| close|   volume|dividends|stock_splits|symbol|         import_time|\n",
      "+-------------------+------+------+------+------+---------+---------+------------+------+--------------------+\n",
      "|2024-12-10 00:00:00|246.89|248.21|245.34|247.77| 36914800|      0.0|         0.0|  AAPL|2024-12-24 15:23:...|\n",
      "|2024-12-11 00:00:00|247.96| 250.8|246.26|246.49| 45205800|      0.0|         0.0|  AAPL|2024-12-24 15:23:...|\n",
      "|2024-12-12 00:00:00|246.89|248.74|245.68|247.96| 32777500|      0.0|         0.0|  AAPL|2024-12-24 15:23:...|\n",
      "|2024-12-13 00:00:00|247.82|249.29|246.24|248.13| 33155300|      0.0|         0.0|  AAPL|2024-12-24 15:23:...|\n",
      "|2024-12-16 00:00:00|247.99|251.38|247.65|251.04| 51694800|      0.0|         0.0|  AAPL|2024-12-24 15:23:...|\n",
      "|2024-12-17 00:00:00|250.08|253.83|249.78|253.48| 51356400|      0.0|         0.0|  AAPL|2024-12-24 15:23:...|\n",
      "|2024-12-18 00:00:00|252.16|254.28|247.74|248.05| 56774100|      0.0|         0.0|  AAPL|2024-12-24 15:23:...|\n",
      "|2024-12-19 00:00:00| 247.5| 252.0|247.09|249.79| 60882300|      0.0|         0.0|  AAPL|2024-12-24 15:23:...|\n",
      "|2024-12-20 00:00:00|248.04| 255.0|245.69|254.49|147495300|      0.0|         0.0|  AAPL|2024-12-24 15:23:...|\n",
      "|2024-12-23 00:00:00|254.77|255.65|253.45|255.27| 40828600|      0.0|         0.0|  AAPL|2024-12-24 15:23:...|\n",
      "|2024-12-10 00:00:00|444.39|449.62| 441.6|443.33| 18469500|      0.0|         0.0|  MSFT|2024-12-24 15:23:...|\n",
      "|2024-12-11 00:00:00|444.05|450.35|444.05|448.99| 19200200|      0.0|         0.0|  MSFT|2024-12-24 15:23:...|\n",
      "|2024-12-12 00:00:00|449.11|456.16|449.11|449.56| 20834800|      0.0|         0.0|  MSFT|2024-12-24 15:23:...|\n",
      "|2024-12-13 00:00:00|448.44|451.43|445.58|447.27| 20177800|      0.0|         0.0|  MSFT|2024-12-24 15:23:...|\n",
      "|2024-12-16 00:00:00|447.27|452.18|445.28|451.59| 23598800|      0.0|         0.0|  MSFT|2024-12-24 15:23:...|\n",
      "|2024-12-17 00:00:00|451.01|455.29|449.57|454.46| 22733500|      0.0|         0.0|  MSFT|2024-12-24 15:23:...|\n",
      "|2024-12-18 00:00:00|451.32|452.65|437.02|437.39| 24444500|      0.0|         0.0|  MSFT|2024-12-24 15:23:...|\n",
      "|2024-12-19 00:00:00|441.62|443.18|436.32|437.03| 22963700|      0.0|         0.0|  MSFT|2024-12-24 15:23:...|\n",
      "|2024-12-20 00:00:00|433.11|443.74|428.63| 436.6| 64263700|      0.0|         0.0|  MSFT|2024-12-24 15:23:...|\n",
      "|2024-12-23 00:00:00|436.74|437.65|432.83|435.25| 19127000|      0.0|         0.0|  MSFT|2024-12-24 15:23:...|\n",
      "+-------------------+------+------+------+------+---------+---------+------------+------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# List of stock symbols and start dates\n",
    "yf_param_pairs = [\n",
    "    ('AAPL', '2024-12-10'),\n",
    "    ('MSFT', '2024-12-10'),\n",
    "    ('GOOGL', '2024-12-10'),\n",
    "]\n",
    "\n",
    "import_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S.\") + str(datetime.now().microsecond)[:3]\n",
    "zone='raw'\n",
    "sink_table='nessie.raw.stock_eod_yfinance'\n",
    "config_file_path='cfg_registered_table_schemas.yaml'\n",
    "\n",
    "table_manager=TableManager(config_file_path)\n",
    "registered_column_list = table_manager.get_column_list(sink_table)\n",
    "create_table_query = table_manager.get_create_table_query(sink_table)\n",
    "\n",
    "# Fetch data in parallel\n",
    "df_raw_eod_yfinance = parallel_fetch(yf_param_pairs, registered_column_list)\n",
    "# df_raw_eod_yfinance.show()\n",
    "\n",
    "# print(create_table_query)\n",
    "spark.sql(create_table_query)\n",
    "\n",
    "\n",
    "# df_raw_eod_yfinance.writeTo(sink_table).append()\n",
    "spark.sql(f\"select * from {sink_table}\").show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242a2f86-e3f0-40d6-9670-5b6fa2cd6280",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Poetry",
   "language": "python",
   "name": "poetry-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
