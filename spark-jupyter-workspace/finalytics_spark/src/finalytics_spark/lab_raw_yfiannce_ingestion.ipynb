{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc71f1ce-97ce-4e88-a374-747acfc540fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import nbimporter\n",
    "from datetime import datetime, date\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType,  DateType, TimestampType\n",
    "from pyspark.sql.functions import to_date, to_timestamp\n",
    "from lab_table_manager import TableManager\n",
    "import yfinance as yf\n",
    "from lab_finalytics_database import FinalyticsDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8e29c97-3bf7-4c3f-8608-65c8a39b910f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.postgresql#postgresql added as a dependency\n",
      "org.apache.iceberg#iceberg-spark-runtime-3.5_2.12 added as a dependency\n",
      "org.projectnessie.nessie-integrations#nessie-spark-extensions-3.5_2.12 added as a dependency\n",
      "software.amazon.awssdk#bundle added as a dependency\n",
      "software.amazon.awssdk#url-connection-client added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-dcf8406f-a94f-4b16-954f-2ba9b4e66692;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.postgresql#postgresql;42.7.3 in central\n",
      "\tfound org.checkerframework#checker-qual;3.42.0 in central\n",
      "\tfound org.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.5.0 in central\n",
      "\tfound org.projectnessie.nessie-integrations#nessie-spark-extensions-3.5_2.12;0.77.1 in central\n",
      "\tfound software.amazon.awssdk#bundle;2.24.8 in central\n",
      "\tfound software.amazon.awssdk#url-connection-client;2.24.8 in central\n",
      "\tfound software.amazon.awssdk#utils;2.24.8 in central\n",
      "\tfound org.reactivestreams#reactive-streams;1.0.4 in central\n",
      "\tfound software.amazon.awssdk#annotations;2.24.8 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      "\tfound software.amazon.awssdk#http-client-spi;2.24.8 in central\n",
      "\tfound software.amazon.awssdk#metrics-spi;2.24.8 in central\n",
      ":: resolution report :: resolve 454ms :: artifacts dl 24ms\n",
      "\t:: modules in use:\n",
      "\torg.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.5.0 from central in [default]\n",
      "\torg.checkerframework#checker-qual;3.42.0 from central in [default]\n",
      "\torg.postgresql#postgresql;42.7.3 from central in [default]\n",
      "\torg.projectnessie.nessie-integrations#nessie-spark-extensions-3.5_2.12;0.77.1 from central in [default]\n",
      "\torg.reactivestreams#reactive-streams;1.0.4 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\tsoftware.amazon.awssdk#annotations;2.24.8 from central in [default]\n",
      "\tsoftware.amazon.awssdk#bundle;2.24.8 from central in [default]\n",
      "\tsoftware.amazon.awssdk#http-client-spi;2.24.8 from central in [default]\n",
      "\tsoftware.amazon.awssdk#metrics-spi;2.24.8 from central in [default]\n",
      "\tsoftware.amazon.awssdk#url-connection-client;2.24.8 from central in [default]\n",
      "\tsoftware.amazon.awssdk#utils;2.24.8 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   12  |   0   |   0   |   0   ||   12  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-dcf8406f-a94f-4b16-954f-2ba9b4e66692\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 12 already retrieved (0kB/13ms)\n",
      "24/12/25 04:00:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"cfg_connections.yaml\",\"r\") as file:\n",
    "    config=yaml.safe_load(file)\n",
    "    catalog_uri = config['docker_env']['catalog_uri'] \n",
    "    warehouse = config['docker_env']['warehouse']     # Minio Address to Write to\n",
    "    storage_uri = config['docker_env']['storage_uri'] # Minio IP address from docker inspec\n",
    "\n",
    "# Configure Spark with necessary packages and Iceberg/Nessie settings\n",
    "conf = (\n",
    "    pyspark.SparkConf()\n",
    "        .setAppName('finalytics_app')\n",
    "        # Include necessary packages\n",
    "        .set('spark.jars.packages',\n",
    "             'org.postgresql:postgresql:42.7.3,'\n",
    "             'org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.0,'\n",
    "             'org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.12:0.77.1,'\n",
    "             'software.amazon.awssdk:bundle:2.24.8,'\n",
    "             'software.amazon.awssdk:url-connection-client:2.24.8')\n",
    "        # Enable Iceberg and Nessie extensions\n",
    "        .set('spark.sql.extensions', \n",
    "             'org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,'\n",
    "             'org.projectnessie.spark.extensions.NessieSparkSessionExtensions')\n",
    "        # Configure Nessie catalog\n",
    "        .set('spark.sql.catalog.nessie', 'org.apache.iceberg.spark.SparkCatalog')\n",
    "        .set('spark.sql.catalog.nessie.uri', catalog_uri)\n",
    "        .set('spark.sql.catalog.nessie.ref', 'main')\n",
    "        .set('spark.sql.catalog.nessie.authentication.type', 'NONE')\n",
    "        .set('spark.sql.catalog.nessie.catalog-impl', 'org.apache.iceberg.nessie.NessieCatalog')\n",
    "        # Set Minio as the S3 endpoint for Iceberg storage\n",
    "        .set('spark.sql.catalog.nessie.s3.endpoint', storage_uri)\n",
    "        .set('spark.sql.catalog.nessie.warehouse', warehouse)\n",
    "        .set('spark.sql.catalog.nessie.io-impl', 'org.apache.iceberg.aws.s3.S3FileIO')       \n",
    ")   \n",
    "\n",
    "# Start Spark session\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()  \n",
    "# Create the \"sales\" namespace\n",
    "spark.sql(\"CREATE NAMESPACE IF NOT EXISTS nessie.raw;\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43f1668e-6c02-40a4-8d8b-4c1e522420fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fetch_yfinance_record(symbol_date_pairs):\n",
    "    try:\n",
    "        symbol, start_date = symbol_date_pairs\n",
    "        # Fetch stock data using yfinance\n",
    "        quote = yf.Ticker(symbol)\n",
    "        current_date = date.today()\n",
    "        hist = quote.history(start=start_date, end=current_date)\n",
    "\n",
    "        # Reset index to include Date as a column and format it\n",
    "        hist.reset_index(inplace=True)\n",
    "        # hist['Date'] = hist['Date'].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        # hist['Date'] = hist['Date'].dt.strftime('%Y-%m-%d').date()\n",
    "        hist[\"Date\"] = hist[\"Date\"].dt.date \n",
    "        \n",
    "        # Add symbol and import_time to each row\n",
    "        record_list = [\n",
    "            tuple(row) + (symbol, import_time) for row in hist.itertuples(index=False)\n",
    "        ]\n",
    "        \n",
    "        # print(record_list)\n",
    "        return record_list\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data for {symbol}: {e}\")\n",
    "        return []  # Return an empty list on error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa7a8628-9f57-478a-9ca5-5e500fceb7a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def parallel_fetch_yfinance_record(symbol_date_pairs, record_schema):    \n",
    "    # Distribute (symbol, start_date) pairs across Spark workers\n",
    "    record_rdd = spark.sparkContext.parallelize(symbol_date_pairs)\n",
    "    \n",
    "    # Fetch data in parallel\n",
    "    mapped_record_rdd = record_rdd.flatMap(fetch_yfinance_record)\n",
    "\n",
    "    # Convert RDD to DataFrame\n",
    "    result_df = spark.createDataFrame(mapped_record_rdd, schema=record_schema)\n",
    "\n",
    "    # Show or save the results\n",
    "    # result_df.show()\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78b02da3-a584-4dbd-b44b-ccd3d5bbed1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_raw_eod_yfinance(symbol_date_pairs, sink_table, schema_config_file):\n",
    "    table_manager=TableManager(schema_config_file)\n",
    "    regd_struct_type=table_manager.get_struct_type(sink_table)   \n",
    "    \n",
    "    # regd_column_list = table_manager.get_column_list(sink_table)\n",
    "    create_table_script = table_manager.get_create_table_query(sink_table)\n",
    "    spark.sql(create_table_script)\n",
    "    \n",
    "    df_raw_eod_yfinance=parallel_fetch_yfinance_record(symbol_date_pairs, regd_struct_type)\n",
    "    # df_raw_eod_yfinance = df_raw_eod_yfinance.withColumn(\"date\", to_date(\"date\", \"yyyy-MM-dd\")).withColumn(\"import_time\", to_timestamp(\"import_time\", \"yyyy-MM-dd HH:mm:ss\"))\n",
    "  \n",
    "    # df_raw_eod_yfinance.writeTo(sink_table).append()\n",
    "    df_raw_eod_yfinance.write.mode(\"overwrite\").saveAsTable(sink_table)    \n",
    "    # df_raw_eod_yfinance.writeTo(sink_table).overwritePartitions()\n",
    "    \n",
    "    print(f\"{sink_table} has been loaded\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "87e4e507-3cca-4314-a3c6-c5c6206fe818",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not get exchangeTimezoneName for ticker 'KSM' reason: 'chart' (2 + 2) / 4]\n",
      "$KSM: possibly delisted; no timezone found\n",
      "Error fetching data for KSM: Can only use .dt accessor with datetimelike values\n",
      "[Stage 28:===========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nessie.raw.stock_eod_yfinance has been loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "  \n",
    "# symbol_start_date_pairs = [\n",
    "#     ('AAPL', '2024-12-10'),\n",
    "#     ('MSFT', '2024-12-10'),\n",
    "#     ('GOOGL', '2024-12-10'),\n",
    "# ]\n",
    "\n",
    "conn_config_file='cfg_connections.yaml'\n",
    "finalytics=FinalyticsDB(conn_config_file)\n",
    "\n",
    "query=\"select symbol, start_date from fin.vw_etl_stock_eod_start_date limit 30\"\n",
    "symbol_start_date_pairs=finalytics.get_symbol_start_date_pairs(query)\n",
    "finalytics_url=finalytics.jdbc_url\n",
    "finalytics_driver=finalytics.driver\n",
    "\n",
    "regd_schema_config_file='cfg_registered_table_schemas.yaml'\n",
    "sink_table='nessie.raw.stock_eod_yfinance'\n",
    "\n",
    "import_time = datetime.now().isoformat()\n",
    "load_raw_eod_yfinance(symbol_start_date_pairs, sink_table, regd_schema_config_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c3904a34-d834-4d28-87b6-21f2b271fbb3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|      17|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select count(*) from nessie.raw.stock_eod_yfinance').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7ff6a1d9-c5fa-4697-9b7b-b257048855eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df=spark.read.table(sink_table)          \n",
    "pg_table = \"stage.stock_eod_quote_yahoo\"  # Replace with the PostgreSQL table name\n",
    "\n",
    "query = f\"truncate table {pg_table};\"\n",
    "finalytics.execute_sql_script(query)\n",
    "\n",
    "# # Write Delta table DataFrame to PostgreSQL\n",
    "df.write.jdbc(url=finalytics_url, table=pg_table, mode=\"append\", properties={\"driver\": finalytics_driver})\n",
    "\n",
    "query = \"call fin.usp_load_stock_eod();\"\n",
    "finalytics.execute_sql_script(query)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "06376922-1939-4c55-bde2-0be33707d47a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|         import_time|\n",
      "+--------------------+\n",
      "|2024-12-25T04:04:...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select import_time from nessie.raw.stock_eod_yfinance order by import_time desc limit 1').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ca139d-8d89-4754-a48d-90010853b91d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Poetry",
   "language": "python",
   "name": "poetry-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
