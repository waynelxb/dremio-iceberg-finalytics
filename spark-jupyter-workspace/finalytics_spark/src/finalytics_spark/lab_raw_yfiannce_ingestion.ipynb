{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc71f1ce-97ce-4e88-a374-747acfc540fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import nbimporter\n",
    "from datetime import datetime, date\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType\n",
    "from lab_table_manager import TableManager\n",
    "import yfinance as yf\n",
    "from lab_finalytics_database import FinalyticsDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8e29c97-3bf7-4c3f-8608-65c8a39b910f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.postgresql#postgresql added as a dependency\n",
      "org.apache.iceberg#iceberg-spark-runtime-3.5_2.12 added as a dependency\n",
      "org.projectnessie.nessie-integrations#nessie-spark-extensions-3.5_2.12 added as a dependency\n",
      "software.amazon.awssdk#bundle added as a dependency\n",
      "software.amazon.awssdk#url-connection-client added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-6ccdc086-005d-4dc6-a975-40c1fc44776d;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.postgresql#postgresql;42.7.3 in central\n",
      "\tfound org.checkerframework#checker-qual;3.42.0 in central\n",
      "\tfound org.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.5.0 in central\n",
      "\tfound org.projectnessie.nessie-integrations#nessie-spark-extensions-3.5_2.12;0.77.1 in central\n",
      "\tfound software.amazon.awssdk#bundle;2.24.8 in central\n",
      "\tfound software.amazon.awssdk#url-connection-client;2.24.8 in central\n",
      "\tfound software.amazon.awssdk#utils;2.24.8 in central\n",
      "\tfound org.reactivestreams#reactive-streams;1.0.4 in central\n",
      "\tfound software.amazon.awssdk#annotations;2.24.8 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      "\tfound software.amazon.awssdk#http-client-spi;2.24.8 in central\n",
      "\tfound software.amazon.awssdk#metrics-spi;2.24.8 in central\n",
      ":: resolution report :: resolve 394ms :: artifacts dl 19ms\n",
      "\t:: modules in use:\n",
      "\torg.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.5.0 from central in [default]\n",
      "\torg.checkerframework#checker-qual;3.42.0 from central in [default]\n",
      "\torg.postgresql#postgresql;42.7.3 from central in [default]\n",
      "\torg.projectnessie.nessie-integrations#nessie-spark-extensions-3.5_2.12;0.77.1 from central in [default]\n",
      "\torg.reactivestreams#reactive-streams;1.0.4 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\tsoftware.amazon.awssdk#annotations;2.24.8 from central in [default]\n",
      "\tsoftware.amazon.awssdk#bundle;2.24.8 from central in [default]\n",
      "\tsoftware.amazon.awssdk#http-client-spi;2.24.8 from central in [default]\n",
      "\tsoftware.amazon.awssdk#metrics-spi;2.24.8 from central in [default]\n",
      "\tsoftware.amazon.awssdk#url-connection-client;2.24.8 from central in [default]\n",
      "\tsoftware.amazon.awssdk#utils;2.24.8 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   12  |   0   |   0   |   0   ||   12  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-6ccdc086-005d-4dc6-a975-40c1fc44776d\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 12 already retrieved (0kB/11ms)\n",
      "24/12/24 17:40:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"cfg_connections.yaml\",\"r\") as file:\n",
    "    config=yaml.safe_load(file)\n",
    "    catalog_uri = config['docker_env']['catalog_uri'] \n",
    "    warehouse = config['docker_env']['warehouse']     # Minio Address to Write to\n",
    "    storage_uri = config['docker_env']['storage_uri'] # Minio IP address from docker inspec\n",
    "\n",
    "# Configure Spark with necessary packages and Iceberg/Nessie settings\n",
    "conf = (\n",
    "    pyspark.SparkConf()\n",
    "        .setAppName('finalytics_app')\n",
    "        # Include necessary packages\n",
    "        .set('spark.jars.packages',\n",
    "             'org.postgresql:postgresql:42.7.3,'\n",
    "             'org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.0,'\n",
    "             'org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.12:0.77.1,'\n",
    "             'software.amazon.awssdk:bundle:2.24.8,'\n",
    "             'software.amazon.awssdk:url-connection-client:2.24.8')\n",
    "        # Enable Iceberg and Nessie extensions\n",
    "        .set('spark.sql.extensions', \n",
    "             'org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,'\n",
    "             'org.projectnessie.spark.extensions.NessieSparkSessionExtensions')\n",
    "        # Configure Nessie catalog\n",
    "        .set('spark.sql.catalog.nessie', 'org.apache.iceberg.spark.SparkCatalog')\n",
    "        .set('spark.sql.catalog.nessie.uri', catalog_uri)\n",
    "        .set('spark.sql.catalog.nessie.ref', 'main')\n",
    "        .set('spark.sql.catalog.nessie.authentication.type', 'NONE')\n",
    "        .set('spark.sql.catalog.nessie.catalog-impl', 'org.apache.iceberg.nessie.NessieCatalog')\n",
    "        # Set Minio as the S3 endpoint for Iceberg storage\n",
    "        .set('spark.sql.catalog.nessie.s3.endpoint', storage_uri)\n",
    "        .set('spark.sql.catalog.nessie.warehouse', warehouse)\n",
    "        .set('spark.sql.catalog.nessie.io-impl', 'org.apache.iceberg.aws.s3.S3FileIO')       \n",
    ")   \n",
    "\n",
    "# Start Spark session\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()  \n",
    "# Create the \"sales\" namespace\n",
    "spark.sql(\"CREATE NAMESPACE IF NOT EXISTS nessie.raw;\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43f1668e-6c02-40a4-8d8b-4c1e522420fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fetch_yfinance_record(symbol_date_pairs):\n",
    "    try:\n",
    "        symbol, start_date = symbol_date_pairs\n",
    "        # Fetch stock data using yfinance\n",
    "        quote = yf.Ticker(symbol)\n",
    "        current_date = date.today()\n",
    "        hist = quote.history(start=start_date, end=current_date)\n",
    "\n",
    "        # Reset index to include Date as a column and format it\n",
    "        hist.reset_index(inplace=True)\n",
    "        hist['Date'] = hist['Date'].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        \n",
    "        # Add symbol and import_time to each row\n",
    "        record_list = [\n",
    "            tuple(row) + (symbol, import_time) for row in hist.itertuples(index=False)\n",
    "        ]\n",
    "        \n",
    "\n",
    "        return record_list\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data for {symbol}: {e}\")\n",
    "        return []  # Return an empty list on error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa7a8628-9f57-478a-9ca5-5e500fceb7a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def parallel_fetch_yfinance_record(symbol_date_pairs, record_schema):    \n",
    "    # Distribute (symbol, start_date) pairs across Spark workers\n",
    "    record_rdd = spark.sparkContext.parallelize(symbol_date_pairs)\n",
    "    \n",
    "    # Fetch data in parallel\n",
    "    mapped_record_rdd = record_rdd.flatMap(fetch_yfinance_record)\n",
    "\n",
    "    # Convert RDD to DataFrame\n",
    "    result_df = spark.createDataFrame(mapped_record_rdd, schema=record_schema)\n",
    "\n",
    "    # Show or save the results\n",
    "    # result_df.show()\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78b02da3-a584-4dbd-b44b-ccd3d5bbed1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_raw_eod_yfinance(symbol_date_pairs, sink_table, schema_config_file):\n",
    "    table_manager=TableManager(schema_config_file)\n",
    "    regd_struct_type=table_manager.get_struct_type(sink_table)   \n",
    "    # regd_column_list = table_manager.get_column_list(sink_table)\n",
    "    create_table_script = table_manager.get_create_table_query(sink_table)\n",
    "    \n",
    "    df_raw_eod_yfinance=parallel_fetch_yfinance_record(symbol_date_pairs, regd_struct_type)\n",
    "    spark.sql(create_table_script)\n",
    "    # df_raw_eod_yfinance.writeTo(sink_table).append()\n",
    "    # df_raw_eod_yfinance.write.mode(\"overwrite\").saveAsTable(sink_table)    \n",
    "    df_raw_eod_yfinance.writeTo(sink_table).overwritePartitions()\n",
    "    print(f\"{sink_table} has been loaded\")\n",
    "\n",
    "         \n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e4e507-3cca-4314-a3c6-c5c6206fe818",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\n",
      "SLF4J: Defaulting to no-operation (NOP) logger implementation\n",
      "SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n",
      "$NSTG: possibly delisted; no price data found  (1d 2024-11-14 -> 2024-12-24)/ 4]\n",
      "Error fetching data for NSTG: Can only use .dt accessor with datetimelike values\n",
      "$AGBA: possibly delisted; no timezone found\n",
      "Error fetching data for AGBA: Can only use .dt accessor with datetimelike values\n",
      "[Stage 0:>                                                          (0 + 4) / 4]\r",
      "$APTO: possibly delisted; no price data found  (1d 2024-12-13 -> 2024-12-24)\n",
      "Error fetching data for APTO: Can only use .dt accessor with datetimelike values\n",
      "Failed to get ticker 'DAL' reason: Expecting value: line 1 column 1 (char 0)\n",
      "$DAL: possibly delisted; no timezone found\n",
      "Error fetching data for DAL: Can only use .dt accessor with datetimelike values\n",
      "$OM: possibly delisted; no price data found  (1d 2024-12-13 -> 2024-12-24)\n",
      "Error fetching data for OM: Can only use .dt accessor with datetimelike values\n",
      "Failed to get ticker 'IMAQ' reason: Expecting value: line 1 column 1 (char 0)\n",
      "$IMAQ: possibly delisted; no timezone found\n",
      "Error fetching data for IMAQ: Can only use .dt accessor with datetimelike values\n",
      "Failed to get ticker 'APTV' reason: Expecting value: line 1 column 1 (char 0)\n",
      "$APTV: possibly delisted; no timezone found\n",
      "Error fetching data for APTV: Can only use .dt accessor with datetimelike values\n",
      "Failed to get ticker 'IMAQU' reason: Expecting value: line 1 column 1 (char 0)\n",
      "$IMAQU: possibly delisted; no timezone found\n",
      "Error fetching data for IMAQU: Can only use .dt accessor with datetimelike values\n",
      "Failed to get ticker 'OMAB' reason: Expecting value: line 1 column 1 (char 0)\n",
      "$OMAB: possibly delisted; no timezone found\n",
      "Error fetching data for OMAB: Can only use .dt accessor with datetimelike values\n",
      "Failed to get ticker 'DALN' reason: Expecting value: line 1 column 1 (char 0)\n",
      "$DALN: possibly delisted; no timezone found\n",
      "Error fetching data for DALN: Can only use .dt accessor with datetimelike values\n",
      "Failed to get ticker 'APVO' reason: Expecting value: line 1 column 1 (char 0)\n",
      "$APVO: possibly delisted; no timezone found\n",
      "Error fetching data for APVO: Can only use .dt accessor with datetimelike values\n",
      "Failed to get ticker 'OMC' reason: Expecting value: line 1 column 1 (char 0)\n",
      "$OMC: possibly delisted; no timezone found\n",
      "Error fetching data for OMC: Can only use .dt accessor with datetimelike values\n",
      "Failed to get ticker 'IMAX' reason: Expecting value: line 1 column 1 (char 0)\n",
      "$IMAX: possibly delisted; no timezone found\n",
      "Error fetching data for IMAX: Can only use .dt accessor with datetimelike values\n",
      "Failed to get ticker 'DAN' reason: Expecting value: line 1 column 1 (char 0)\n",
      "$DAN: possibly delisted; no timezone found\n",
      "Error fetching data for DAN: Can only use .dt accessor with datetimelike values\n",
      "Failed to get ticker 'APWC' reason: Expecting value: line 1 column 1 (char 0)\n",
      "$APWC: possibly delisted; no timezone found\n",
      "Error fetching data for APWC: Can only use .dt accessor with datetimelike values\n",
      "Failed to get ticker 'OMCL' reason: Expecting value: line 1 column 1 (char 0)\n",
      "$OMCL: possibly delisted; no timezone found\n",
      "Error fetching data for OMCL: Can only use .dt accessor with datetimelike values\n",
      "Failed to get ticker 'DAO' reason: Expecting value: line 1 column 1 (char 0)\n",
      "$DAO: possibly delisted; no timezone found\n",
      "Error fetching data for DAO: Can only use .dt accessor with datetimelike values\n",
      "Failed to get ticker 'IMCC' reason: Expecting value: line 1 column 1 (char 0)\n",
      "$IMCC: possibly delisted; no timezone found\n",
      "Error fetching data for IMCC: Can only use .dt accessor with datetimelike values\n",
      "Failed to get ticker 'APXIU' reason: Expecting value: line 1 column 1 (char 0)\n",
      "$APXIU: possibly delisted; no timezone found\n",
      "Error fetching data for APXIU: Can only use .dt accessor with datetimelike values\n",
      "Failed to get ticker 'OMER' reason: Expecting value: line 1 column 1 (char 0)\n",
      "$OMER: possibly delisted; no timezone found\n",
      "Error fetching data for OMER: Can only use .dt accessor with datetimelike values\n",
      "Failed to get ticker 'DAR' reason: Expecting value: line 1 column 1 (char 0)\n",
      "$DAR: possibly delisted; no timezone found\n",
      "Error fetching data for DAR: Can only use .dt accessor with datetimelike values\n",
      "Failed to get ticker 'IMCR' reason: Expecting value: line 1 column 1 (char 0)\n",
      "$IMCR: possibly delisted; no timezone found\n",
      "Error fetching data for IMCR: Can only use .dt accessor with datetimelike values\n",
      "Failed to get ticker 'APYX' reason: Expecting value: line 1 column 1 (char 0)\n",
      "$APYX: possibly delisted; no timezone found\n",
      "Error fetching data for APYX: Can only use .dt accessor with datetimelike values\n",
      "Failed to get ticker 'DARE' reason: Expecting value: line 1 column 1 (char 0)\n",
      "$DARE: possibly delisted; no timezone found\n",
      "Error fetching data for DARE: Can only use .dt accessor with datetimelike values\n",
      "Failed to get ticker 'IMKTA' reason: Expecting value: line 1 column 1 (char 0)\n",
      "$IMKTA: possibly delisted; no timezone found\n",
      "Error fetching data for IMKTA: Can only use .dt accessor with datetimelike values\n",
      "Failed to get ticker 'OMEX' reason: Expecting value: line 1 column 1 (char 0)\n",
      "$OMEX: possibly delisted; no timezone found\n",
      "Error fetching data for OMEX: Can only use .dt accessor with datetimelike values\n",
      "Failed to get ticker 'AQB' reason: Expecting value: line 1 column 1 (char 0)\n",
      "$AQB: possibly delisted; no timezone found\n",
      "Error fetching data for AQB: Can only use .dt accessor with datetimelike values\n",
      "Failed to get ticker 'DASH' reason: Expecting value: line 1 column 1 (char 0)\n",
      "$DASH: possibly delisted; no timezone found\n",
      "Error fetching data for DASH: Can only use .dt accessor with datetimelike values\n",
      "Failed to get ticker 'IMMP' reason: Expecting value: line 1 column 1 (char 0)\n",
      "$IMMP: possibly delisted; no timezone found\n",
      "Error fetching data for IMMP: Can only use .dt accessor with datetimelike values\n",
      "Failed to get ticker 'OMF' reason: Expecting value: line 1 column 1 (char 0)\n",
      "$OMF: possibly delisted; no timezone found\n",
      "Error fetching data for OMF: Can only use .dt accessor with datetimelike values\n",
      "Failed to get ticker 'AQMS' reason: Expecting value: line 1 column 1 (char 0)\n",
      "Error fetching data for AQMS: Can only use .dt accessor with datetimelike values\n",
      "$AQMS: possibly delisted; no timezone found\n",
      "Failed to get ticker 'DATS' reason: Expecting value: line 1 column 1 (char 0)\n",
      "$DATS: possibly delisted; no timezone found\n",
      "Error fetching data for DATS: Can only use .dt accessor with datetimelike values\n",
      "Failed to get ticker 'OMGA' reason: Expecting value: line 1 column 1 (char 0)\n",
      "$OMGA: possibly delisted; no timezone found\n",
      "Error fetching data for OMGA: Can only use .dt accessor with datetimelike values\n",
      "Failed to get ticker 'IMMR' reason: Expecting value: line 1 column 1 (char 0)\n",
      "$IMMR: possibly delisted; no timezone found\n",
      "Error fetching data for IMMR: Can only use .dt accessor with datetimelike values\n",
      "Failed to get ticker 'OMI' reason: Expecting value: line 1 column 1 (char 0)\n",
      "$OMI: possibly delisted; no timezone found\n",
      "Error fetching data for OMI: Can only use .dt accessor with datetimelike values\n",
      "Failed to get ticker 'IMMX' reason: Expecting value: line 1 column 1 (char 0)\n",
      "$IMMX: possibly delisted; no timezone found\n",
      "Error fetching data for IMMX: Can only use .dt accessor with datetimelike values\n",
      "Failed to get ticker 'AQN' reason: Expecting value: line 1 column 1 (char 0)\n",
      "$AQN: possibly delisted; no timezone found\n",
      "Error fetching data for AQN: Can only use .dt accessor with datetimelike values\n",
      "Failed to get ticker 'DAVA' reason: Expecting value: line 1 column 1 (char 0)\n",
      "$DAVA: possibly delisted; no timezone found\n",
      "Error fetching data for DAVA: Can only use .dt accessor with datetimelike values\n",
      "Failed to get ticker 'IMNM' reason: Expecting value: line 1 column 1 (char 0)\n",
      "$IMNM: possibly delisted; no timezone found\n",
      "Error fetching data for IMNM: Can only use .dt accessor with datetimelike values\n",
      "Failed to get ticker 'OMIC' reason: Expecting value: line 1 column 1 (char 0)\n",
      "$OMIC: possibly delisted; no timezone found\n",
      "Error fetching data for OMIC: Can only use .dt accessor with datetimelike values\n",
      "Failed to get ticker 'AQST' reason: Expecting value: line 1 column 1 (char 0)\n",
      "$AQST: possibly delisted; no timezone found\n",
      "Error fetching data for AQST: Can only use .dt accessor with datetimelike values\n",
      "Failed to get ticker 'DAWN' reason: Expecting value: line 1 column 1 (char 0)\n",
      "$DAWN: possibly delisted; no timezone found\n",
      "Error fetching data for DAWN: Can only use .dt accessor with datetimelike values\n",
      "Failed to get ticker 'OMQS' reason: Expecting value: line 1 column 1 (char 0)\n",
      "$OMQS: possibly delisted; no timezone found\n",
      "Error fetching data for OMQS: Can only use .dt accessor with datetimelike values\n",
      "Failed to get ticker 'IMO' reason: Expecting value: line 1 column 1 (char 0)\n",
      "$IMO: possibly delisted; no timezone found\n",
      "Error fetching data for IMO: Can only use .dt accessor with datetimelike values\n",
      "Failed to get ticker 'AR' reason: Expecting value: line 1 column 1 (char 0)\n",
      "$AR: possibly delisted; no timezone found\n",
      "Error fetching data for AR: Can only use .dt accessor with datetimelike values\n",
      "Failed to get ticker 'DB' reason: Expecting value: line 1 column 1 (char 0)\n",
      "$DB: possibly delisted; no timezone found\n",
      "Error fetching data for DB: Can only use .dt accessor with datetimelike values\n",
      "Failed to get ticker 'ON' reason: Expecting value: line 1 column 1 (char 0)\n",
      "$ON: possibly delisted; no timezone found\n",
      "Error fetching data for ON: Can only use .dt accessor with datetimelike values\n",
      "Failed to get ticker 'ARAY' reason: Expecting value: line 1 column 1 (char 0)\n",
      "$ARAY: possibly delisted; no timezone found\n",
      "Error fetching data for ARAY: Can only use .dt accessor with datetimelike values\n",
      "Failed to get ticker 'IMOS' reason: Expecting value: line 1 column 1 (char 0)\n",
      "$IMOS: possibly delisted; no timezone found\n",
      "Error fetching data for IMOS: Can only use .dt accessor with datetimelike values\n",
      "Failed to get ticker 'DBD' reason: Expecting value: line 1 column 1 (char 0)\n",
      "$DBD: possibly delisted; no timezone found\n",
      "Error fetching data for DBD: Can only use .dt accessor with datetimelike values\n",
      "Failed to get ticker 'ONB' reason: Expecting value: line 1 column 1 (char 0)\n",
      "$ONB: possibly delisted; no timezone found\n",
      "Error fetching data for ONB: Can only use .dt accessor with datetimelike values\n",
      "Failed to get ticker 'IMPP' reason: Expecting value: line 1 column 1 (char 0)\n",
      "$IMPP: possibly delisted; no timezone found\n",
      "Error fetching data for IMPP: Can only use .dt accessor with datetimelike values\n",
      "Failed to get ticker 'ARBE' reason: Expecting value: line 1 column 1 (char 0)\n",
      "$ARBE: possibly delisted; no timezone found\n",
      "Error fetching data for ARBE: Can only use .dt accessor with datetimelike values\n",
      "Failed to get ticker 'ONCT' reason: Expecting value: line 1 column 1 (char 0)\n",
      "$ONCT: possibly delisted; no timezone found\n",
      "Error fetching data for ONCT: Can only use .dt accessor with datetimelike values\n",
      "Failed to get ticker 'ARBK' reason: Expecting value: line 1 column 1 (char 0)\n",
      "$ARBK: possibly delisted; no timezone found\n",
      "Error fetching data for ARBK: Can only use .dt accessor with datetimelike values\n",
      "Failed to get ticker 'DBGI' reason: Expecting value: line 1 column 1 (char 0)\n",
      "$DBGI: possibly delisted; no timezone found\n",
      "Error fetching data for DBGI: Can only use .dt accessor with datetimelike values\n",
      "Failed to get ticker 'IMRN' reason: Expecting value: line 1 column 1 (char 0)\n",
      "$IMRN: possibly delisted; no timezone found\n",
      "Error fetching data for IMRN: Can only use .dt accessor with datetimelike values\n",
      "Failed to get ticker 'ONCY' reason: Expecting value: line 1 column 1 (char 0)\n",
      "$ONCY: possibly delisted; no timezone found\n",
      "Error fetching data for ONCY: Can only use .dt accessor with datetimelike values\n",
      "Failed to get ticker 'ARC' reason: Expecting value: line 1 column 1 (char 0)\n",
      "$ARC: possibly delisted; no timezone found\n",
      "Error fetching data for ARC: Can only use .dt accessor with datetimelike values\n",
      "Failed to get ticker 'DBI' reason: Expecting value: line 1 column 1 (char 0)\n",
      "$DBI: possibly delisted; no timezone found\n",
      "Error fetching data for DBI: Can only use .dt accessor with datetimelike values\n",
      "Failed to get ticker 'IMRX' reason: Expecting value: line 1 column 1 (char 0)\n",
      "$IMRX: possibly delisted; no timezone found\n",
      "Error fetching data for IMRX: Can only use .dt accessor with datetimelike values\n",
      "Failed to get ticker 'ONDS' reason: Expecting value: line 1 column 1 (char 0)\n",
      "$ONDS: possibly delisted; no timezone found\n",
      "Error fetching data for ONDS: Can only use .dt accessor with datetimelike values\n",
      "Failed to get ticker 'ARCB' reason: Expecting value: line 1 column 1 (char 0)\n",
      "$ARCB: possibly delisted; no timezone found\n",
      "Error fetching data for ARCB: Can only use .dt accessor with datetimelike values\n",
      "Failed to get ticker 'DBL' reason: Expecting value: line 1 column 1 (char 0)\n",
      "$DBL: possibly delisted; no timezone found\n",
      "Error fetching data for DBL: Can only use .dt accessor with datetimelike values\n",
      "Failed to get ticker 'IMTE' reason: Expecting value: line 1 column 1 (char 0)\n",
      "$IMTE: possibly delisted; no timezone found\n",
      "Error fetching data for IMTE: Can only use .dt accessor with datetimelike values\n",
      "Failed to get ticker 'ONEW' reason: Expecting value: line 1 column 1 (char 0)\n",
      "$ONEW: possibly delisted; no timezone found\n",
      "Error fetching data for ONEW: Can only use .dt accessor with datetimelike values\n",
      "Failed to get ticker 'ARCC' reason: Expecting value: line 1 column 1 (char 0)\n",
      "$ARCC: possibly delisted; no timezone found\n",
      "Error fetching data for ARCC: Can only use .dt accessor with datetimelike values\n",
      "Failed to get ticker 'DBRG' reason: Expecting value: line 1 column 1 (char 0)\n",
      "$DBRG: possibly delisted; no timezone found\n",
      "Error fetching data for DBRG: Can only use .dt accessor with datetimelike values\n",
      "Failed to get ticker 'IMTX' reason: Expecting value: line 1 column 1 (char 0)\n",
      "$IMTX: possibly delisted; no timezone found\n",
      "Error fetching data for IMTX: Can only use .dt accessor with datetimelike values\n",
      "Failed to get ticker 'ONL' reason: Expecting value: line 1 column 1 (char 0)\n",
      "$ONL: possibly delisted; no timezone found\n",
      "Error fetching data for ONL: Can only use .dt accessor with datetimelike values\n",
      "Error fetching data for ARCH: Can only use .dt accessor with datetimelike values\n",
      "Failed to get ticker 'ARCH' reason: Expecting value: line 1 column 1 (char 0)\n",
      "$ARCH: possibly delisted; no timezone found\n",
      "Failed to get ticker 'DBVT' reason: Expecting value: line 1 column 1 (char 0)\n",
      "$DBVT: possibly delisted; no timezone found\n",
      "Error fetching data for DBVT: Can only use .dt accessor with datetimelike values\n",
      "Failed to get ticker 'IMUX' reason: Expecting value: line 1 column 1 (char 0)\n",
      "$IMUX: possibly delisted; no timezone found\n",
      "Error fetching data for IMUX: Can only use .dt accessor with datetimelike values\n",
      "Failed to get ticker 'ONON' reason: Expecting value: line 1 column 1 (char 0)\n",
      "$ONON: possibly delisted; no timezone found\n",
      "Error fetching data for ONON: Can only use .dt accessor with datetimelike values\n",
      "Failed to get ticker 'ARCO' reason: Expecting value: line 1 column 1 (char 0)\n",
      "$ARCO: possibly delisted; no timezone found\n",
      "Error fetching data for ARCO: Can only use .dt accessor with datetimelike values\n",
      "Failed to get ticker 'IMVT' reason: Expecting value: line 1 column 1 (char 0)\n",
      "$IMVT: possibly delisted; no timezone found\n",
      "Error fetching data for IMVT: Can only use .dt accessor with datetimelike values\n",
      "Failed to get ticker 'DBX' reason: Expecting value: line 1 column 1 (char 0)\n",
      "$DBX: possibly delisted; no timezone found\n",
      "Error fetching data for DBX: Can only use .dt accessor with datetimelike values\n",
      "Failed to get ticker 'ONTF' reason: Expecting value: line 1 column 1 (char 0)\n",
      "$ONTF: possibly delisted; no timezone found\n",
      "Error fetching data for ONTF: Can only use .dt accessor with datetimelike values\n",
      "Failed to get ticker 'ARCT' reason: Expecting value: line 1 column 1 (char 0)\n",
      "$ARCT: possibly delisted; no timezone found\n",
      "Error fetching data for ARCT: Can only use .dt accessor with datetimelike values\n",
      "Failed to get ticker 'IMXI' reason: Expecting value: line 1 column 1 (char 0)\n",
      "$IMXI: possibly delisted; no timezone found\n",
      "Error fetching data for IMXI: Can only use .dt accessor with datetimelike values\n",
      "Failed to get ticker 'DCBO' reason: Expecting value: line 1 column 1 (char 0)\n",
      "$DCBO: possibly delisted; no timezone found\n",
      "Error fetching data for DCBO: Can only use .dt accessor with datetimelike values\n",
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/workspace/finalytics_spark/.venv/lib/python3.10/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/usr/lib/python3.10/socket.py\", line 705, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/workspace/finalytics_spark/.venv/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/workspace/finalytics_spark/.venv/lib/python3.10/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "KeyboardInterrupt\n",
      "Failed to get ticker 'ONTO' reason: Expecting value: line 1 column 1 (char 0)\n",
      "$ONTO: possibly delisted; no timezone found\n",
      "Error fetching data for ONTO: Can only use .dt accessor with datetimelike values\n",
      "24/12/24 17:42:06 ERROR AppendDataExec: Data source write support IcebergBatchWrite(table=nessie.raw.stock_eod_yfinance, format=PARQUET) is aborting.\n",
      "24/12/24 17:42:06 ERROR AppendDataExec: Data source write support IcebergBatchWrite(table=nessie.raw.stock_eod_yfinance, format=PARQUET) aborted.\n",
      "Failed to get ticker 'ARDC' reason: Expecting value: line 1 column 1 (char 0)\n",
      "$ARDC: possibly delisted; no timezone found\n",
      "Error fetching data for ARDC: Can only use .dt accessor with datetimelike values\n",
      "24/12/24 17:42:06 ERROR Utils: Aborting task\n",
      "org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:612)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:594)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:789)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$1(WriteToDataSourceV2Exec.scala:441)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:486)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:425)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:388)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.EOFException\n",
      "\tat java.base/java.io.DataInputStream.readFully(DataInputStream.java:202)\n",
      "\tat java.base/java.io.DataInputStream.readFully(DataInputStream.java:170)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:777)\n",
      "\t... 26 more\n",
      "24/12/24 17:42:06 ERROR DataWritingSparkTask: Aborting commit for partition 1 (task 1, attempt 0, stage 0.0)\n",
      "24/12/24 17:42:06 ERROR Utils: Aborting task\n",
      "org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:612)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:594)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:789)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$1(WriteToDataSourceV2Exec.scala:441)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:486)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:425)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:388)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.EOFException\n",
      "\tat java.base/java.io.DataInputStream.readFully(DataInputStream.java:202)\n",
      "\tat java.base/java.io.DataInputStream.readFully(DataInputStream.java:170)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:777)\n",
      "\t... 26 more\n",
      "24/12/24 17:42:06 ERROR DataWritingSparkTask: Aborting commit for partition 2 (task 2, attempt 0, stage 0.0)\n",
      "24/12/24 17:42:06 ERROR Utils: Aborting task\n",
      "org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:612)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:594)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:789)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$1(WriteToDataSourceV2Exec.scala:441)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:486)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:425)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:388)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.EOFException\n",
      "\tat java.base/java.io.DataInputStream.readFully(DataInputStream.java:202)\n",
      "\tat java.base/java.io.DataInputStream.readFully(DataInputStream.java:170)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:777)\n",
      "\t... 26 more\n",
      "24/12/24 17:42:06 ERROR DataWritingSparkTask: Aborting commit for partition 0 (task 0, attempt 0, stage 0.0)\n",
      "24/12/24 17:42:06 ERROR Utils: Aborting task\n",
      "org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:612)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:594)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:789)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$1(WriteToDataSourceV2Exec.scala:441)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:486)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:425)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:388)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.EOFException\n",
      "\tat java.base/java.io.DataInputStream.readFully(DataInputStream.java:202)\n",
      "\tat java.base/java.io.DataInputStream.readFully(DataInputStream.java:170)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:777)\n",
      "\t... 26 more\n",
      "24/12/24 17:42:06 ERROR DataWritingSparkTask: Aborting commit for partition 3 (task 3, attempt 0, stage 0.0)\n",
      "24/12/24 17:42:06 WARN Utils: Suppressing exception in catch: Shutdown in progress\n",
      "java.lang.IllegalStateException: Shutdown in progress\n",
      "\tat java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:66)\n",
      "\tat java.base/java.lang.Runtime.addShutdownHook(Runtime.java:216)\n",
      "\tat org.apache.iceberg.relocated.com.google.common.util.concurrent.MoreExecutors$Application.addShutdownHook(MoreExecutors.java:289)\n",
      "\tat org.apache.iceberg.relocated.com.google.common.util.concurrent.MoreExecutors$Application.addDelayedShutdownHook(MoreExecutors.java:266)\n",
      "\tat org.apache.iceberg.relocated.com.google.common.util.concurrent.MoreExecutors$Application.getExitingExecutorService(MoreExecutors.java:241)\n",
      "\tat org.apache.iceberg.relocated.com.google.common.util.concurrent.MoreExecutors$Application.getExitingExecutorService(MoreExecutors.java:246)\n",
      "\tat org.apache.iceberg.relocated.com.google.common.util.concurrent.MoreExecutors.getExitingExecutorService(MoreExecutors.java:129)\n",
      "\tat org.apache.iceberg.aws.s3.S3OutputStream.<init>(S3OutputStream.java:115)\n",
      "\tat org.apache.iceberg.aws.s3.S3OutputFile.createOrOverwrite(S3OutputFile.java:70)\n",
      "\tat org.apache.iceberg.parquet.ParquetIO$ParquetOutputFile.createOrOverwrite(ParquetIO.java:151)\n",
      "\tat org.apache.iceberg.shaded.org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:345)\n",
      "\tat org.apache.iceberg.shaded.org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:323)\n",
      "\tat org.apache.iceberg.parquet.ParquetWriter.ensureWriterInitialized(ParquetWriter.java:111)\n",
      "\tat org.apache.iceberg.parquet.ParquetWriter.flushRowGroup(ParquetWriter.java:210)\n",
      "\tat org.apache.iceberg.parquet.ParquetWriter.close(ParquetWriter.java:254)\n",
      "\tat org.apache.iceberg.io.DataWriter.close(DataWriter.java:82)\n",
      "\tat org.apache.iceberg.io.RollingFileWriter.closeCurrentWriter(RollingFileWriter.java:122)\n",
      "\tat org.apache.iceberg.io.RollingFileWriter.close(RollingFileWriter.java:147)\n",
      "\tat org.apache.iceberg.io.RollingDataWriter.close(RollingDataWriter.java:32)\n",
      "\tat org.apache.iceberg.spark.source.SparkWrite$UnpartitionedDataWriter.close(SparkWrite.java:747)\n",
      "\tat org.apache.iceberg.spark.source.SparkWrite$UnpartitionedDataWriter.abort(SparkWrite.java:739)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$6(WriteToDataSourceV2Exec.scala:482)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1408)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:486)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:425)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:388)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "24/12/24 17:42:06 WARN Utils: Suppressing exception in catch: Shutdown in progress\n",
      "java.lang.IllegalStateException: Shutdown in progress\n",
      "\tat java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:66)\n",
      "\tat java.base/java.lang.Runtime.addShutdownHook(Runtime.java:216)\n",
      "\tat org.apache.iceberg.relocated.com.google.common.util.concurrent.MoreExecutors$Application.addShutdownHook(MoreExecutors.java:289)\n",
      "\tat org.apache.iceberg.relocated.com.google.common.util.concurrent.MoreExecutors$Application.addDelayedShutdownHook(MoreExecutors.java:266)\n",
      "\tat org.apache.iceberg.relocated.com.google.common.util.concurrent.MoreExecutors$Application.getExitingExecutorService(MoreExecutors.java:241)\n",
      "\tat org.apache.iceberg.relocated.com.google.common.util.concurrent.MoreExecutors$Application.getExitingExecutorService(MoreExecutors.java:246)\n",
      "\tat org.apache.iceberg.relocated.com.google.common.util.concurrent.MoreExecutors.getExitingExecutorService(MoreExecutors.java:129)\n",
      "\tat org.apache.iceberg.aws.s3.S3OutputStream.<init>(S3OutputStream.java:115)\n",
      "\tat org.apache.iceberg.aws.s3.S3OutputFile.createOrOverwrite(S3OutputFile.java:70)\n",
      "\tat org.apache.iceberg.parquet.ParquetIO$ParquetOutputFile.createOrOverwrite(ParquetIO.java:151)\n",
      "\tat org.apache.iceberg.shaded.org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:345)\n",
      "\tat org.apache.iceberg.shaded.org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:323)\n",
      "\tat org.apache.iceberg.parquet.ParquetWriter.ensureWriterInitialized(ParquetWriter.java:111)\n",
      "\tat org.apache.iceberg.parquet.ParquetWriter.flushRowGroup(ParquetWriter.java:210)\n",
      "\tat org.apache.iceberg.parquet.ParquetWriter.close(ParquetWriter.java:254)\n",
      "\tat org.apache.iceberg.io.DataWriter.close(DataWriter.java:82)\n",
      "\tat org.apache.iceberg.io.RollingFileWriter.closeCurrentWriter(RollingFileWriter.java:122)\n",
      "\tat org.apache.iceberg.io.RollingFileWriter.close(RollingFileWriter.java:147)\n",
      "\tat org.apache.iceberg.io.RollingDataWriter.close(RollingDataWriter.java:32)\n",
      "\tat org.apache.iceberg.spark.source.SparkWrite$UnpartitionedDataWriter.close(SparkWrite.java:747)\n",
      "\tat org.apache.iceberg.spark.source.SparkWrite$UnpartitionedDataWriter.abort(SparkWrite.java:739)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$6(WriteToDataSourceV2Exec.scala:482)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1408)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:486)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:425)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:388)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "24/12/24 17:42:06 WARN Utils: Suppressing exception in catch: Shutdown in progress\n",
      "java.lang.IllegalStateException: Shutdown in progress\n",
      "\tat java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:66)\n",
      "\tat java.base/java.lang.Runtime.addShutdownHook(Runtime.java:216)\n",
      "\tat org.apache.iceberg.relocated.com.google.common.util.concurrent.MoreExecutors$Application.addShutdownHook(MoreExecutors.java:289)\n",
      "\tat org.apache.iceberg.relocated.com.google.common.util.concurrent.MoreExecutors$Application.addDelayedShutdownHook(MoreExecutors.java:266)\n",
      "\tat org.apache.iceberg.relocated.com.google.common.util.concurrent.MoreExecutors$Application.getExitingExecutorService(MoreExecutors.java:241)\n",
      "\tat org.apache.iceberg.relocated.com.google.common.util.concurrent.MoreExecutors$Application.getExitingExecutorService(MoreExecutors.java:246)\n",
      "\tat org.apache.iceberg.relocated.com.google.common.util.concurrent.MoreExecutors.getExitingExecutorService(MoreExecutors.java:129)\n",
      "\tat org.apache.iceberg.aws.s3.S3OutputStream.<init>(S3OutputStream.java:115)\n",
      "\tat org.apache.iceberg.aws.s3.S3OutputFile.createOrOverwrite(S3OutputFile.java:70)\n",
      "\tat org.apache.iceberg.parquet.ParquetIO$ParquetOutputFile.createOrOverwrite(ParquetIO.java:151)\n",
      "\tat org.apache.iceberg.shaded.org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:345)\n",
      "\tat org.apache.iceberg.shaded.org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:323)\n",
      "\tat org.apache.iceberg.parquet.ParquetWriter.ensureWriterInitialized(ParquetWriter.java:111)\n",
      "\tat org.apache.iceberg.parquet.ParquetWriter.flushRowGroup(ParquetWriter.java:210)\n",
      "\tat org.apache.iceberg.parquet.ParquetWriter.close(ParquetWriter.java:254)\n",
      "\tat org.apache.iceberg.io.DataWriter.close(DataWriter.java:82)\n",
      "\tat org.apache.iceberg.io.RollingFileWriter.closeCurrentWriter(RollingFileWriter.java:122)\n",
      "\tat org.apache.iceberg.io.RollingFileWriter.close(RollingFileWriter.java:147)\n",
      "\tat org.apache.iceberg.io.RollingDataWriter.close(RollingDataWriter.java:32)\n",
      "\tat org.apache.iceberg.spark.source.SparkWrite$UnpartitionedDataWriter.close(SparkWrite.java:747)\n",
      "\tat org.apache.iceberg.spark.source.SparkWrite$UnpartitionedDataWriter.abort(SparkWrite.java:739)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$6(WriteToDataSourceV2Exec.scala:482)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1408)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:486)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:425)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:388)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "24/12/24 17:42:06 WARN Utils: Suppressing exception in catch: Shutdown in progress\n",
      "java.lang.IllegalStateException: Shutdown in progress\n",
      "\tat java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:66)\n",
      "\tat java.base/java.lang.Runtime.addShutdownHook(Runtime.java:216)\n",
      "\tat org.apache.iceberg.relocated.com.google.common.util.concurrent.MoreExecutors$Application.addShutdownHook(MoreExecutors.java:289)\n",
      "\tat org.apache.iceberg.relocated.com.google.common.util.concurrent.MoreExecutors$Application.addDelayedShutdownHook(MoreExecutors.java:266)\n",
      "\tat org.apache.iceberg.relocated.com.google.common.util.concurrent.MoreExecutors$Application.getExitingExecutorService(MoreExecutors.java:241)\n",
      "\tat org.apache.iceberg.relocated.com.google.common.util.concurrent.MoreExecutors$Application.getExitingExecutorService(MoreExecutors.java:246)\n",
      "\tat org.apache.iceberg.relocated.com.google.common.util.concurrent.MoreExecutors.getExitingExecutorService(MoreExecutors.java:129)\n",
      "\tat org.apache.iceberg.aws.s3.S3OutputStream.<init>(S3OutputStream.java:115)\n",
      "\tat org.apache.iceberg.aws.s3.S3OutputFile.createOrOverwrite(S3OutputFile.java:70)\n",
      "\tat org.apache.iceberg.parquet.ParquetIO$ParquetOutputFile.createOrOverwrite(ParquetIO.java:151)\n",
      "\tat org.apache.iceberg.shaded.org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:345)\n",
      "\tat org.apache.iceberg.shaded.org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:323)\n",
      "\tat org.apache.iceberg.parquet.ParquetWriter.ensureWriterInitialized(ParquetWriter.java:111)\n",
      "\tat org.apache.iceberg.parquet.ParquetWriter.flushRowGroup(ParquetWriter.java:210)\n",
      "\tat org.apache.iceberg.parquet.ParquetWriter.close(ParquetWriter.java:254)\n",
      "\tat org.apache.iceberg.io.DataWriter.close(DataWriter.java:82)\n",
      "\tat org.apache.iceberg.io.RollingFileWriter.closeCurrentWriter(RollingFileWriter.java:122)\n",
      "\tat org.apache.iceberg.io.RollingFileWriter.close(RollingFileWriter.java:147)\n",
      "\tat org.apache.iceberg.io.RollingDataWriter.close(RollingDataWriter.java:32)\n",
      "\tat org.apache.iceberg.spark.source.SparkWrite$UnpartitionedDataWriter.close(SparkWrite.java:747)\n",
      "\tat org.apache.iceberg.spark.source.SparkWrite$UnpartitionedDataWriter.abort(SparkWrite.java:739)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$6(WriteToDataSourceV2Exec.scala:482)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1408)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:486)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:425)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:388)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "24/12/24 17:42:06 ERROR Executor: Exception in task 3.0 in stage 0.0 (TID 3): Python worker exited unexpectedly (crashed)\n",
      "24/12/24 17:42:06 ERROR Executor: Exception in task 1.0 in stage 0.0 (TID 1): Python worker exited unexpectedly (crashed)\n",
      "24/12/24 17:42:06 ERROR Executor: Exception in task 0.0 in stage 0.0 (TID 0): Python worker exited unexpectedly (crashed)\n",
      "24/12/24 17:42:06 ERROR Executor: Exception in task 2.0 in stage 0.0 (TID 2): Python worker exited unexpectedly (crashed)\n"
     ]
    }
   ],
   "source": [
    "          \n",
    "# yf_param_pairs = [\n",
    "#     ('AAPL', '2024-12-10'),\n",
    "#     ('MSFT', '2024-12-10'),\n",
    "#     ('GOOGL', '2024-12-10'),\n",
    "# ]\n",
    "\n",
    "\n",
    "conn_config_file='cfg_connections.yaml'\n",
    "x=FinalyticsDB(conn_config_file)\n",
    "query=\"select symbol, start_date from fin.vw_etl_stock_eod_start_date\"\n",
    "symbol_start_date_pairs=x.get_symbol_start_date_pairs(query)\n",
    "# print(symbol_start_date_pairs)\n",
    "\n",
    "regd_schema_config_file='cfg_registered_table_schemas.yaml'\n",
    "sink_table='nessie.raw.stock_eod_yfinance'\n",
    "\n",
    "import_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S.\") + str(datetime.now().microsecond)[:3]\n",
    "load_raw_eod_yfinance(symbol_start_date_pairs, sink_table, regd_schema_config_file)\n",
    "          \n",
    "          \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff6a1d9-c5fa-4697-9b7b-b257048855eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "spark.sql('select count(* from nessie.raw.stock_eod_yfinance').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e022f151-9324-4786-83c9-55a283c5ff19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Poetry",
   "language": "python",
   "name": "poetry-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
