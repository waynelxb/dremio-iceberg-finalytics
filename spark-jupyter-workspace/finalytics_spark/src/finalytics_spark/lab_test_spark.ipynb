{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc71f1ce-97ce-4e88-a374-747acfc540fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import nbimporter\n",
    "from datetime import datetime, date\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType,  DateType, TimestampType\n",
    "from pyspark.sql.functions import to_date, to_timestamp\n",
    "from lab_table_manager import TableManager\n",
    "import yfinance as yf\n",
    "import time\n",
    "import random\n",
    "from lab_pg_database_manager import PGDatabaseManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6948fdf-6976-483c-b9b7-0d19cfbe368c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spark_session(app_name):\n",
    "    with open(\"cfg_connections.yaml\",\"r\") as file:\n",
    "        config=yaml.safe_load(file)\n",
    "        catalog_uri = config['docker_env']['catalog_uri'] \n",
    "        warehouse = config['docker_env']['warehouse']     # Minio Address to Write to\n",
    "        storage_uri = config['docker_env']['storage_uri'] # Minio IP address from docker inspec\n",
    "    \n",
    "    # Configure Spark with necessary packages and Iceberg/Nessie settings\n",
    "    conf = (\n",
    "        pyspark.SparkConf()\n",
    "            .setAppName(app_name)\n",
    "            # Include necessary packages\n",
    "            .set('spark.jars.packages',\n",
    "                 'org.postgresql:postgresql:42.7.3,'\n",
    "                 'org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.0,'\n",
    "                 'org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.12:0.77.1,'\n",
    "                 'software.amazon.awssdk:bundle:2.24.8,'\n",
    "                 'software.amazon.awssdk:url-connection-client:2.24.8')\n",
    "            # Enable Iceberg and Nessie extensions\n",
    "            .set('spark.sql.extensions', \n",
    "                 'org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,'\n",
    "                 'org.projectnessie.spark.extensions.NessieSparkSessionExtensions')\n",
    "            # Configure Nessie catalog\n",
    "            .set('spark.sql.catalog.nessie', 'org.apache.iceberg.spark.SparkCatalog')\n",
    "            .set('spark.sql.catalog.nessie.uri', catalog_uri)\n",
    "            .set('spark.sql.catalog.nessie.ref', 'main')\n",
    "            .set('spark.sql.catalog.nessie.authentication.type', 'NONE')\n",
    "            .set('spark.sql.catalog.nessie.catalog-impl', 'org.apache.iceberg.nessie.NessieCatalog')\n",
    "            # Set Minio as the S3 endpoint for Iceberg storage\n",
    "            .set('spark.sql.catalog.nessie.s3.endpoint', storage_uri)\n",
    "            .set('spark.sql.catalog.nessie.warehouse', warehouse)\n",
    "            .set('spark.sql.catalog.nessie.io-impl', 'org.apache.iceberg.aws.s3.S3FileIO')       \n",
    "    )   \n",
    "    \n",
    "    # Start Spark session\n",
    "    spark = SparkSession.builder.config(conf=conf).getOrCreate()  \n",
    "    # Create the \"raw\" namespace\n",
    "    spark.sql(\"CREATE NAMESPACE IF NOT EXISTS nessie.raw;\")\n",
    "\n",
    "    return spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb81d18d-f8df-4805-aaf2-f5a5b48d9d52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Poetry",
   "language": "python",
   "name": "poetry-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
