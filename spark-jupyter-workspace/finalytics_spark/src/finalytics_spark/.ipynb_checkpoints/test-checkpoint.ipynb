{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e29c97-3bf7-4c3f-8608-65c8a39b910f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType\n",
    "\n",
    "with open(\"config.yaml\",\"r\") as file_object:\n",
    "    config_dict=yaml.load(file_object,Loader=yaml.SafeLoader)\n",
    "\n",
    "## DEFINE SENSITIVE VARIABLES\n",
    "CATALOG_URI = config_dict[\"iceberg_env\"][\"CATALOG_URI\"]    # Nessie Server URI\n",
    "WAREHOUSE = config_dict[\"iceberg_env\"][\"WAREHOUSE\"]               # Minio Address to Write to\n",
    "STORAGE_URI = config_dict[\"iceberg_env\"][\"STORAGE_URI\"]      # Minio IP address from docker inspect\n",
    "\n",
    "# Configure Spark with necessary packages and Iceberg/Nessie settings\n",
    "conf = (\n",
    "    pyspark.SparkConf()\n",
    "        .setAppName('sales_data_app')\n",
    "        # Include necessary packages\n",
    "        .set('spark.jars.packages', 'org.postgresql:postgresql:42.7.3,org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.0,org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.12:0.77.1,software.amazon.awssdk:bundle:2.24.8,software.amazon.awssdk:url-connection-client:2.24.8')\n",
    "        # Enable Iceberg and Nessie extensions\n",
    "        .set('spark.sql.extensions', 'org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions')\n",
    "        # Configure Nessie catalog\n",
    "        .set('spark.sql.catalog.nessie', 'org.apache.iceberg.spark.SparkCatalog')\n",
    "        .set('spark.sql.catalog.nessie.uri', CATALOG_URI)\n",
    "        .set('spark.sql.catalog.nessie.ref', 'main')\n",
    "        .set('spark.sql.catalog.nessie.authentication.type', 'NONE')\n",
    "        .set('spark.sql.catalog.nessie.catalog-impl', 'org.apache.iceberg.nessie.NessieCatalog')\n",
    "        # Set Minio as the S3 endpoint for Iceberg storage\n",
    "        .set('spark.sql.catalog.nessie.s3.endpoint', STORAGE_URI)\n",
    "        .set('spark.sql.catalog.nessie.warehouse', WAREHOUSE)\n",
    "        .set('spark.sql.catalog.nessie.io-impl', 'org.apache.iceberg.aws.s3.S3FileIO')\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d3bfb9-8661-4721-bc0a-f3dd17f78834",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839a418f-a445-432c-8043-d334d15a3887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import yaml\n",
    "# # Create an empty dictionary\n",
    "# config_dict=dict()\n",
    "# # Add configuration data to the dictionary\n",
    "# config_dict[\"server\"]={'port': 8080, 'host': '0.0.0.0'}\n",
    "# config_dict[\"logging\"]={'level': 'info', 'file': '/var/log/web-server.log'}\n",
    "# config_dict[\"database\"]={'url': 'postgres://user:password@host:port/database','pool': 100}\n",
    "# # Save the dictionary to a YAML file\n",
    "# with open(\"./config.yaml\",\"w\") as file_object:\n",
    "#     print (file_object)\n",
    "#     yaml.dump(config_dict,file_object)\n",
    "#     print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc09c08b-a34a-4aa7-ac70-978a84522411",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "with open(\"./config.yaml\",\"r\") as file_object:\n",
    "    data=yaml.load(file_object,Loader=yaml.SafeLoader)\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baea7bb3-64a3-4cd1-b0d0-c54eb02b12f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "# Create an empty dictionary\n",
    "config_dict=dict()\n",
    "# Add configuration data to the dictionary\n",
    "config_dict[\"server\"]={'port': 8080, 'host': '0.0.0.0'}\n",
    "config_dict[\"logging\"]={'level': 'info', 'file': '/var/log/web-server.log'}\n",
    "config_dict[\"database\"]={'url': 'postgres://user:password@host:port/database','pool': 100}\n",
    "# Create another dictionary\n",
    "details_dict= {\"Website Name\":\"HoneyBadger\",\"Author\":\"Aditya\", \"Topic\":\"Configuration Files\", \"Content Type\":\"Blog\"}\n",
    "x_dict= {\"Website Name\":\"HoneyBadger\",\"Author\":\"Aditya\", \"Topic\":\"Configuration Files\", \"Content Type\":\"Blog\"}\n",
    "list_of_dicts=[config_dict,details_dict, x_dict]\n",
    "# Save data to a YAML file\n",
    "with open(\"web-server-details-1.yaml\",\"w\") as file_object:\n",
    "    yaml.dump_all(list_of_dicts,file_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3100b031-7748-460b-963f-16c51f4b185d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "with open(\"config.yaml\",\"r\") as file_object:\n",
    "    config_dict=yaml.load(file_object,Loader=yaml.SafeLoader)\n",
    "print(config_dict)\n",
    "print(config_dict[\"iceberg_env\"][\"STORAGE_URI\"])\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fd244c-cf7a-40d7-b987-ec829fd90e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType, TimestampType\n",
    "import yfinance as yf\n",
    "from pyspark.sql import Row\n",
    "from multiprocessing import Pool\n",
    "\n",
    "# Initialize PySpark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"StockDataLoader\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define a schema for the stock data\n",
    "schema = StructType([\n",
    "    StructField(\"Ticker\", StringType(), True),\n",
    "    StructField(\"Date\", TimestampType(), True),\n",
    "    StructField(\"Open\", FloatType(), True),\n",
    "    StructField(\"High\", FloatType(), True),\n",
    "    StructField(\"Low\", FloatType(), True),\n",
    "    StructField(\"Close\", FloatType(), True),\n",
    "    StructField(\"Adj Close\", FloatType(), True),\n",
    "    StructField(\"Volume\", FloatType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa57384-05ba-4aa6-be47-fa44039ae014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fetch stock data for a single ticker\n",
    "from datetime import date, datetime, timedelta\n",
    "\n",
    "\n",
    "def fetch_stock_data(symbol):\n",
    "    try:\n",
    "        quote = yf.Ticker(symbol)\n",
    "        start_date = '2024-12-01'\n",
    "        current_date = date.today()\n",
    "        hist = quote.history(start=start_date, end=current_date)\n",
    "        import_time=datetime.now()\n",
    "        # if hist.empty:\n",
    "        #     sql_script = f\"UPDATE fin.stock_symbol SET is_valid= false WHERE symbol='{symbol}';\"\n",
    "        #     # print(sql_script)\n",
    "        #     self.execute_sql_script(sql_script)\n",
    "\n",
    "        # Reset index to include the Date column in the DataFrame\n",
    "        hist.reset_index(inplace=True)\n",
    "        # # get column list with extra fields\n",
    "        column_list = [x.lower().replace(\" \", \"_\") for x in hist.columns]\n",
    "        extra_field_list = ['symbol', 'import_time']\n",
    "        column_list.extend(extra_field_list)\n",
    "\n",
    "        # get records with appended extra fields\n",
    "        hist_records_map = hist.itertuples(index=False)\n",
    "        record_list = [tuple(row) + (symbol,) + (import_time,) for row in hist_records_map]\n",
    "\n",
    "        return column_list, record_list\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data for {ticker}: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af29e04b-276e-4e63-bb4f-66437dc9586f",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_list, record_list=fetch_stock_data('C')\n",
    "print(column_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8da32b-3482-4bfd-bbfe-f22e28ccc617",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel_fetch(tickers):\n",
    "    with Pool(processes=4) as pool:  # Adjust the number of processes based on your machine's capacity\n",
    "        results = pool.map(fetch_stock_data, tickers)\n",
    "    return [row for sublist in results for row in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0ce568-1c9d-4a89-9e41-914aef2b2e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of tickers to fetch\n",
    "tickers = [\"AAPL\", \"MSFT\", \"GOOGL\", \"AMZN\"]  # Add more tickers as needed\n",
    "\n",
    "# Fetch data in parallel\n",
    "stock_data_rows = parallel_fetch(tickers)\n",
    "\n",
    "# Convert the data to a Spark DataFrame\n",
    "stock_df = spark.createDataFrame(stock_data_rows, schema=schema)\n",
    "\n",
    "# Show some rows\n",
    "stock_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea2ad7f-df78-468e-be27-c3338630a0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date, datetime\n",
    "def get_yfinance_record(symbol):\n",
    "    try:\n",
    "        quote = yf.Ticker(symbol)\n",
    "        start_date = '2024-12-01'\n",
    "        current_date = date.today()\n",
    "        print(current_date)\n",
    "        # print(start_date+timedelta(days=1))\n",
    "\n",
    "        hist = quote.history(start=start_date, end=current_date)\n",
    "        print(hist)\n",
    "        # if hist.empty:\n",
    "        #     sql_script = f\"UPDATE fin.stock_symbol SET is_valid= false WHERE symbol='{symbol}';\"\n",
    "        #     # print(sql_script)\n",
    "        #     self.execute_sql_script(sql_script)\n",
    "\n",
    "        # Reset index to include the Date column in the DataFrame\n",
    "        hist.reset_index(inplace=True)\n",
    "\n",
    "        # # get column list with extra fields\n",
    "        # column_list = [x.lower().replace(\" \", \"_\") for x in hist.columns]\n",
    "        # extra_field_list = ['symbol', 'import_time']\n",
    "        # column_list.extend(extra_field_list)\n",
    "        # print(column_list)\n",
    "\n",
    "        # # get records with extra fields\n",
    "        # hist_records_map = hist.itertuples(index=False)\n",
    "\n",
    "        # record_list = [tuple(row) + (symbol,) + (self.import_time,) for row in hist_records_map]\n",
    "\n",
    "        # print(record_list)\n",
    "\n",
    "        # return column_list, record_list\n",
    "    except Exception as e:\n",
    "        print(\"x\")\n",
    "        # print(f\"An error occurred: {e}\")\n",
    "        # if \"delisted\" in e:\n",
    "        #     print(\"abc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de2a1d9-48ba-41ef-8be7-b2b0dce8b8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_yfinance_record('C')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0819f47b-0737-453a-b4a7-dfcd27ad010c",
   "metadata": {},
   "outputs": [],
   "source": [
    "list1 = [1, 2, 3, 4, 5]\n",
    "list2 = [3, 4, 5]\n",
    "\n",
    "# Convert lists to sets\n",
    "set1 = set(list1)\n",
    "set2 = set(list2)\n",
    "\n",
    "# Find the difference between the sets\n",
    "difference1 = set1 - set2  # Elements in list1 but not in list2\n",
    "difference2 = set2 - set1  # Elements in list2 but not in list1\n",
    "\n",
    "\n",
    "print(len(difference1))\n",
    "print(\"Difference (list1 - list2):\", difference1)\n",
    "print(len(difference2))\n",
    "print(\"Difference (list2 - list1):\", difference2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304cdfc5-9860-4c28-9048-c0164ca50aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit\n",
    "import yfinance as yf\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"YahooFinanceData\").getOrCreate()\n",
    "\n",
    "# Input Data: Tickers with their respective start dates\n",
    "tickers_data = [\n",
    "    {\"ticker\": \"AAPL\", \"start_date\": \"2020-01-01\"},\n",
    "    {\"ticker\": \"MSFT\", \"start_date\": \"2019-01-01\"},\n",
    "    {\"ticker\": \"GOOGL\", \"start_date\": \"2021-01-01\"},\n",
    "]\n",
    "\n",
    "# Create RDD from input data\n",
    "tickers_rdd = spark.sparkContext.parallelize(tickers_data)\n",
    "\n",
    "# Function to fetch Yahoo Finance data\n",
    "def fetch_data(params):\n",
    "    ticker = params[\"ticker\"]\n",
    "    start_date = params[\"start_date\"]\n",
    "    try:\n",
    "        data = yf.download(ticker, start=start_date)\n",
    "        data.reset_index(inplace=True)\n",
    "        data[\"Ticker\"] = ticker  # Add a column for ticker\n",
    "        return data.to_dict(\"records\")  # Convert to a list of dictionaries\n",
    "    except Exception as e:\n",
    "        return [{\"Ticker\": ticker, \"Error\": str(e)}]\n",
    "\n",
    "# Fetch data in parallel\n",
    "results = tickers_rdd.flatMap(fetch_data).collect()\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "flattened_results = [item for sublist in results for item in sublist]\n",
    "df = spark.createDataFrame(flattened_results)\n",
    "\n",
    "# Show results\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83236d6c-adb3-4f93-89f7-5dbeec1a8e27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "from datetime import date, datetime, timedelta\n",
    "# import psycopg2.extras\n",
    "# from joblib import Parallel, delayed\n",
    "# import psycopg2\n",
    "import multiprocessing\n",
    "# from pgcopy import CopyManager\n",
    "from io import StringIO\n",
    "import yaml\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# with open(\"config.yaml\",\"r\") as file_object:\n",
    "#     documents=yaml.safe_load_all(file_object)\n",
    "#     for doc in documents:\n",
    "#         doc_name = doc['document_name']\n",
    "#         if doc_name=='yfinance_stock':\n",
    "#             registered_col_list=doc['column_list']\n",
    "\n",
    "#         if doc_name=='iceberg_env':\n",
    "#             CATALOG_URI = doc['catalog_uri'] # Nessie Server URI\n",
    "#             WAREHOUSE = doc['warehouse']     # Minio Address to Write to\n",
    "#             STORAGE_URI = doc['storage_uri'] # Minio IP address from docker inspec\n",
    "\n",
    "\n",
    "\n",
    "class MyCustomException(Exception):\n",
    "    pass\n",
    "\n",
    "class YFinanceStageIngestion:\n",
    "    def __init__(self, equity_type, destination):\n",
    "        self.equity_type = equity_type\n",
    "        self.destination = destination\n",
    "        self.import_time=datetime.now()\n",
    "        \n",
    "        # Get yfinance stock data registered column list\n",
    "        with open(\"config.yaml\",\"r\") as file_object:\n",
    "            documents=yaml.safe_load_all(file_object)\n",
    "            for doc in documents:\n",
    "                doc_name = doc['document_name']\n",
    "                if doc_name==f\"yfinance_{equity_type}\":\n",
    "                    self.registered_col_list=doc['registered_column_list']\n",
    "\n",
    "    \n",
    "        # self.import_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S.\") + str(datetime.now().microsecond)[:3]\n",
    "    \n",
    "    def fetch_yfinance_data(self, args):\n",
    "        try:\n",
    "            symbol = args[\"symbol\"]\n",
    "            start_date = args[\"start_date\"]\n",
    "            \n",
    "            quote = yf.Ticker(symbol)\n",
    "            # start_date = '2024-12-01'\n",
    "            current_date = date.today()\n",
    "            hist = quote.history(start=start_date, end=current_date)\n",
    "            # import_time=datetime.now()\n",
    "    \n",
    "            # Reset index to include the Date column in the DataFrame\n",
    "            hist.reset_index(inplace=True)\n",
    "            \n",
    "            # Standardize the hist column name by lowering the case of the original column name and replacing space with underscore\n",
    "            # This standardized column names are reginstered in configuration file\n",
    "            standardized_column_list = [x.lower().replace(\" \", \"_\") for x in hist.columns]  \n",
    "            \n",
    "            # Add symbol and import_time in column list\n",
    "            extra_field_list = ['symbol', 'import_time']\n",
    "            standardized_column_list.extend(extra_field_list)\n",
    "            \n",
    "            # Check whether the standardized column names match the registered ones   \n",
    "            set_standardized = set(standardized_column_list)\n",
    "            set_registered = set(self.registered_column_list)            \n",
    "            if set_standardized!=set_registered:\n",
    "                raise MyCustomException(f\"Error: standardized_column_list {str(standardized_column_list)} does not match registered_column_list {str(registered_column_list)}!\")\n",
    "\n",
    "            # Add symbol and import_time in each record\n",
    "            hist_records_map = hist.itertuples(index=False)            \n",
    "            record_list = [tuple(row) + (symbol,) + (self.import_time,) for row in hist_records_map]  \n",
    "            \n",
    "            return standardized_column_list, record_list\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching data for {symbol}: {e}\")\n",
    "            return []\n",
    "\n",
    "\n",
    "    def parallel_fetch(self, param_pairs):\n",
    "        yfinance_data_rdd = spark.sparkContext.parallelize(param_pairs)\n",
    "        results = yfinance_data_rdd.flatMap(self.fetch_yfinance_data).collect()\n",
    "        flattened_results = [item for sublist in results for item in sublist]\n",
    "        df = spark.createDataFrame(flattened_results)\n",
    "        df.show(truncate=False)\n",
    "            \n",
    "            \n",
    "\n",
    "stock_stage = YFinanceStageIngestion('stock', 'mytable')\n",
    "        \n",
    "# List of tickers to fetch\n",
    "stock_param_pairs = [(\"AAPL\", \"2024-12-1\"), (\"MSFT\", \"2024-12-5\"), (\"GOOGL\", \"2024-12-9\")]  # Add more tickers as needed\n",
    "\n",
    "stock_stage.parallel_fetch(stock_param_pairs)\n",
    "\n",
    "# Fetch data in parallel\n",
    "stock_data_rows = parallel_fetch(stock_param_pairs)\n",
    "\n",
    "# # Convert the data to a Spark DataFrame\n",
    "# stock_df = spark.createDataFrame(stock_data_rows, schema=schema)\n",
    "\n",
    "# # Show some rows\n",
    "# stock_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009a8d50-9ff1-45c9-8155-092bef2b8a10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa474609-2cbb-43a6-80e5-49026aeb044a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit\n",
    "import yfinance as yf\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"YahooFinanceData\").getOrCreate()\n",
    "\n",
    "# Input Data: Tickers with their respective start dates\n",
    "tickers_data = [\n",
    "    {\"ticker\": \"AAPL\", \"start_date\": \"2020-01-01\"},\n",
    "    {\"ticker\": \"MSFT\", \"start_date\": \"2019-01-01\"},\n",
    "    {\"ticker\": \"GOOGL\", \"start_date\": \"2021-01-01\"},\n",
    "]\n",
    "\n",
    "# Create RDD from input data\n",
    "tickers_rdd = spark.sparkContext.parallelize(tickers_data)\n",
    "\n",
    "# Function to fetch Yahoo Finance data\n",
    "def fetch_data(params):\n",
    "    ticker = params[\"ticker\"]\n",
    "    start_date = params[\"start_date\"]\n",
    "    try:\n",
    "        quote = yf.Ticker(ticker)\n",
    "        hist = quote.history(start=start_date, end=current_date)\n",
    "        hist.reset_index(inplace=True)\n",
    "        return hist.to_dict(\"records\")  # Convert to a list of dictionaries\n",
    "    except Exception as e:\n",
    "        return [{\"Ticker\": ticker, \"Error\": str(e)}]\n",
    "\n",
    "# Fetch data in parallel\n",
    "results = tickers_rdd.flatMap(fetch_data).collect()\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "flattened_results = [item for sublist in results for item in sublist]\n",
    "df = spark.createDataFrame(flattened_results)\n",
    "\n",
    "# Show results\n",
    "df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a3930d-cf0d-45c6-8cdd-0c86390a6437",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba4ceb6-906b-4e34-8f51-9e646440b579",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import yfinance as yf\n",
    "import numpy as np\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, DateType\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"YahooFinanceData\").getOrCreate()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "symbol = 'aasdfljksadfjlksd'\n",
    "start_date = \"\"\n",
    "\n",
    "quote = yf.Ticker('AAPL')\n",
    "# start_date = '2024-12-01'\n",
    "current_date = date.today()\n",
    "hist = quote.history()\n",
    "# import_time=datetime.now()\n",
    "\n",
    "# Reset index to include the Date column in the DataFrame\n",
    "hist.reset_index(inplace=True)\n",
    "\n",
    "# Standardize the hist column name by lowering the case of the original column name and replacing space with underscore\n",
    "# This standardized column names are reginstered in configuration file\n",
    "standardized_column_list = [x.lower().replace(\" \", \"_\") for x in hist.columns]  \n",
    "\n",
    "# Add symbol and import_time in column list\n",
    "extra_field_list = ['symbol', 'import_time']\n",
    "standardized_column_list.extend(extra_field_list)\n",
    "print(standardized_column_list)\n",
    "\n",
    "# # Check whether the standardized column names match the registered ones   \n",
    "# set_standardized = set(standardized_column_list)\n",
    "# set_registered = set(self.registered_column_list)            \n",
    "# if set_standardized!=set_registered:\n",
    "#     raise MyCustomException(f\"Error: standardized_column_list {str(standardized_column_list)} does not match registered_column_list {str(registered_column_list)}!\")\n",
    "\n",
    "# # Add symbol and import_time in each record\n",
    "# hist_records_map = hist.itertuples(index=False)            \n",
    "# record_list = [tuple(row) + (symbol,) + (self.import_time,) for row in hist_records_map]  \n",
    "\n",
    "# return standardized_column_list, record_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "bad98a3a-3105-4726-88fd-cefa466a52fb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "('GOOGL', '2024-12-01')\n",
      "('MSFT', '2024-12-01')\n",
      "('AAPL', '2024-12-01')\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+------+------+------+------+---------+---------+------------+------+----------+\n",
      "|Date                    |Open  |High  |Low   |Close |Volume   |Dividends|Stock Splits|Ticker|Start Date|\n",
      "+------------------------+------+------+------+------+---------+---------+------------+------+----------+\n",
      "|2024-12-02 00:00:00-0500|237.27|240.79|237.16|239.59|48137100 |0.0      |0.0         |AAPL  |2024-12-01|\n",
      "|2024-12-03 00:00:00-0500|239.81|242.76|238.9 |242.65|38861000 |0.0      |0.0         |AAPL  |2024-12-01|\n",
      "|2024-12-04 00:00:00-0500|242.87|244.11|241.25|243.01|44383900 |0.0      |0.0         |AAPL  |2024-12-01|\n",
      "|2024-12-05 00:00:00-0500|243.99|244.54|242.13|243.04|40033900 |0.0      |0.0         |AAPL  |2024-12-01|\n",
      "|2024-12-06 00:00:00-0500|242.91|244.63|242.08|242.84|36870600 |0.0      |0.0         |AAPL  |2024-12-01|\n",
      "|2024-12-09 00:00:00-0500|241.83|247.24|241.75|246.75|44649200 |0.0      |0.0         |AAPL  |2024-12-01|\n",
      "|2024-12-10 00:00:00-0500|246.89|248.21|245.34|247.77|36914800 |0.0      |0.0         |AAPL  |2024-12-01|\n",
      "|2024-12-11 00:00:00-0500|247.96|250.8 |246.26|246.49|45205800 |0.0      |0.0         |AAPL  |2024-12-01|\n",
      "|2024-12-12 00:00:00-0500|246.89|248.74|245.68|247.96|32777500 |0.0      |0.0         |AAPL  |2024-12-01|\n",
      "|2024-12-13 00:00:00-0500|247.82|249.29|246.24|248.13|33155300 |0.0      |0.0         |AAPL  |2024-12-01|\n",
      "|2024-12-16 00:00:00-0500|247.99|251.38|247.65|251.04|51694800 |0.0      |0.0         |AAPL  |2024-12-01|\n",
      "|2024-12-17 00:00:00-0500|250.08|253.83|249.78|253.48|51356400 |0.0      |0.0         |AAPL  |2024-12-01|\n",
      "|2024-12-18 00:00:00-0500|252.16|254.28|247.74|248.05|56774100 |0.0      |0.0         |AAPL  |2024-12-01|\n",
      "|2024-12-19 00:00:00-0500|247.5 |252.0 |247.09|249.79|60882300 |0.0      |0.0         |AAPL  |2024-12-01|\n",
      "|2024-12-20 00:00:00-0500|248.04|255.0 |245.69|254.49|146890100|0.0      |0.0         |AAPL  |2024-12-01|\n",
      "|2024-12-02 00:00:00-0500|421.57|433.0 |421.31|430.98|20207200 |0.0      |0.0         |MSFT  |2024-12-01|\n",
      "|2024-12-03 00:00:00-0500|429.84|432.47|427.74|431.2 |18302000 |0.0      |0.0         |MSFT  |2024-12-01|\n",
      "|2024-12-04 00:00:00-0500|433.03|439.67|432.63|437.42|26009400 |0.0      |0.0         |MSFT  |2024-12-01|\n",
      "|2024-12-05 00:00:00-0500|437.92|444.66|436.17|442.62|21697800 |0.0      |0.0         |MSFT  |2024-12-01|\n",
      "|2024-12-06 00:00:00-0500|442.3 |446.1 |441.77|443.57|18821000 |0.0      |0.0         |MSFT  |2024-12-01|\n",
      "+------------------------+------+------+------+------+---------+---------+------------+------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType, TimestampType, IntegerType\n",
    "import yfinance as yf\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"YahooFinanceData\").getOrCreate()\n",
    "\n",
    "# Input data: List of tickers (start_date is fixed as '2024-12-01' for all)\n",
    "tickers_data = [\n",
    "    ('AAPL', '2024-12-01'),\n",
    "    ('MSFT', '2024-12-01'),\n",
    "    ('GOOGL', '2024-12-01'),\n",
    "]\n",
    "\n",
    "# Create an RDD from the input data\n",
    "tickers_rdd = spark.sparkContext.parallelize(tickers_data)\n",
    "\n",
    "# Fetch function to retrieve all stock data fields with ticker and fixed start_date\n",
    "def fetch_stock_data_with_ticker_and_date(ticker, start_date):\n",
    "    try:\n",
    "        # Fetch stock data using yfinance\n",
    "        stock_data = yf.Ticker(ticker).history(start=start_date)\n",
    "        stock_data.reset_index(inplace=True)  # Reset index to include 'Date' as a column\n",
    "        stock_data['Date'] = stock_data['Date'].dt.strftime('%Y-%m-%d %H:%M:%S%z')\n",
    "        \n",
    "        # Convert all rows to tuples and append ticker and start_date\n",
    "        return [tuple(row) + (ticker, start_date) for row in stock_data.itertuples(index=False)]\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data for {ticker}: {e}\")\n",
    "        return []\n",
    "\n",
    "# Function to process ticker data and call the fetch function with unpacked params\n",
    "def process_ticker_data(ticker_data):\n",
    "    print(ticker_data)\n",
    "    return fetch_stock_data_with_ticker_and_date(*ticker_data)\n",
    "\n",
    "# Use flatMap to process data in parallel and collect all rows as a single list of tuples\n",
    "results_rdd = tickers_rdd.flatMap(process_ticker_data)\n",
    "\n",
    "# Collect the data as a list of tuples\n",
    "results = results_rdd.collect()\n",
    "\n",
    "# Define the schema for the DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"Date\", StringType(), True),        # 'Date' column as Timestamp\n",
    "    StructField(\"Open\", FloatType(), True),            # 'Open' column as Float\n",
    "    StructField(\"High\", FloatType(), True),            # 'High' column as Float\n",
    "    StructField(\"Low\", FloatType(), True),             # 'Low' column as Float\n",
    "    StructField(\"Close\", FloatType(), True),           # 'Close' column as Float\n",
    "    StructField(\"Volume\", IntegerType(), True),            # 'Volume' column as Int\n",
    "    StructField(\"Dividends\", FloatType(), True),       # 'Dividends' column as Float\n",
    "    StructField(\"Stock Splits\", FloatType(), True),    # 'Stock Splits' column as Float\n",
    "    StructField(\"Ticker\", StringType(), True),         # 'Ticker' column as String\n",
    "    StructField(\"Start Date\", StringType(), True)      # 'Start Date' column as String\n",
    "])\n",
    "\n",
    "# Create a DataFrame using the schema\n",
    "df = spark.createDataFrame(results, schema)\n",
    "\n",
    "# Show the DataFrame content\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a179b8-b0c3-4e90-9c01-3a660e1decbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import yfinance as yf\n",
    "import numpy as np\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"YahooFinanceData\").getOrCreate()\n",
    "\n",
    "# Input data: List of tuples with tickers and start dates\n",
    "tickers_data = [\n",
    "    ('AAPL', '2024-12-01'),\n",
    "    ('MSFT', '2024-12-01'),\n",
    "    ('GOOGL', '2024-12-01'),\n",
    "]\n",
    "\n",
    "# Create an RDD from the input data\n",
    "tickers_rdd = spark.sparkContext.parallelize(tickers_data)\n",
    "\n",
    "# Function to fetch daily data for a ticker using yf.Ticker\n",
    "def fetch_data(params):\n",
    "    ticker, start_date = params\n",
    "    try:\n",
    "        # Fetch the ticker object\n",
    "        quote = yf.Ticker(ticker)\n",
    "        \n",
    "        # Get historical market data\n",
    "        data = quote.history(start=start_date)\n",
    "        \n",
    "        # Reset index to include the 'Date' column\n",
    "        data.reset_index(inplace=True)\n",
    "        \n",
    "        \n",
    "        rows = data.apply(lambda row: tuple(row), axis=1).tolist()       \n",
    "\n",
    "        return rows\n",
    "    except Exception as e:\n",
    "        # Return empty list on error\n",
    "        return []\n",
    "\n",
    "# Use flatMap to fetch data in parallel\n",
    "results_rdd = tickers_rdd.flatMap(fetch_data)\n",
    "\n",
    "# Create the DataFrame by inferring schema from the RDD\n",
    "df = spark.createDataFrame(results_rdd)\n",
    "\n",
    "# Show the results\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e5f9ba-dc0f-4938-8816-ab462bd8b8da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import numpy as np\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, DateType\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType, TimestampType, IntegerType\n",
    "import yfinance as yf\n",
    "\n",
    "\n",
    "from datetime import date, datetime, timedelta\n",
    "\n",
    "class MyCustomException(Exception):\n",
    "    pass\n",
    "\n",
    "class YFinanceStageIngestion:\n",
    "    def __init__(self, equity_type, destination):\n",
    "        self.equity_type = equity_type\n",
    "        self.destination = destination\n",
    "        self.import_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S.\") + str(datetime.now().microsecond)[:3]      \n",
    "        \n",
    "   \n",
    "    def fetch_yfinance_record(self, params):\n",
    "        try:\n",
    "            symbol, start_date = params      \n",
    "            quote = yf.Ticker(symbol)\n",
    "            current_date = date.today()\n",
    "            hist = quote.history(start=start_date, end=current_date)\n",
    "   \n",
    "            # Reset index to include the Date column in the DataFrame\n",
    "            hist.reset_index(inplace=True)\n",
    "            hist['Date'] = hist['Date'].dt.strftime('%Y-%m-%d %H:%M:%S%z')\n",
    "\n",
    "            # Add symbol and import_time in each record\n",
    "            hist_records_map = hist.itertuples(index=False)            \n",
    "            record_list = [tuple(row) + (symbol, self.import_time) for row in hist_records_map]\n",
    "            record_list=[]\n",
    "            return record_list\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching data for {symbol}: {e}\")\n",
    "            return []    \n",
    "    \n",
    "    def process_yfinance_record(self, param):\n",
    "        print(param)\n",
    "        return self.fetch_yfinance_record(*param)\n",
    "\n",
    "        \n",
    "    def parallel_fetch(self, param_pairs):       \n",
    "        spark = SparkSession.builder.appName(\"YahooFinanceData\").getOrCreate()\n",
    "        record_rdd = spark.sparkContext.parallelize(param_pairs)        \n",
    "        results_rdd = record_rdd.flatMap(self.process_yfinance_record)  \n",
    "        results = results_rdd.collect()\n",
    "        print(results)\n",
    "        \n",
    "\n",
    "stock_param_pairs = [\n",
    "    ('AAPL', '2024-12-10'),\n",
    "    ('MSFT', '2024-12-10'),\n",
    "    ('GOOGL', '2024-12-10'),\n",
    "]\n",
    "      \n",
    "stock_stage = YFinanceStageIngestion('stock', 'mytable')\n",
    "        \n",
    "\n",
    "# Fetch data in parallel\n",
    "stock_data_rows = stock_stage.parallel_fetch(stock_param_pairs)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Input data: List of tickers (start_date is fixed as '2024-12-01' for all)\n",
    "# tickers_data = [\n",
    "#     ('AAPL', '2024-12-01'),\n",
    "#     ('MSFT', '2024-12-01'),\n",
    "#     ('GOOGL', '2024-12-01'),\n",
    "# ]\n",
    "\n",
    "# # Create an RDD from the input data\n",
    "# tickers_rdd = spark.sparkContext.parallelize(tickers_data)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c65748f6-d437-4a7f-b2d2-3b3e00992ef0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing ('AAPL', '2024-12-10')                                   (0 + 4) / 4]\n",
      "Processing ('MSFT', '2024-12-10')                                   (1 + 3) / 4]\n",
      "Processing ('GOOGL', '2024-12-10')\n",
      "24/12/23 14:50:44 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+------------------+------------------+------------------+---------+---------+------------+------+--------------------+\n",
      "|               Date|              Open|              High|               Low|             Close|   Volume|Dividends|Stock Splits|Ticker|          ImportTime|\n",
      "+-------------------+------------------+------------------+------------------+------------------+---------+---------+------------+------+--------------------+\n",
      "|2024-12-10 00:00:00|246.88999938964844| 248.2100067138672|245.33999633789062|247.77000427246094| 36914800|      0.0|         0.0|  AAPL|2024-12-23 14:50:...|\n",
      "|2024-12-11 00:00:00| 247.9600067138672| 250.8000030517578|246.25999450683594|246.49000549316406| 45205800|      0.0|         0.0|  AAPL|2024-12-23 14:50:...|\n",
      "|2024-12-12 00:00:00|246.88999938964844|248.74000549316406|245.67999267578125| 247.9600067138672| 32777500|      0.0|         0.0|  AAPL|2024-12-23 14:50:...|\n",
      "|2024-12-13 00:00:00|247.82000732421875| 249.2899932861328|246.24000549316406| 248.1300048828125| 33155300|      0.0|         0.0|  AAPL|2024-12-23 14:50:...|\n",
      "|2024-12-16 00:00:00|247.99000549316406| 251.3800048828125|247.64999389648438| 251.0399932861328| 51694800|      0.0|         0.0|  AAPL|2024-12-23 14:50:...|\n",
      "|2024-12-17 00:00:00| 250.0800018310547| 253.8300018310547|249.77999877929688|253.47999572753906| 51356400|      0.0|         0.0|  AAPL|2024-12-23 14:50:...|\n",
      "|2024-12-18 00:00:00|252.16000366210938|254.27999877929688|247.74000549316406| 248.0500030517578| 56774100|      0.0|         0.0|  AAPL|2024-12-23 14:50:...|\n",
      "|2024-12-19 00:00:00|             247.5|             252.0|247.08999633789062| 249.7899932861328| 60882300|      0.0|         0.0|  AAPL|2024-12-23 14:50:...|\n",
      "|2024-12-20 00:00:00| 248.0399932861328|             255.0|245.69000244140625|254.49000549316406|146890100|      0.0|         0.0|  AAPL|2024-12-23 14:50:...|\n",
      "|2024-12-10 00:00:00| 444.3900146484375| 449.6199951171875| 441.6000061035156| 443.3299865722656| 18469500|      0.0|         0.0|  MSFT|2024-12-23 14:50:...|\n",
      "|2024-12-11 00:00:00|444.04998779296875| 450.3500061035156|444.04998779296875|  448.989990234375| 19200200|      0.0|         0.0|  MSFT|2024-12-23 14:50:...|\n",
      "|2024-12-12 00:00:00| 449.1099853515625| 456.1600036621094| 449.1099853515625|449.55999755859375| 20834800|      0.0|         0.0|  MSFT|2024-12-23 14:50:...|\n",
      "|2024-12-13 00:00:00|448.44000244140625|451.42999267578125| 445.5799865722656| 447.2699890136719| 20177800|      0.0|         0.0|  MSFT|2024-12-23 14:50:...|\n",
      "|2024-12-16 00:00:00| 447.2699890136719|452.17999267578125| 445.2799987792969| 451.5899963378906| 23598800|      0.0|         0.0|  MSFT|2024-12-23 14:50:...|\n",
      "|2024-12-17 00:00:00|  451.010009765625| 455.2900085449219|449.57000732421875| 454.4599914550781| 22733500|      0.0|         0.0|  MSFT|2024-12-23 14:50:...|\n",
      "|2024-12-18 00:00:00|451.32000732421875| 452.6499938964844| 437.0199890136719| 437.3900146484375| 24444500|      0.0|         0.0|  MSFT|2024-12-23 14:50:...|\n",
      "|2024-12-19 00:00:00| 441.6199951171875|443.17999267578125|436.32000732421875| 437.0299987792969| 22963700|      0.0|         0.0|  MSFT|2024-12-23 14:50:...|\n",
      "|2024-12-20 00:00:00| 433.1099853515625|  443.739990234375| 428.6300048828125| 436.6000061035156| 64235200|      0.0|         0.0|  MSFT|2024-12-23 14:50:...|\n",
      "|2024-12-10 00:00:00|182.85000610351562|186.36000061035156| 181.0500030517578| 185.1699981689453| 54813000|      0.0|         0.0| GOOGL|2024-12-23 14:50:...|\n",
      "|2024-12-11 00:00:00|185.30999755859375|195.61000061035156|184.85000610351562|195.39999389648438| 67894100|      0.0|         0.0| GOOGL|2024-12-23 14:50:...|\n",
      "+-------------------+------------------+------------------+------------------+------------------+---------+---------+------------+------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType, TimestampType\n",
    "import yfinance as yf\n",
    "from datetime import datetime, date\n",
    "\n",
    "# Define the custom exception\n",
    "class MyCustomException(Exception):\n",
    "    pass\n",
    "\n",
    "# Main class for ingestion\n",
    "class YFinanceStageIngestion:\n",
    "    def __init__(self, equity_type, destination):\n",
    "        self.equity_type = equity_type\n",
    "        self.destination = destination\n",
    "        self.import_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S.\") + str(datetime.now().microsecond)[:3]      \n",
    "\n",
    "    # Function to fetch data from Yahoo Finance\n",
    "    def fetch_yfinance_record(self, params):\n",
    "        try:\n",
    "            symbol, start_date = params\n",
    "            # Fetch stock data using yfinance\n",
    "            quote = yf.Ticker(symbol)\n",
    "            current_date = date.today()\n",
    "            hist = quote.history(start=start_date, end=current_date)\n",
    "            \n",
    "            # Reset index to include Date as a column and format it\n",
    "            hist.reset_index(inplace=True)\n",
    "            hist['Date'] = hist['Date'].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "            # Add symbol and import_time to each row\n",
    "            record_list = [\n",
    "                tuple(row) + (symbol, self.import_time) for row in hist.itertuples(index=False)\n",
    "            ]\n",
    "            return record_list\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching data for {symbol}: {e}\")\n",
    "            return []  # Return an empty list on error\n",
    "    \n",
    "    # Function to process the records (pass through parameters)\n",
    "    def process_yfinance_record(self, param):\n",
    "        print(f\"Processing {param}\")\n",
    "        return self.fetch_yfinance_record(param)\n",
    "\n",
    "    # Parallel fetch function\n",
    "    def parallel_fetch(self, param_pairs):\n",
    "        # Create Spark session\n",
    "        spark = SparkSession.builder.appName(\"YahooFinanceData\").getOrCreate()\n",
    "        \n",
    "        # Create RDD from the input parameter pairs\n",
    "        record_rdd = spark.sparkContext.parallelize(param_pairs)\n",
    "        \n",
    "        # Use flatMap to return a flattened list of records\n",
    "        results_rdd = record_rdd.flatMap(self.process_yfinance_record)\n",
    "        \n",
    "        # Collect the results from the RDD\n",
    "        results = results_rdd.collect()\n",
    "        return results\n",
    "\n",
    "# List of stock symbols and start dates\n",
    "stock_param_pairs = [\n",
    "    ('AAPL', '2024-12-10'),\n",
    "    ('MSFT', '2024-12-10'),\n",
    "    ('GOOGL', '2024-12-10'),\n",
    "]\n",
    "\n",
    "# Instantiate the class\n",
    "stock_stage = YFinanceStageIngestion('stock', 'mytable')\n",
    "\n",
    "# Fetch data in parallel\n",
    "stock_data_rows = stock_stage.parallel_fetch(stock_param_pairs)\n",
    "\n",
    "spark = SparkSession.builder.appName(\"YahooFinanceDatax\").getOrCreate()\n",
    "\n",
    "\n",
    "# You can also load the result into a DataFrame if required\n",
    "from pyspark.sql import Row\n",
    "schema = ['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Dividends', 'Stock Splits', 'Ticker', 'ImportTime']\n",
    "rdd_rows = spark.sparkContext.parallelize(stock_data_rows)\n",
    "df = spark.createDataFrame(rdd_rows, schema)\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "3f1a4b76-06bc-4799-9df5-54651bc9ab3f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 167:===================>                                     (1 + 2) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+------------------+------------------+------------------+---------+---+---+\n",
      "|_1 |_2                |_3                |_4                |_5                |_6       |_7 |_8 |\n",
      "+---+------------------+------------------+------------------+------------------+---------+---+---+\n",
      "|{} |237.27000427246094|240.7899932861328 |237.16000366210938|239.58999633789062|48137100 |0.0|0.0|\n",
      "|{} |239.80999755859375|242.75999450683594|238.89999389648438|242.64999389648438|38861000 |0.0|0.0|\n",
      "|{} |242.8699951171875 |244.11000061035156|241.25            |243.00999450683594|44383900 |0.0|0.0|\n",
      "|{} |243.99000549316406|244.5399932861328 |242.1300048828125 |243.0399932861328 |40033900 |0.0|0.0|\n",
      "|{} |242.91000366210938|244.6300048828125 |242.0800018310547 |242.83999633789062|36870600 |0.0|0.0|\n",
      "|{} |241.8300018310547 |247.24000549316406|241.75            |246.75            |44649200 |0.0|0.0|\n",
      "|{} |246.88999938964844|248.2100067138672 |245.33999633789062|247.77000427246094|36914800 |0.0|0.0|\n",
      "|{} |247.9600067138672 |250.8000030517578 |246.25999450683594|246.49000549316406|45205800 |0.0|0.0|\n",
      "|{} |246.88999938964844|248.74000549316406|245.67999267578125|247.9600067138672 |32777500 |0.0|0.0|\n",
      "|{} |247.82000732421875|249.2899932861328 |246.24000549316406|248.1300048828125 |33155300 |0.0|0.0|\n",
      "|{} |247.99000549316406|251.3800048828125 |247.64999389648438|251.0399932861328 |51694800 |0.0|0.0|\n",
      "|{} |250.0800018310547 |253.8300018310547 |249.77999877929688|253.47999572753906|51356400 |0.0|0.0|\n",
      "|{} |252.16000366210938|254.27999877929688|247.74000549316406|248.0500030517578 |56774100 |0.0|0.0|\n",
      "|{} |247.5             |252.0             |247.08999633789062|249.7899932861328 |60882300 |0.0|0.0|\n",
      "|{} |248.0399932861328 |255.0             |245.69000244140625|254.49000549316406|146890100|0.0|0.0|\n",
      "|{} |421.57000732421875|433.0             |421.30999755859375|430.9800109863281 |20207200 |0.0|0.0|\n",
      "|{} |429.8399963378906 |432.4700012207031 |427.739990234375  |431.20001220703125|18302000 |0.0|0.0|\n",
      "|{} |433.0299987792969 |439.6700134277344 |432.6300048828125 |437.4200134277344 |26009400 |0.0|0.0|\n",
      "|{} |437.9200134277344 |444.6600036621094 |436.1700134277344 |442.6199951171875 |21697800 |0.0|0.0|\n",
      "|{} |442.29998779296875|446.1000061035156 |441.7699890136719 |443.57000732421875|18821000 |0.0|0.0|\n",
      "+---+------------------+------------------+------------------+------------------+---------+---+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import yfinance as yf\n",
    "import numpy as np\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"YahooFinanceData\").getOrCreate()\n",
    "\n",
    "# Input data: List of tuples with tickers and start dates\n",
    "tickers_data = [\n",
    "    ('AAPL', '2024-12-01'),\n",
    "    ('MSFT', '2024-12-01'),\n",
    "    ('GOOGL', '2024-12-01'),\n",
    "]\n",
    "\n",
    "# Create an RDD from the input data\n",
    "tickers_rdd = spark.sparkContext.parallelize(tickers_data)\n",
    "\n",
    "# Function to fetch daily data for a ticker using yf.Ticker\n",
    "def fetch_data(params):\n",
    "    ticker, start_date = params\n",
    "    try:\n",
    "        # Fetch the ticker object\n",
    "        quote = yf.Ticker(ticker)\n",
    "        \n",
    "        # Get historical market data\n",
    "        data = quote.history(start=start_date)\n",
    "        \n",
    "        # Reset index to include the 'Date' column\n",
    "        data.reset_index(inplace=True)\n",
    "        \n",
    "        \n",
    "        rows = data.apply(lambda row: tuple(row), axis=1).tolist()       \n",
    "\n",
    "        return rows\n",
    "    except Exception as e:\n",
    "        # Return empty list on error\n",
    "        return []\n",
    "\n",
    "# Use flatMap to fetch data in parallel\n",
    "results_rdd = tickers_rdd.flatMap(fetch_data)\n",
    "\n",
    "# Create the DataFrame by inferring schema from the RDD\n",
    "df = spark.createDataFrame(results_rdd)\n",
    "\n",
    "# Show the results\n",
    "df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9f7a2b46-da60-4d92-a6bd-5e7a0bdd0a99",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'PipelinedRDD' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 45\u001b[0m\n\u001b[1;32m     41\u001b[0m results_rdd \u001b[38;5;241m=\u001b[39m tickers_rdd\u001b[38;5;241m.\u001b[39mflatMap(fetch_data)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Extract column names dynamically from the data\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# After resetting the index, the columns should include 'Date', 'Open', 'Close', etc.\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mresults_rdd\u001b[49m\u001b[43m)\u001b[49m)  \u001b[38;5;66;03m# Get the first element to infer columns\u001b[39;00m\n\u001b[1;32m     46\u001b[0m columns \u001b[38;5;241m=\u001b[39m [col \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m data[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_fields]  \u001b[38;5;66;03m# Extract column names dynamically from the first row\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Create the DataFrame by inferring schema from the RDD\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'PipelinedRDD' object is not iterable"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import yfinance as yf\n",
    "import numpy as np\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"YahooFinanceData\").getOrCreate()\n",
    "\n",
    "# Input data: List of tuples with tickers and start dates\n",
    "tickers_data = [\n",
    "    ('AAPL', '2020-01-01'),\n",
    "    ('MSFT', '2019-01-01'),\n",
    "    ('GOOGL', '2021-01-01'),\n",
    "]\n",
    "\n",
    "# Create an RDD from the input data\n",
    "tickers_rdd = spark.sparkContext.parallelize(tickers_data)\n",
    "\n",
    "# Function to fetch daily data for a ticker using yf.Ticker\n",
    "def fetch_data(params):\n",
    "    ticker, start_date = params\n",
    "    try:\n",
    "        # Fetch the ticker object\n",
    "        quote = yf.Ticker(ticker)\n",
    "        \n",
    "        # Get historical market data\n",
    "        data = quote.history(start=start_date)\n",
    "        \n",
    "        # Reset index to include the 'Date' column\n",
    "        data.reset_index(inplace=True)\n",
    "        \n",
    "        # Convert the DataFrame to a list of tuples\n",
    "        rows = data.apply(lambda row: tuple(row), axis=1).tolist()\n",
    "        \n",
    "        return rows\n",
    "    except Exception as e:\n",
    "        # Return empty list on error\n",
    "        print(f\"Error fetching data for {ticker}: {e}\")\n",
    "        return []\n",
    "\n",
    "# Use flatMap to fetch data in parallel\n",
    "results_rdd = tickers_rdd.flatMap(fetch_data)\n",
    "\n",
    "# Extract column names dynamically from the data\n",
    "# After resetting the index, the columns should include 'Date', 'Open', 'Close', etc.\n",
    "data = next(iter(results_rdd))  # Get the first element to infer columns\n",
    "columns = [col for col in data[0]._fields]  # Extract column names dynamically from the first row\n",
    "\n",
    "# Create the DataFrame by inferring schema from the RDD\n",
    "df = spark.createDataFrame(results_rdd, schema=columns)\n",
    "\n",
    "# Show the results\n",
    "df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3335d545-6ffa-4492-9e56-a1fc59d97de0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(Timestamp('2024-12-11 00:00:00-0500', tz='America/New_York'), 73.0, 73.26000213623047, 71.2699966430664, 71.95999908447266, 22533600, 0.0, 0.0, 'C', datetime.datetime(2024, 12, 22, 23, 6, 43, 209819)), (Timestamp('2024-12-12 00:00:00-0500', tz='America/New_York'), 71.86000061035156, 72.33000183105469, 71.41000366210938, 71.43000030517578, 8893000, 0.0, 0.0, 'C', datetime.datetime(2024, 12, 22, 23, 6, 43, 209819)), (Timestamp('2024-12-13 00:00:00-0500', tz='America/New_York'), 71.70999908447266, 71.91000366210938, 70.76000213623047, 71.01000213623047, 9819900, 0.0, 0.0, 'C', datetime.datetime(2024, 12, 22, 23, 6, 43, 209819)), (Timestamp('2024-12-16 00:00:00-0500', tz='America/New_York'), 71.2699966430664, 71.7699966430664, 70.83000183105469, 71.48999786376953, 11833800, 0.0, 0.0, 'C', datetime.datetime(2024, 12, 22, 23, 6, 43, 209819)), (Timestamp('2024-12-17 00:00:00-0500', tz='America/New_York'), 70.9000015258789, 71.3499984741211, 70.80000305175781, 71.12000274658203, 12639000, 0.0, 0.0, 'C', datetime.datetime(2024, 12, 22, 23, 6, 43, 209819)), (Timestamp('2024-12-18 00:00:00-0500', tz='America/New_York'), 71.18000030517578, 71.47000122070312, 67.91999816894531, 68.12000274658203, 18028000, 0.0, 0.0, 'C', datetime.datetime(2024, 12, 22, 23, 6, 43, 209819)), (Timestamp('2024-12-19 00:00:00-0500', tz='America/New_York'), 69.20999908447266, 69.83999633789062, 68.31999969482422, 68.41999816894531, 13464300, 0.0, 0.0, 'C', datetime.datetime(2024, 12, 22, 23, 6, 43, 209819)), (Timestamp('2024-12-20 00:00:00-0500', tz='America/New_York'), 68.29000091552734, 70.26000213623047, 68.02999877929688, 69.19000244140625, 27718800, 0.0, 0.0, 'C', datetime.datetime(2024, 12, 22, 23, 6, 43, 209819))]\n"
     ]
    }
   ],
   "source": [
    "symbol='C'\n",
    "quote = yf.Ticker(symbol)\n",
    "start_date ='2024-12-11'\n",
    "current_date = date.today()\n",
    "# print(start_date+timedelta(days=1))\n",
    "\n",
    "hist = quote.history(start=start_date, end=current_date)\n",
    "import_time=datetime.now()\n",
    "\n",
    "# Reset index to include the Date column in the DataFrame\n",
    "hist.reset_index(inplace=True)\n",
    "\n",
    "# get column list with extra fields\n",
    "column_list = [x.lower().replace(\" \", \"_\") for x in hist.columns]\n",
    "extra_field_list = ['symbol', 'import_time']\n",
    "column_list.extend(extra_field_list)\n",
    "# print(column_list)\n",
    "\n",
    "# get records with extra fields\n",
    "hist_records_map = hist.itertuples(index=False)\n",
    "\n",
    "record_list = [tuple(row) + (symbol,) + (import_time,) for row in hist_records_map]\n",
    "\n",
    "print(record_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d1a7c322-c74c-41ba-83a6-75f6b9945276",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 87:===================>                                      (1 + 2) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------+------------------+------------------+------------------+--------+---+---+\n",
      "|date|open              |high              |low               |close             |volume  |_7 |_8 |\n",
      "+----+------------------+------------------+------------------+------------------+--------+---+---+\n",
      "|{}  |186.2376030973938 |187.52132261231023|182.9935016661129 |184.73497009277344|82488700|0.0|0.0|\n",
      "|{}  |183.32189307602442|184.973803899141  |182.53573597559208|183.35174560546875|58414500|0.0|0.0|\n",
      "|{}  |181.26198308479132|182.19740286158623|179.99818547387906|181.02316284179688|71983600|0.0|0.0|\n",
      "|{}  |181.10277101042314|181.8690061871609 |179.29163655398915|180.2967071533203 |62303300|0.0|0.0|\n",
      "|{}  |181.20228054123743|184.6951784875532 |180.6151605231867 |184.65536499023438|59144500|0.0|0.0|\n",
      "|{}  |183.02335798455337|184.24735728099782|181.83915700040632|184.23741149902344|42841800|0.0|0.0|\n",
      "|{}  |183.4512616231925 |185.49125530087693|183.02335006640843|185.28228759765625|46792900|0.0|0.0|\n",
      "|{}  |185.6305920580053 |186.13811547472974|182.7248291738061 |184.6852264404297 |49128400|0.0|0.0|\n",
      "|{}  |185.1529277118819 |185.8296205077815 |184.2871739488164 |185.01361083984375|40444700|0.0|0.0|\n",
      "|{}  |181.2719521956393 |183.36170533773222|180.04793765769844|182.7347869873047 |65603000|0.0|0.0|\n",
      "|{}  |180.38628371133655|182.03817940296344|179.42101140332082|181.78939819335938|47317400|0.0|0.0|\n",
      "|{}  |185.18277784532793|188.21791164443712|184.9240508532924 |187.7104034423828 |78005800|0.0|0.0|\n",
      "|{}  |188.4069897270921 |191.01421197583775|187.89948151982946|190.62611389160156|68741000|0.0|0.0|\n",
      "|{}  |191.3625182620258 |194.37774544629707|191.32270476339474|192.94476318359375|60133900|0.0|0.0|\n",
      "|{}  |194.06924615918857|194.79568302432682|192.8850451968675 |194.22845458984375|42355600|0.0|0.0|\n",
      "|{}  |194.46730139362447|195.42262795546443|193.3925647057047 |193.55178833007812|53631300|0.0|0.0|\n",
      "|{}  |194.26827279243787|195.31315691300077|192.1685587692801 |193.223388671875  |54822100|0.0|0.0|\n",
      "|{}  |193.32288994227028|193.81049135303678|191.00424744654802|191.48190307617188|44594000|0.0|0.0|\n",
      "|{}  |191.07392183626405|191.2629979920715 |188.65577567784422|190.7952880859375 |47145600|0.0|0.0|\n",
      "|{}  |190.00913152540437|190.86493945980703|186.55604725857967|187.12326049804688|55859400|0.0|0.0|\n",
      "+----+------------------+------------------+------------------+------------------+--------+---+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"YahooFinanceData\").getOrCreate()\n",
    "\n",
    "# Input data: List of tuples with ticker and start date\n",
    "tickers_data = [\n",
    "    ('AAPL', '2024-01-01'),\n",
    "    ('MSFT', '2024-01-01'),\n",
    "    ('GOOGL', '2024-01-01'),\n",
    "]\n",
    "\n",
    "# Create an RDD from the input data\n",
    "tickers_rdd = spark.sparkContext.parallelize(tickers_data)\n",
    "\n",
    "# General fetch function\n",
    "# def fetch_stock_data(ticker, start_date):\n",
    "#     try:\n",
    "#         # Fetch the stock data using yfinance\n",
    "#         stock_data = yf.Ticker(ticker).history(start=start_date)\n",
    "#         stock_data.reset_index(inplace=True)  # Reset index to get 'Date' as a column\n",
    "        \n",
    "#         # Convert the DataFrame into a list of tuples\n",
    "#         rows = [\n",
    "#             (\n",
    "#                 row['Date'].strftime('%Y-%m-%d') if isinstance(row['Date'], pd.Timestamp) else row['Date'],\n",
    "#                 float(row['Open']) if not np.isnan(row['Open']) else None,\n",
    "#                 float(row['High']) if not np.isnan(row['High']) else None,\n",
    "#                 float(row['Low']) if not np.isnan(row['Low']) else None,\n",
    "#                 float(row['Close']) if not np.isnan(row['Close']) else None,\n",
    "#                 float(row['Volume']) if not np.isnan(row['Volume']) else None,\n",
    "#             )\n",
    "#             for _, row in stock_data.iterrows()\n",
    "#         ]\n",
    "#         return rows\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error fetching data for {ticker}: {e}\")\n",
    "#         return []\n",
    "\n",
    "def fetch_stock_data(ticker, start_date):\n",
    "    try:\n",
    "        # Fetch stock data using yfinance\n",
    "        stock_data = yf.Ticker(ticker).history(start=start_date)\n",
    "        stock_data.reset_index(inplace=True)  # Reset index to include 'Date' as a column\n",
    "        \n",
    "        # Convert all fields from the DataFrame into a list of tuples\n",
    "        data_tuples = [tuple(row) for row in stock_data.itertuples(index=False)]\n",
    "        \n",
    "        return data_tuples\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data for {ticker}: {e}\")\n",
    "        return []\n",
    "\n",
    "# Wrapper function for map\n",
    "def wrapper(params, fetch_fn):\n",
    "    ticker, start_date = params\n",
    "    return fetch_fn(ticker, start_date)\n",
    "\n",
    "# Use map with fetch function as parameter\n",
    "results_rdd = tickers_rdd.map(lambda params: wrapper(params, fetch_stock_data))\n",
    "\n",
    "# Flatten the results RDD\n",
    "flattened_rdd = results_rdd.flatMap(lambda x: x)\n",
    "\n",
    "# Define schema for the final DataFrame\n",
    "schema = [\"date\", \"open\", \"high\", \"low\", \"close\", \"volume\"]\n",
    "\n",
    "# Create a DataFrame from the flattened RDD\n",
    "stock_df = spark.createDataFrame(flattened_rdd, schema=schema)\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "stock_df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c5280dd4-9d20-47e2-ab5e-5bf9dbc1bd9d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('2020-01-02', 71.79987336808419, 72.85661316556887, 71.54538716413333, 72.7960205078125, 135480400.0), ('2020-01-03', 72.02041625768727, 72.8517455706424, 71.86287686024052, 72.0882797241211, 146322800.0), ('2020-01-06', 71.20606216043667, 72.7014846521864, 70.95399466003437, 72.66270446777344, 118387200.0), ('2020-01-07', 72.67239390199568, 72.92930612536188, 72.10040288289755, 72.32096099853516, 108872000.0), ('2020-01-08', 72.02286498225183, 73.78732306103302, 72.02286498225183, 73.48435974121094, 132079200.0)]\n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def fetch_stock_data_as_tuples(ticker, start_date):\n",
    "    try:\n",
    "        # Fetch the stock data using yfinance\n",
    "        stock_data = yf.Ticker(ticker).history(start=start_date)\n",
    "        stock_data.reset_index(inplace=True)  # Reset index to include 'Date' as a column\n",
    "\n",
    "        # Convert the DataFrame into a list of tuples\n",
    "        data_tuples = [\n",
    "            (\n",
    "                row['Date'].strftime('%Y-%m-%d') if isinstance(row['Date'], pd.Timestamp) else row['Date'],\n",
    "                float(row['Open']) if not np.isnan(row['Open']) else None,\n",
    "                float(row['High']) if not np.isnan(row['High']) else None,\n",
    "                float(row['Low']) if not np.isnan(row['Low']) else None,\n",
    "                float(row['Close']) if not np.isnan(row['Close']) else None,\n",
    "                float(row['Volume']) if not np.isnan(row['Volume']) else None,\n",
    "            )\n",
    "            for _, row in stock_data.iterrows()\n",
    "        ]\n",
    "        return data_tuples\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data for {ticker}: {e}\")\n",
    "        return []\n",
    "\n",
    "# Example usage\n",
    "ticker = \"AAPL\"\n",
    "start_date = \"2020-01-01\"\n",
    "stock_tuples = fetch_stock_data_as_tuples(ticker, start_date)\n",
    "\n",
    "# Print the first few tuples\n",
    "print(stock_tuples[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "68996a6b-1320-4a9e-93d8-ed04aa9af57c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(Timestamp('2020-01-02 00:00:00-0500', tz='America/New_York'), 71.79987336808419, 72.85661316556887, 71.54538716413333, 72.7960205078125, 135480400, 0.0, 0.0), (Timestamp('2020-01-03 00:00:00-0500', tz='America/New_York'), 72.02043912432407, 72.85176870122804, 71.8628996768582, 72.08830261230469, 146322800, 0.0, 0.0), (Timestamp('2020-01-06 00:00:00-0500', tz='America/New_York'), 71.20606216043667, 72.7014846521864, 70.95399466003437, 72.66270446777344, 118387200, 0.0, 0.0), (Timestamp('2020-01-07 00:00:00-0500', tz='America/New_York'), 72.67241690140098, 72.92932920607493, 72.10042570127877, 72.32098388671875, 108872000, 0.0, 0.0), (Timestamp('2020-01-08 00:00:00-0500', tz='America/New_York'), 72.022835071623, 73.7872924176362, 72.022835071623, 73.48432922363281, 132079200, 0.0, 0.0)]\n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "\n",
    "def fetch_all_fields_as_tuples(ticker, start_date):\n",
    "    try:\n",
    "        # Fetch stock data using yfinance\n",
    "        stock_data = yf.Ticker(ticker).history(start=start_date)\n",
    "        stock_data.reset_index(inplace=True)  # Reset index to include 'Date' as a column\n",
    "        \n",
    "        # Convert all fields from the DataFrame into a list of tuples\n",
    "        data_tuples = [tuple(row) for row in stock_data.itertuples(index=False)]\n",
    "        \n",
    "        return data_tuples\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data for {ticker}: {e}\")\n",
    "        return []\n",
    "\n",
    "# Example usage\n",
    "ticker = \"AAPL\"\n",
    "start_date = \"2020-01-01\"\n",
    "stock_tuples = fetch_all_fields_as_tuples(ticker, start_date)\n",
    "\n",
    "# Print the first few tuples\n",
    "print(stock_tuples[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5430ed4-ed33-4be7-89c4-b38d0e6fa4be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import yfinance as yf\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"YahooFinanceData\").getOrCreate()\n",
    "\n",
    "# Input data: List of tuples with tickers and start dates\n",
    "tickers_data = [\n",
    "    ('AAPL', '2020-01-01'),\n",
    "    ('MSFT', '2019-01-01'),\n",
    "    ('GOOGL', '2021-01-01'),\n",
    "]\n",
    "\n",
    "# Create an RDD from the input data\n",
    "tickers_rdd = spark.sparkContext.parallelize(tickers_data)\n",
    "\n",
    "# Fetch function to retrieve all stock data fields\n",
    "def fetch_stock_data_all_fields(ticker, start_date):\n",
    "    try:\n",
    "        # Fetch stock data using yfinance\n",
    "        stock_data = yf.Ticker(ticker).history(start=start_date)\n",
    "        stock_data.reset_index(inplace=True)  # Reset index to include 'Date' as a column\n",
    "        \n",
    "        # Convert all rows to tuples\n",
    "        return [tuple(row) for row in stock_data.itertuples(index=False)]\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data for {ticker}: {e}\")\n",
    "        return []\n",
    "\n",
    "# Use map instead of flatMap\n",
    "results_rdd = tickers_rdd.map(lambda params: fetch_stock_data_all_fields(params[0], params[1]))\n",
    "\n",
    "# Collect and print results\n",
    "results = results_rdd.collect()\n",
    "print(results)\n",
    "# for result in results:\n",
    "#     print(result[:3])  # Print the first 3 rows for each ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6dc05701-dba2-46ad-9b12-2f466121554f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 90:===========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(Timestamp('2024-12-02 00:00:00-0500', tz='America/New_York'), 237.27000427246094, 240.7899932861328, 237.16000366210938, 239.58999633789062, 48137100, 0.0, 0.0, 'AAPL', '2024-12-01'), (Timestamp('2024-12-03 00:00:00-0500', tz='America/New_York'), 239.80999755859375, 242.75999450683594, 238.89999389648438, 242.64999389648438, 38861000, 0.0, 0.0, 'AAPL', '2024-12-01'), (Timestamp('2024-12-04 00:00:00-0500', tz='America/New_York'), 242.8699951171875, 244.11000061035156, 241.25, 243.00999450683594, 44383900, 0.0, 0.0, 'AAPL', '2024-12-01')]\n",
      "[(Timestamp('2024-12-02 00:00:00-0500', tz='America/New_York'), 421.57000732421875, 433.0, 421.30999755859375, 430.9800109863281, 20207200, 0.0, 0.0, 'MSFT', '2024-12-01'), (Timestamp('2024-12-03 00:00:00-0500', tz='America/New_York'), 429.8399963378906, 432.4700012207031, 427.739990234375, 431.20001220703125, 18302000, 0.0, 0.0, 'MSFT', '2024-12-01'), (Timestamp('2024-12-04 00:00:00-0500', tz='America/New_York'), 433.0299987792969, 439.6700134277344, 432.6300048828125, 437.4200134277344, 26009400, 0.0, 0.0, 'MSFT', '2024-12-01')]\n",
      "[(Timestamp('2024-12-02 00:00:00-0500', tz='America/New_York'), 168.5767985152317, 171.88300683066288, 168.37703052095557, 171.29368591308594, 23789100, 0.0, 0.0, 'GOOGL', '2024-12-01'), (Timestamp('2024-12-03 00:00:00-0500', tz='America/New_York'), 171.29368202779142, 172.4823069012363, 170.65441531581507, 171.1438446044922, 22248700, 0.0, 0.0, 'GOOGL', '2024-12-01'), (Timestamp('2024-12-04 00:00:00-0500', tz='America/New_York'), 170.95406461457648, 174.70976999200622, 170.86417130279446, 174.17037963867188, 31615100, 0.0, 0.0, 'GOOGL', '2024-12-01')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import yfinance as yf\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"YahooFinanceData\").getOrCreate()\n",
    "\n",
    "# Input data: List of tickers (start_date is fixed as '2024-12-01' for all)\n",
    "tickers_data = [\n",
    "    ('AAPL', '2024-12-01'),\n",
    "    ('MSFT', '2024-12-01'),\n",
    "    ('GOOGL', '2024-12-01'),\n",
    "]\n",
    "\n",
    "# Create an RDD from the input data\n",
    "tickers_rdd = spark.sparkContext.parallelize(tickers_data)\n",
    "\n",
    "# Fetch function to retrieve all stock data fields with ticker and fixed start_date\n",
    "def fetch_stock_data_with_ticker_and_date(ticker, start_date):\n",
    "    try:\n",
    "        # Fetch stock data using yfinance\n",
    "        stock_data = yf.Ticker(ticker).history(start=start_date)\n",
    "        stock_data.reset_index(inplace=True)  # Reset index to include 'Date' as a column\n",
    "        \n",
    "        # Convert all rows to tuples and append ticker and start_date\n",
    "        return [tuple(row) + (ticker, start_date) for row in stock_data.itertuples(index=False)]\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data for {ticker}: {e}\")\n",
    "        return []\n",
    "\n",
    "# Use map to process data in parallel\n",
    "results_rdd = tickers_rdd.map(lambda params: fetch_stock_data_with_ticker_and_date(params[0], params[1]))\n",
    "\n",
    "# Collect and print results\n",
    "results = results_rdd.collect()\n",
    "for ticker_data in results:\n",
    "    print(ticker_data[:3])  # Print the first 3 rows for each ticker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2563b2b2-b2b2-4637-b0ac-1e76e475d50a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 91:===========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------+------------------+------------------+------------------+--------+---------+------------+------+----------+\n",
      "|Date|              Open|              High|               Low|             Close|  Volume|Dividends|Stock Splits|Ticker|Start_Date|\n",
      "+----+------------------+------------------+------------------+------------------+--------+---------+------------+------+----------+\n",
      "|  {}|237.27000427246094| 240.7899932861328|237.16000366210938|239.58999633789062|48137100|      0.0|         0.0|  AAPL|2024-12-01|\n",
      "|  {}|239.80999755859375|242.75999450683594|238.89999389648438|242.64999389648438|38861000|      0.0|         0.0|  AAPL|2024-12-01|\n",
      "|  {}| 242.8699951171875|244.11000061035156|            241.25|243.00999450683594|44383900|      0.0|         0.0|  AAPL|2024-12-01|\n",
      "|  {}|243.99000549316406| 244.5399932861328| 242.1300048828125| 243.0399932861328|40033900|      0.0|         0.0|  AAPL|2024-12-01|\n",
      "|  {}|242.91000366210938| 244.6300048828125| 242.0800018310547|242.83999633789062|36870600|      0.0|         0.0|  AAPL|2024-12-01|\n",
      "+----+------------------+------------------+------------------+------------------+--------+---------+------------+------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import yfinance as yf\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"YahooFinanceData\").getOrCreate()\n",
    "\n",
    "# Input data: List of tickers (start_date is fixed as '2024-12-01' for all)\n",
    "tickers_data = [\n",
    "    ('AAPL', '2024-12-01'),\n",
    "    ('MSFT', '2024-12-01'),\n",
    "    ('GOOGL', '2024-12-01'),\n",
    "]\n",
    "\n",
    "# Create an RDD from the input data\n",
    "tickers_rdd = spark.sparkContext.parallelize(tickers_data)\n",
    "\n",
    "# Fetch function to retrieve all stock data fields with ticker and fixed start_date\n",
    "def fetch_stock_data_with_ticker_and_date(ticker, start_date):\n",
    "    try:\n",
    "        # Fetch stock data using yfinance\n",
    "        stock_data = yf.Ticker(ticker).history(start=start_date)\n",
    "        stock_data.reset_index(inplace=True)  # Reset index to include 'Date' as a column\n",
    "        \n",
    "        # Convert all rows to tuples and append ticker and start_date\n",
    "        return [tuple(row) + (ticker, start_date) for row in stock_data.itertuples(index=False)]\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data for {ticker}: {e}\")\n",
    "        return []\n",
    "\n",
    "# Use flatMap to process data in parallel and collect all rows as tuples\n",
    "results_rdd = tickers_rdd.flatMap(lambda params: fetch_stock_data_with_ticker_and_date(params[0], params[1]))\n",
    "\n",
    "# Collect the data as a list of tuples\n",
    "results = results_rdd.collect()\n",
    "\n",
    "# Define the schema based on the yfinance columns and additional fields\n",
    "columns = [\n",
    "    \"Date\", \"Open\", \"High\", \"Low\", \"Close\", \"Volume\", \"Dividends\", \"Stock Splits\",\n",
    "    \"Ticker\", \"Start_Date\"\n",
    "]\n",
    "\n",
    "# Create a Spark DataFrame\n",
    "df = spark.createDataFrame(results, schema=columns)\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0051d310-8180-48db-82fe-a272a45af39c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 93:===========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+------------------+------------------+------------------+--------+---------+------------+------+----------+\n",
      "|      Date|              Open|              High|               Low|             Close|  Volume|Dividends|Stock Splits|Ticker|Start_Date|\n",
      "+----------+------------------+------------------+------------------+------------------+--------+---------+------------+------+----------+\n",
      "|2024-12-02|237.27000427246094| 240.7899932861328|237.16000366210938|239.58999633789062|48137100|      0.0|         0.0|  AAPL|2024-12-01|\n",
      "|2024-12-03|239.80999755859375|242.75999450683594|238.89999389648438|242.64999389648438|38861000|      0.0|         0.0|  AAPL|2024-12-01|\n",
      "|2024-12-04| 242.8699951171875|244.11000061035156|            241.25|243.00999450683594|44383900|      0.0|         0.0|  AAPL|2024-12-01|\n",
      "|2024-12-05|243.99000549316406| 244.5399932861328| 242.1300048828125| 243.0399932861328|40033900|      0.0|         0.0|  AAPL|2024-12-01|\n",
      "|2024-12-06|242.91000366210938| 244.6300048828125| 242.0800018310547|242.83999633789062|36870600|      0.0|         0.0|  AAPL|2024-12-01|\n",
      "+----------+------------------+------------------+------------------+------------------+--------+---------+------------+------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import yfinance as yf\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"YahooFinanceData\").getOrCreate()\n",
    "\n",
    "# Input data: List of tickers (start_date is fixed as '2024-12-01' for all)\n",
    "tickers_data = [\n",
    "    ('AAPL', '2024-12-01'),\n",
    "    ('MSFT', '2024-12-01'),\n",
    "    ('GOOGL', '2024-12-01'),\n",
    "]\n",
    "\n",
    "# Create an RDD from the input data\n",
    "tickers_rdd = spark.sparkContext.parallelize(tickers_data)\n",
    "\n",
    "# Fetch function to retrieve all stock data fields with ticker and fixed start_date\n",
    "def fetch_stock_data_with_ticker_and_date(ticker, start_date):\n",
    "    try:\n",
    "        # Fetch stock data using yfinance\n",
    "        stock_data = yf.Ticker(ticker).history(start=start_date)\n",
    "        stock_data.reset_index(inplace=True)  # Reset index to include 'Date' as a column\n",
    "        \n",
    "        # Convert 'Date' to string format and append ticker and start_date\n",
    "        result = []\n",
    "        for row in stock_data.itertuples(index=False):\n",
    "            row_date = row[0].strftime('%Y-%m-%d')  # Convert Date to string\n",
    "            result.append(tuple([row_date] + list(row[1:]) + [ticker, start_date]))\n",
    "        \n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data for {ticker}: {e}\")\n",
    "        return []\n",
    "\n",
    "# Use flatMap to process data in parallel and collect all rows as tuples\n",
    "results_rdd = tickers_rdd.flatMap(lambda params: fetch_stock_data_with_ticker_and_date(params[0], params[1]))\n",
    "\n",
    "# Collect the data as a list of tuples\n",
    "results = results_rdd.collect()\n",
    "\n",
    "# Define the schema based on the yfinance columns and additional fields\n",
    "columns = [\n",
    "    \"Date\", \"Open\", \"High\", \"Low\", \"Close\", \"Volume\", \"Dividends\", \"Stock Splits\",\n",
    "    \"Ticker\", \"Start_Date\"\n",
    "]\n",
    "\n",
    "# Create a Spark DataFrame\n",
    "df = spark.createDataFrame(results, schema=columns)\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2039ff18-3704-41a3-bc7a-208b2073b0c5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(Timestamp('2024-12-02 00:00:00-0500', tz='America/New_York'), 237.27000427246094, 240.7899932861328, 237.16000366210938, 239.58999633789062, 48137100, 0.0, 0.0, 'AAPL', '2024-12-01'), (Timestamp('2024-12-03 00:00:00-0500', tz='America/New_York'), 239.80999755859375, 242.75999450683594, 238.89999389648438, 242.64999389648438, 38861000, 0.0, 0.0, 'AAPL', '2024-12-01'), (Timestamp('2024-12-04 00:00:00-0500', tz='America/New_York'), 242.8699951171875, 244.11000061035156, 241.25, 243.00999450683594, 44383900, 0.0, 0.0, 'AAPL', '2024-12-01'), (Timestamp('2024-12-05 00:00:00-0500', tz='America/New_York'), 243.99000549316406, 244.5399932861328, 242.1300048828125, 243.0399932861328, 40033900, 0.0, 0.0, 'AAPL', '2024-12-01'), (Timestamp('2024-12-06 00:00:00-0500', tz='America/New_York'), 242.91000366210938, 244.6300048828125, 242.0800018310547, 242.83999633789062, 36870600, 0.0, 0.0, 'AAPL', '2024-12-01'), (Timestamp('2024-12-09 00:00:00-0500', tz='America/New_York'), 241.8300018310547, 247.24000549316406, 241.75, 246.75, 44649200, 0.0, 0.0, 'AAPL', '2024-12-01'), (Timestamp('2024-12-10 00:00:00-0500', tz='America/New_York'), 246.88999938964844, 248.2100067138672, 245.33999633789062, 247.77000427246094, 36914800, 0.0, 0.0, 'AAPL', '2024-12-01'), (Timestamp('2024-12-11 00:00:00-0500', tz='America/New_York'), 247.9600067138672, 250.8000030517578, 246.25999450683594, 246.49000549316406, 45205800, 0.0, 0.0, 'AAPL', '2024-12-01'), (Timestamp('2024-12-12 00:00:00-0500', tz='America/New_York'), 246.88999938964844, 248.74000549316406, 245.67999267578125, 247.9600067138672, 32777500, 0.0, 0.0, 'AAPL', '2024-12-01'), (Timestamp('2024-12-13 00:00:00-0500', tz='America/New_York'), 247.82000732421875, 249.2899932861328, 246.24000549316406, 248.1300048828125, 33155300, 0.0, 0.0, 'AAPL', '2024-12-01'), (Timestamp('2024-12-16 00:00:00-0500', tz='America/New_York'), 247.99000549316406, 251.3800048828125, 247.64999389648438, 251.0399932861328, 51694800, 0.0, 0.0, 'AAPL', '2024-12-01'), (Timestamp('2024-12-17 00:00:00-0500', tz='America/New_York'), 250.0800018310547, 253.8300018310547, 249.77999877929688, 253.47999572753906, 51356400, 0.0, 0.0, 'AAPL', '2024-12-01'), (Timestamp('2024-12-18 00:00:00-0500', tz='America/New_York'), 252.16000366210938, 254.27999877929688, 247.74000549316406, 248.0500030517578, 56774100, 0.0, 0.0, 'AAPL', '2024-12-01'), (Timestamp('2024-12-19 00:00:00-0500', tz='America/New_York'), 247.5, 252.0, 247.08999633789062, 249.7899932861328, 60882300, 0.0, 0.0, 'AAPL', '2024-12-01'), (Timestamp('2024-12-20 00:00:00-0500', tz='America/New_York'), 248.0399932861328, 255.0, 245.69000244140625, 254.49000549316406, 146890100, 0.0, 0.0, 'AAPL', '2024-12-01'), (Timestamp('2024-12-02 00:00:00-0500', tz='America/New_York'), 421.57000732421875, 433.0, 421.30999755859375, 430.9800109863281, 20207200, 0.0, 0.0, 'MSFT', '2024-12-01'), (Timestamp('2024-12-03 00:00:00-0500', tz='America/New_York'), 429.8399963378906, 432.4700012207031, 427.739990234375, 431.20001220703125, 18302000, 0.0, 0.0, 'MSFT', '2024-12-01'), (Timestamp('2024-12-04 00:00:00-0500', tz='America/New_York'), 433.0299987792969, 439.6700134277344, 432.6300048828125, 437.4200134277344, 26009400, 0.0, 0.0, 'MSFT', '2024-12-01'), (Timestamp('2024-12-05 00:00:00-0500', tz='America/New_York'), 437.9200134277344, 444.6600036621094, 436.1700134277344, 442.6199951171875, 21697800, 0.0, 0.0, 'MSFT', '2024-12-01'), (Timestamp('2024-12-06 00:00:00-0500', tz='America/New_York'), 442.29998779296875, 446.1000061035156, 441.7699890136719, 443.57000732421875, 18821000, 0.0, 0.0, 'MSFT', '2024-12-01'), (Timestamp('2024-12-09 00:00:00-0500', tz='America/New_York'), 442.6000061035156, 448.3299865722656, 440.5, 446.0199890136719, 19144400, 0.0, 0.0, 'MSFT', '2024-12-01'), (Timestamp('2024-12-10 00:00:00-0500', tz='America/New_York'), 444.3900146484375, 449.6199951171875, 441.6000061035156, 443.3299865722656, 18469500, 0.0, 0.0, 'MSFT', '2024-12-01'), (Timestamp('2024-12-11 00:00:00-0500', tz='America/New_York'), 444.04998779296875, 450.3500061035156, 444.04998779296875, 448.989990234375, 19200200, 0.0, 0.0, 'MSFT', '2024-12-01'), (Timestamp('2024-12-12 00:00:00-0500', tz='America/New_York'), 449.1099853515625, 456.1600036621094, 449.1099853515625, 449.55999755859375, 20834800, 0.0, 0.0, 'MSFT', '2024-12-01'), (Timestamp('2024-12-13 00:00:00-0500', tz='America/New_York'), 448.44000244140625, 451.42999267578125, 445.5799865722656, 447.2699890136719, 20177800, 0.0, 0.0, 'MSFT', '2024-12-01'), (Timestamp('2024-12-16 00:00:00-0500', tz='America/New_York'), 447.2699890136719, 452.17999267578125, 445.2799987792969, 451.5899963378906, 23598800, 0.0, 0.0, 'MSFT', '2024-12-01'), (Timestamp('2024-12-17 00:00:00-0500', tz='America/New_York'), 451.010009765625, 455.2900085449219, 449.57000732421875, 454.4599914550781, 22733500, 0.0, 0.0, 'MSFT', '2024-12-01'), (Timestamp('2024-12-18 00:00:00-0500', tz='America/New_York'), 451.32000732421875, 452.6499938964844, 437.0199890136719, 437.3900146484375, 24444500, 0.0, 0.0, 'MSFT', '2024-12-01'), (Timestamp('2024-12-19 00:00:00-0500', tz='America/New_York'), 441.6199951171875, 443.17999267578125, 436.32000732421875, 437.0299987792969, 22963700, 0.0, 0.0, 'MSFT', '2024-12-01'), (Timestamp('2024-12-20 00:00:00-0500', tz='America/New_York'), 433.1099853515625, 443.739990234375, 428.6300048828125, 436.6000061035156, 64235200, 0.0, 0.0, 'MSFT', '2024-12-01'), (Timestamp('2024-12-02 00:00:00-0500', tz='America/New_York'), 168.5767985152317, 171.88300683066288, 168.37703052095557, 171.29368591308594, 23789100, 0.0, 0.0, 'GOOGL', '2024-12-01'), (Timestamp('2024-12-03 00:00:00-0500', tz='America/New_York'), 171.29368202779142, 172.4823069012363, 170.65441531581507, 171.1438446044922, 22248700, 0.0, 0.0, 'GOOGL', '2024-12-01'), (Timestamp('2024-12-04 00:00:00-0500', tz='America/New_York'), 170.95406461457648, 174.70976999200622, 170.86417130279446, 174.17037963867188, 31615100, 0.0, 0.0, 'GOOGL', '2024-12-01'), (Timestamp('2024-12-05 00:00:00-0500', tz='America/New_York'), 175.15925501840792, 175.858450636237, 172.13272486876673, 172.44236755371094, 21356200, 0.0, 0.0, 'GOOGL', '2024-12-01'), (Timestamp('2024-12-06 00:00:00-0500', tz='America/New_York'), 171.83306973436635, 174.8795813358406, 171.6632661685559, 174.510009765625, 21462400, 0.0, 0.0, 'GOOGL', '2024-12-01'), (Timestamp('2024-12-09 00:00:00-0500', tz='America/New_York'), 173.9600067138672, 176.25999450683594, 173.64999389648438, 175.3699951171875, 25389600, 0.2, 0.0, 'GOOGL', '2024-12-01'), (Timestamp('2024-12-10 00:00:00-0500', tz='America/New_York'), 182.85000610351562, 186.36000061035156, 181.0500030517578, 185.1699981689453, 54813000, 0.0, 0.0, 'GOOGL', '2024-12-01'), (Timestamp('2024-12-11 00:00:00-0500', tz='America/New_York'), 185.30999755859375, 195.61000061035156, 184.85000610351562, 195.39999389648438, 67894100, 0.0, 0.0, 'GOOGL', '2024-12-01'), (Timestamp('2024-12-12 00:00:00-0500', tz='America/New_York'), 195.0, 195.17999267578125, 191.7100067138672, 191.9600067138672, 34817500, 0.0, 0.0, 'GOOGL', '2024-12-01'), (Timestamp('2024-12-13 00:00:00-0500', tz='America/New_York'), 191.00999450683594, 192.72999572753906, 189.63999938964844, 189.82000732421875, 25143500, 0.0, 0.0, 'GOOGL', '2024-12-01'), (Timestamp('2024-12-16 00:00:00-0500', tz='America/New_York'), 192.8699951171875, 199.0, 192.6199951171875, 196.66000366210938, 44934900, 0.0, 0.0, 'GOOGL', '2024-12-01'), (Timestamp('2024-12-17 00:00:00-0500', tz='America/New_York'), 197.25, 201.4199981689453, 194.97999572753906, 195.4199981689453, 43504000, 0.0, 0.0, 'GOOGL', '2024-12-01'), (Timestamp('2024-12-18 00:00:00-0500', tz='America/New_York'), 195.22000122070312, 197.0, 187.74000549316406, 188.39999389648438, 34166100, 0.0, 0.0, 'GOOGL', '2024-12-01'), (Timestamp('2024-12-19 00:00:00-0500', tz='America/New_York'), 191.6300048828125, 193.02999877929688, 188.3800048828125, 188.50999450683594, 32265200, 0.0, 0.0, 'GOOGL', '2024-12-01'), (Timestamp('2024-12-20 00:00:00-0500', tz='America/New_York'), 185.77999877929688, 192.88999938964844, 185.22000122070312, 191.41000366210938, 63326300, 0.0, 0.0, 'GOOGL', '2024-12-01')]\n",
      "(Timestamp('2024-12-02 00:00:00-0500', tz='America/New_York'), 237.27000427246094, 240.7899932861328, 237.16000366210938, 239.58999633789062, 48137100, 0.0, 0.0, 'AAPL', '2024-12-01')\n",
      "(Timestamp('2024-12-03 00:00:00-0500', tz='America/New_York'), 239.80999755859375, 242.75999450683594, 238.89999389648438, 242.64999389648438, 38861000, 0.0, 0.0, 'AAPL', '2024-12-01')\n",
      "(Timestamp('2024-12-04 00:00:00-0500', tz='America/New_York'), 242.8699951171875, 244.11000061035156, 241.25, 243.00999450683594, 44383900, 0.0, 0.0, 'AAPL', '2024-12-01')\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import yfinance as yf\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"YahooFinanceData\").getOrCreate()\n",
    "\n",
    "# Input data: List of tickers (start_date is fixed as '2024-12-01' for all)\n",
    "tickers_data = [\n",
    "    ('AAPL', '2024-12-01'),\n",
    "    ('MSFT', '2024-12-01'),\n",
    "    ('GOOGL', '2024-12-01'),\n",
    "]\n",
    "\n",
    "# Create an RDD from the input data\n",
    "tickers_rdd = spark.sparkContext.parallelize(tickers_data)\n",
    "\n",
    "# Fetch function to retrieve all stock data fields with ticker and fixed start_date\n",
    "def fetch_stock_data_with_ticker_and_date(ticker, start_date):\n",
    "    try:\n",
    "        # Fetch stock data using yfinance\n",
    "        stock_data = yf.Ticker(ticker).history(start=start_date)\n",
    "        stock_data.reset_index(inplace=True)  # Reset index to include 'Date' as a column\n",
    "        \n",
    "        # Convert all rows to tuples and append ticker and start_date\n",
    "        return [tuple(row) + (ticker, start_date) for row in stock_data.itertuples(index=False)]\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data for {ticker}: {e}\")\n",
    "        return []\n",
    "\n",
    "# Use flatMap to process data in parallel and collect all rows as a single list of tuples\n",
    "results_rdd = tickers_rdd.flatMap(lambda params: fetch_stock_data_with_ticker_and_date(params[0], params[1]))\n",
    "\n",
    "# Collect the data as a list of tuples\n",
    "results = results_rdd.collect()\n",
    "print(results)\n",
    "\n",
    "# Print the first 3 tuples to check the result\n",
    "for row in results[:3]:\n",
    "    print(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "1175deeb-8a83-47e7-81ab-171bbd59af7a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 141:==========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('2024-12-10 00:00:00-0500', 246.88999938964844, 248.2100067138672, 245.33999633789062, 247.77000427246094, 36914800, 0.0, 0.0, 'AAPL', '2024-12-10'), ('2024-12-11 00:00:00-0500', 247.9600067138672, 250.8000030517578, 246.25999450683594, 246.49000549316406, 45205800, 0.0, 0.0, 'AAPL', '2024-12-10'), ('2024-12-12 00:00:00-0500', 246.88999938964844, 248.74000549316406, 245.67999267578125, 247.9600067138672, 32777500, 0.0, 0.0, 'AAPL', '2024-12-10'), ('2024-12-13 00:00:00-0500', 247.82000732421875, 249.2899932861328, 246.24000549316406, 248.1300048828125, 33155300, 0.0, 0.0, 'AAPL', '2024-12-10'), ('2024-12-16 00:00:00-0500', 247.99000549316406, 251.3800048828125, 247.64999389648438, 251.0399932861328, 51694800, 0.0, 0.0, 'AAPL', '2024-12-10'), ('2024-12-17 00:00:00-0500', 250.0800018310547, 253.8300018310547, 249.77999877929688, 253.47999572753906, 51356400, 0.0, 0.0, 'AAPL', '2024-12-10'), ('2024-12-18 00:00:00-0500', 252.16000366210938, 254.27999877929688, 247.74000549316406, 248.0500030517578, 56774100, 0.0, 0.0, 'AAPL', '2024-12-10'), ('2024-12-19 00:00:00-0500', 247.5, 252.0, 247.08999633789062, 249.7899932861328, 60882300, 0.0, 0.0, 'AAPL', '2024-12-10'), ('2024-12-20 00:00:00-0500', 248.0399932861328, 255.0, 245.69000244140625, 254.49000549316406, 146890100, 0.0, 0.0, 'AAPL', '2024-12-10'), ('2024-12-10 00:00:00-0500', 444.3900146484375, 449.6199951171875, 441.6000061035156, 443.3299865722656, 18469500, 0.0, 0.0, 'MSFT', '2024-12-10'), ('2024-12-11 00:00:00-0500', 444.04998779296875, 450.3500061035156, 444.04998779296875, 448.989990234375, 19200200, 0.0, 0.0, 'MSFT', '2024-12-10'), ('2024-12-12 00:00:00-0500', 449.1099853515625, 456.1600036621094, 449.1099853515625, 449.55999755859375, 20834800, 0.0, 0.0, 'MSFT', '2024-12-10'), ('2024-12-13 00:00:00-0500', 448.44000244140625, 451.42999267578125, 445.5799865722656, 447.2699890136719, 20177800, 0.0, 0.0, 'MSFT', '2024-12-10'), ('2024-12-16 00:00:00-0500', 447.2699890136719, 452.17999267578125, 445.2799987792969, 451.5899963378906, 23598800, 0.0, 0.0, 'MSFT', '2024-12-10'), ('2024-12-17 00:00:00-0500', 451.010009765625, 455.2900085449219, 449.57000732421875, 454.4599914550781, 22733500, 0.0, 0.0, 'MSFT', '2024-12-10'), ('2024-12-18 00:00:00-0500', 451.32000732421875, 452.6499938964844, 437.0199890136719, 437.3900146484375, 24444500, 0.0, 0.0, 'MSFT', '2024-12-10'), ('2024-12-19 00:00:00-0500', 441.6199951171875, 443.17999267578125, 436.32000732421875, 437.0299987792969, 22963700, 0.0, 0.0, 'MSFT', '2024-12-10'), ('2024-12-20 00:00:00-0500', 433.1099853515625, 443.739990234375, 428.6300048828125, 436.6000061035156, 64235200, 0.0, 0.0, 'MSFT', '2024-12-10'), ('2024-12-10 00:00:00-0500', 182.85000610351562, 186.36000061035156, 181.0500030517578, 185.1699981689453, 54813000, 0.0, 0.0, 'GOOGL', '2024-12-10'), ('2024-12-11 00:00:00-0500', 185.30999755859375, 195.61000061035156, 184.85000610351562, 195.39999389648438, 67894100, 0.0, 0.0, 'GOOGL', '2024-12-10'), ('2024-12-12 00:00:00-0500', 195.0, 195.17999267578125, 191.7100067138672, 191.9600067138672, 34817500, 0.0, 0.0, 'GOOGL', '2024-12-10'), ('2024-12-13 00:00:00-0500', 191.00999450683594, 192.72999572753906, 189.63999938964844, 189.82000732421875, 25143500, 0.0, 0.0, 'GOOGL', '2024-12-10'), ('2024-12-16 00:00:00-0500', 192.8699951171875, 199.0, 192.6199951171875, 196.66000366210938, 44934900, 0.0, 0.0, 'GOOGL', '2024-12-10'), ('2024-12-17 00:00:00-0500', 197.25, 201.4199981689453, 194.97999572753906, 195.4199981689453, 43504000, 0.0, 0.0, 'GOOGL', '2024-12-10'), ('2024-12-18 00:00:00-0500', 195.22000122070312, 197.0, 187.74000549316406, 188.39999389648438, 34166100, 0.0, 0.0, 'GOOGL', '2024-12-10'), ('2024-12-19 00:00:00-0500', 191.6300048828125, 193.02999877929688, 188.3800048828125, 188.50999450683594, 32265200, 0.0, 0.0, 'GOOGL', '2024-12-10'), ('2024-12-20 00:00:00-0500', 185.77999877929688, 192.88999938964844, 185.22000122070312, 191.41000366210938, 63326300, 0.0, 0.0, 'GOOGL', '2024-12-10')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import yfinance as yf\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"YahooFinanceData\").getOrCreate()\n",
    "\n",
    "# Input data: List of tickers (start_date is fixed as '2024-12-01' for all)\n",
    "tickers_data = [\n",
    "    ('AAPL', '2024-12-10'),\n",
    "    ('MSFT', '2024-12-10'),\n",
    "    ('GOOGL', '2024-12-10'),\n",
    "]\n",
    "\n",
    "# Create an RDD from the input data\n",
    "tickers_rdd = spark.sparkContext.parallelize(tickers_data)\n",
    "\n",
    "# Fetch function to retrieve all stock data fields with ticker and fixed start_date\n",
    "def fetch_stock_data_with_ticker_and_date(ticker, start_date):\n",
    "    try:\n",
    "        # Fetch stock data using yfinance\n",
    "        stock_data = yf.Ticker(ticker).history(start=start_date)\n",
    "        stock_data.reset_index(inplace=True)  # Reset index to include 'Date' as a column\n",
    "\n",
    "        stock_data['Date'] = stock_data['Date'].dt.strftime('%Y-%m-%d %H:%M:%S%z')\n",
    "        \n",
    "        \n",
    "        # Convert all rows to tuples and append ticker and start_date\n",
    "        return [tuple(row) + (ticker, start_date) for row in stock_data.itertuples(index=False)]\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data for {ticker}: {e}\")\n",
    "        return []\n",
    "\n",
    "# Function to process ticker data and call the fetch function with unpacked params\n",
    "def process_ticker_data(ticker_data):\n",
    "    return fetch_stock_data_with_ticker_and_date(*ticker_data)\n",
    "\n",
    "# Use flatMap to process data in parallel and collect all rows as a single list of tuples\n",
    "results_rdd = tickers_rdd.flatMap(process_ticker_data)\n",
    "\n",
    "# Collect the data as a list of tuples\n",
    "results = results_rdd.collect()\n",
    "print(results)\n",
    "# # Print the first 3 tuples to check the result\n",
    "# for row in results[:10]:\n",
    "#     print(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "338e7d35-b37e-4bd6-a21e-d973310eab39",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(Timestamp('2024-12-11 00:00:00'), 73.0, 73.26000213623047, 71.2699966430664, 71.95999908447266, 22533600, 0.0, 0.0, 'C', '2024-12-11'), (Timestamp('2024-12-12 00:00:00'), 71.86000061035156, 72.33000183105469, 71.41000366210938, 71.43000030517578, 8893000, 0.0, 0.0, 'C', '2024-12-11'), (Timestamp('2024-12-13 00:00:00'), 71.70999908447266, 71.91000366210938, 70.76000213623047, 71.01000213623047, 9819900, 0.0, 0.0, 'C', '2024-12-11'), (Timestamp('2024-12-16 00:00:00'), 71.2699966430664, 71.7699966430664, 70.83000183105469, 71.48999786376953, 11833800, 0.0, 0.0, 'C', '2024-12-11'), (Timestamp('2024-12-17 00:00:00'), 70.9000015258789, 71.3499984741211, 70.80000305175781, 71.12000274658203, 12639000, 0.0, 0.0, 'C', '2024-12-11'), (Timestamp('2024-12-18 00:00:00'), 71.18000030517578, 71.47000122070312, 67.91999816894531, 68.12000274658203, 18028000, 0.0, 0.0, 'C', '2024-12-11'), (Timestamp('2024-12-19 00:00:00'), 69.20999908447266, 69.83999633789062, 68.31999969482422, 68.41999816894531, 13464300, 0.0, 0.0, 'C', '2024-12-11'), (Timestamp('2024-12-20 00:00:00'), 68.29000091552734, 70.26000213623047, 68.02999877929688, 69.19000244140625, 27718800, 0.0, 0.0, 'C', '2024-12-11')]\n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "\n",
    "def fetch_stock_data_with_ticker_and_date(ticker, start_date):\n",
    "    try:\n",
    "        # Fetch stock data using yfinance\n",
    "        stock_data = yf.Ticker(ticker).history(start=start_date)\n",
    "        stock_data.reset_index(inplace=True)  # Reset index to include 'Date' as a column\n",
    "        \n",
    "        if stock_data['Date'].dt.tz is not None:\n",
    "            stock_data['Date'] = stock_data['Date'].dt.tz_localize(None)\n",
    "        # Convert all rows to tuples, include ticker and start_date, dynamically using all fields\n",
    "        return [\n",
    "            tuple(row) + (ticker, start_date)  # Add ticker and start_date to the tuple\n",
    "            for row in stock_data.itertuples(index=False)\n",
    "        ]\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data for {ticker}: {e}\")\n",
    "        return []\n",
    "\n",
    "    \n",
    "x=fetch_stock_data_with_ticker_and_date('C', '2024-12-11')\n",
    "print(x)\n",
    "                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "5e29cc9c-a208-4bde-a560-d738ca3d2e72",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+------+------+------+------+---------+---------+------------+------+----------+\n",
      "|Date                    |Open  |High  |Low   |Close |Volume   |Dividends|Stock Splits|Ticker|Start Date|\n",
      "+------------------------+------+------+------+------+---------+---------+------------+------+----------+\n",
      "|2024-12-02 00:00:00-0500|237.27|240.79|237.16|239.59|48137100 |0.0      |0.0         |AAPL  |2024-12-01|\n",
      "|2024-12-03 00:00:00-0500|239.81|242.76|238.9 |242.65|38861000 |0.0      |0.0         |AAPL  |2024-12-01|\n",
      "|2024-12-04 00:00:00-0500|242.87|244.11|241.25|243.01|44383900 |0.0      |0.0         |AAPL  |2024-12-01|\n",
      "|2024-12-05 00:00:00-0500|243.99|244.54|242.13|243.04|40033900 |0.0      |0.0         |AAPL  |2024-12-01|\n",
      "|2024-12-06 00:00:00-0500|242.91|244.63|242.08|242.84|36870600 |0.0      |0.0         |AAPL  |2024-12-01|\n",
      "|2024-12-09 00:00:00-0500|241.83|247.24|241.75|246.75|44649200 |0.0      |0.0         |AAPL  |2024-12-01|\n",
      "|2024-12-10 00:00:00-0500|246.89|248.21|245.34|247.77|36914800 |0.0      |0.0         |AAPL  |2024-12-01|\n",
      "|2024-12-11 00:00:00-0500|247.96|250.8 |246.26|246.49|45205800 |0.0      |0.0         |AAPL  |2024-12-01|\n",
      "|2024-12-12 00:00:00-0500|246.89|248.74|245.68|247.96|32777500 |0.0      |0.0         |AAPL  |2024-12-01|\n",
      "|2024-12-13 00:00:00-0500|247.82|249.29|246.24|248.13|33155300 |0.0      |0.0         |AAPL  |2024-12-01|\n",
      "|2024-12-16 00:00:00-0500|247.99|251.38|247.65|251.04|51694800 |0.0      |0.0         |AAPL  |2024-12-01|\n",
      "|2024-12-17 00:00:00-0500|250.08|253.83|249.78|253.48|51356400 |0.0      |0.0         |AAPL  |2024-12-01|\n",
      "|2024-12-18 00:00:00-0500|252.16|254.28|247.74|248.05|56774100 |0.0      |0.0         |AAPL  |2024-12-01|\n",
      "|2024-12-19 00:00:00-0500|247.5 |252.0 |247.09|249.79|60882300 |0.0      |0.0         |AAPL  |2024-12-01|\n",
      "|2024-12-20 00:00:00-0500|248.04|255.0 |245.69|254.49|146890100|0.0      |0.0         |AAPL  |2024-12-01|\n",
      "|2024-12-02 00:00:00-0500|421.57|433.0 |421.31|430.98|20207200 |0.0      |0.0         |MSFT  |2024-12-01|\n",
      "|2024-12-03 00:00:00-0500|429.84|432.47|427.74|431.2 |18302000 |0.0      |0.0         |MSFT  |2024-12-01|\n",
      "|2024-12-04 00:00:00-0500|433.03|439.67|432.63|437.42|26009400 |0.0      |0.0         |MSFT  |2024-12-01|\n",
      "|2024-12-05 00:00:00-0500|437.92|444.66|436.17|442.62|21697800 |0.0      |0.0         |MSFT  |2024-12-01|\n",
      "|2024-12-06 00:00:00-0500|442.3 |446.1 |441.77|443.57|18821000 |0.0      |0.0         |MSFT  |2024-12-01|\n",
      "+------------------------+------+------+------+------+---------+---------+------------+------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType, TimestampType, IntegerType\n",
    "import yfinance as yf\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"YahooFinanceData\").getOrCreate()\n",
    "\n",
    "# Define the schema for the DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"Date\", StringType(), True),\n",
    "    StructField(\"Open\", FloatType(), True),\n",
    "    StructField(\"High\", FloatType(), True),\n",
    "    StructField(\"Low\", FloatType(), True),\n",
    "    StructField(\"Close\", FloatType(), True),\n",
    "    StructField(\"Volume\", IntegerType(), True),\n",
    "    StructField(\"Dividends\", FloatType(), True),\n",
    "    StructField(\"Stock Splits\", FloatType(), True),\n",
    "    StructField(\"Ticker\", StringType(), True),\n",
    "    StructField(\"Start Date\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Input data: List of tickers and start date\n",
    "tickers_data = [\n",
    "    ('AAPL', '2024-12-01'),\n",
    "    ('MSFT', '2024-12-01'),\n",
    "    ('GOOGL', '2024-12-01')\n",
    "]\n",
    "\n",
    "# Fetch function to retrieve stock data\n",
    "def fetch_stock_data_with_ticker_and_date(ticker, start_date):\n",
    "    try:\n",
    "        # Fetch stock data using yfinance\n",
    "        stock_data = yf.Ticker(ticker).history(start=start_date)\n",
    "        stock_data.reset_index(inplace=True)  # Reset index to include 'Date' as a column\n",
    "        \n",
    "        \n",
    "        stock_data['Date'] = stock_data['Date'].dt.strftime('%Y-%m-%d %H:%M:%S%z')\n",
    "        \n",
    "        # Convert all rows to tuples, include ticker and start_date\n",
    "        return [\n",
    "            tuple(row) + (ticker, start_date)  # Add ticker and start_date to the tuple\n",
    "            for row in stock_data.itertuples(index=False)\n",
    "        ]\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data for {ticker}: {e}\")\n",
    "        return []\n",
    "\n",
    "# Create an RDD from the tickers data\n",
    "tickers_rdd = spark.sparkContext.parallelize(tickers_data)\n",
    "\n",
    "# Fetch the stock data for all tickers using map and collect the results\n",
    "results_rdd = tickers_rdd.flatMap(lambda params: fetch_stock_data_with_ticker_and_date(params[0], params[1]))\n",
    "\n",
    "# Collect the results as a list of tuples\n",
    "results = results_rdd.collect()\n",
    "\n",
    "# Create a Spark DataFrame from the results\n",
    "df = spark.createDataFrame(results, schema)\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "26d4c836-9cca-4a70-96c9-6929e3c4871e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(Timestamp('2024-12-02 00:00:00-0500', tz='America/New_York'), 237.27000427246094, 240.7899932861328, 237.16000366210938, 239.58999633789062, 48137100, 0.0, 0.0, 'AAPL', '2024-12-01'), (Timestamp('2024-12-03 00:00:00-0500', tz='America/New_York'), 239.80999755859375, 242.75999450683594, 238.89999389648438, 242.64999389648438, 38861000, 0.0, 0.0, 'AAPL', '2024-12-01'), (Timestamp('2024-12-04 00:00:00-0500', tz='America/New_York'), 242.8699951171875, 244.11000061035156, 241.25, 243.00999450683594, 44383900, 0.0, 0.0, 'AAPL', '2024-12-01'), (Timestamp('2024-12-05 00:00:00-0500', tz='America/New_York'), 243.99000549316406, 244.5399932861328, 242.1300048828125, 243.0399932861328, 40033900, 0.0, 0.0, 'AAPL', '2024-12-01'), (Timestamp('2024-12-06 00:00:00-0500', tz='America/New_York'), 242.91000366210938, 244.6300048828125, 242.0800018310547, 242.83999633789062, 36870600, 0.0, 0.0, 'AAPL', '2024-12-01'), (Timestamp('2024-12-09 00:00:00-0500', tz='America/New_York'), 241.8300018310547, 247.24000549316406, 241.75, 246.75, 44649200, 0.0, 0.0, 'AAPL', '2024-12-01'), (Timestamp('2024-12-10 00:00:00-0500', tz='America/New_York'), 246.88999938964844, 248.2100067138672, 245.33999633789062, 247.77000427246094, 36914800, 0.0, 0.0, 'AAPL', '2024-12-01'), (Timestamp('2024-12-11 00:00:00-0500', tz='America/New_York'), 247.9600067138672, 250.8000030517578, 246.25999450683594, 246.49000549316406, 45205800, 0.0, 0.0, 'AAPL', '2024-12-01'), (Timestamp('2024-12-12 00:00:00-0500', tz='America/New_York'), 246.88999938964844, 248.74000549316406, 245.67999267578125, 247.9600067138672, 32777500, 0.0, 0.0, 'AAPL', '2024-12-01'), (Timestamp('2024-12-13 00:00:00-0500', tz='America/New_York'), 247.82000732421875, 249.2899932861328, 246.24000549316406, 248.1300048828125, 33155300, 0.0, 0.0, 'AAPL', '2024-12-01'), (Timestamp('2024-12-16 00:00:00-0500', tz='America/New_York'), 247.99000549316406, 251.3800048828125, 247.64999389648438, 251.0399932861328, 51694800, 0.0, 0.0, 'AAPL', '2024-12-01'), (Timestamp('2024-12-17 00:00:00-0500', tz='America/New_York'), 250.0800018310547, 253.8300018310547, 249.77999877929688, 253.47999572753906, 51356400, 0.0, 0.0, 'AAPL', '2024-12-01'), (Timestamp('2024-12-18 00:00:00-0500', tz='America/New_York'), 252.16000366210938, 254.27999877929688, 247.74000549316406, 248.0500030517578, 56774100, 0.0, 0.0, 'AAPL', '2024-12-01'), (Timestamp('2024-12-19 00:00:00-0500', tz='America/New_York'), 247.5, 252.0, 247.08999633789062, 249.7899932861328, 60882300, 0.0, 0.0, 'AAPL', '2024-12-01'), (Timestamp('2024-12-20 00:00:00-0500', tz='America/New_York'), 248.0399932861328, 255.0, 245.69000244140625, 254.49000549316406, 146890100, 0.0, 0.0, 'AAPL', '2024-12-01'), (Timestamp('2024-12-02 00:00:00-0500', tz='America/New_York'), 421.57000732421875, 433.0, 421.30999755859375, 430.9800109863281, 20207200, 0.0, 0.0, 'MSFT', '2024-12-01'), (Timestamp('2024-12-03 00:00:00-0500', tz='America/New_York'), 429.8399963378906, 432.4700012207031, 427.739990234375, 431.20001220703125, 18302000, 0.0, 0.0, 'MSFT', '2024-12-01'), (Timestamp('2024-12-04 00:00:00-0500', tz='America/New_York'), 433.0299987792969, 439.6700134277344, 432.6300048828125, 437.4200134277344, 26009400, 0.0, 0.0, 'MSFT', '2024-12-01'), (Timestamp('2024-12-05 00:00:00-0500', tz='America/New_York'), 437.9200134277344, 444.6600036621094, 436.1700134277344, 442.6199951171875, 21697800, 0.0, 0.0, 'MSFT', '2024-12-01'), (Timestamp('2024-12-06 00:00:00-0500', tz='America/New_York'), 442.29998779296875, 446.1000061035156, 441.7699890136719, 443.57000732421875, 18821000, 0.0, 0.0, 'MSFT', '2024-12-01'), (Timestamp('2024-12-09 00:00:00-0500', tz='America/New_York'), 442.6000061035156, 448.3299865722656, 440.5, 446.0199890136719, 19144400, 0.0, 0.0, 'MSFT', '2024-12-01'), (Timestamp('2024-12-10 00:00:00-0500', tz='America/New_York'), 444.3900146484375, 449.6199951171875, 441.6000061035156, 443.3299865722656, 18469500, 0.0, 0.0, 'MSFT', '2024-12-01'), (Timestamp('2024-12-11 00:00:00-0500', tz='America/New_York'), 444.04998779296875, 450.3500061035156, 444.04998779296875, 448.989990234375, 19200200, 0.0, 0.0, 'MSFT', '2024-12-01'), (Timestamp('2024-12-12 00:00:00-0500', tz='America/New_York'), 449.1099853515625, 456.1600036621094, 449.1099853515625, 449.55999755859375, 20834800, 0.0, 0.0, 'MSFT', '2024-12-01'), (Timestamp('2024-12-13 00:00:00-0500', tz='America/New_York'), 448.44000244140625, 451.42999267578125, 445.5799865722656, 447.2699890136719, 20177800, 0.0, 0.0, 'MSFT', '2024-12-01'), (Timestamp('2024-12-16 00:00:00-0500', tz='America/New_York'), 447.2699890136719, 452.17999267578125, 445.2799987792969, 451.5899963378906, 23598800, 0.0, 0.0, 'MSFT', '2024-12-01'), (Timestamp('2024-12-17 00:00:00-0500', tz='America/New_York'), 451.010009765625, 455.2900085449219, 449.57000732421875, 454.4599914550781, 22733500, 0.0, 0.0, 'MSFT', '2024-12-01'), (Timestamp('2024-12-18 00:00:00-0500', tz='America/New_York'), 451.32000732421875, 452.6499938964844, 437.0199890136719, 437.3900146484375, 24444500, 0.0, 0.0, 'MSFT', '2024-12-01'), (Timestamp('2024-12-19 00:00:00-0500', tz='America/New_York'), 441.6199951171875, 443.17999267578125, 436.32000732421875, 437.0299987792969, 22963700, 0.0, 0.0, 'MSFT', '2024-12-01'), (Timestamp('2024-12-20 00:00:00-0500', tz='America/New_York'), 433.1099853515625, 443.739990234375, 428.6300048828125, 436.6000061035156, 64235200, 0.0, 0.0, 'MSFT', '2024-12-01'), (Timestamp('2024-12-02 00:00:00-0500', tz='America/New_York'), 168.5767985152317, 171.88300683066288, 168.37703052095557, 171.29368591308594, 23789100, 0.0, 0.0, 'GOOGL', '2024-12-01'), (Timestamp('2024-12-03 00:00:00-0500', tz='America/New_York'), 171.29368202779142, 172.4823069012363, 170.65441531581507, 171.1438446044922, 22248700, 0.0, 0.0, 'GOOGL', '2024-12-01'), (Timestamp('2024-12-04 00:00:00-0500', tz='America/New_York'), 170.95406461457648, 174.70976999200622, 170.86417130279446, 174.17037963867188, 31615100, 0.0, 0.0, 'GOOGL', '2024-12-01'), (Timestamp('2024-12-05 00:00:00-0500', tz='America/New_York'), 175.15925501840792, 175.858450636237, 172.13272486876673, 172.44236755371094, 21356200, 0.0, 0.0, 'GOOGL', '2024-12-01'), (Timestamp('2024-12-06 00:00:00-0500', tz='America/New_York'), 171.83306973436635, 174.8795813358406, 171.6632661685559, 174.510009765625, 21462400, 0.0, 0.0, 'GOOGL', '2024-12-01'), (Timestamp('2024-12-09 00:00:00-0500', tz='America/New_York'), 173.9600067138672, 176.25999450683594, 173.64999389648438, 175.3699951171875, 25389600, 0.2, 0.0, 'GOOGL', '2024-12-01'), (Timestamp('2024-12-10 00:00:00-0500', tz='America/New_York'), 182.85000610351562, 186.36000061035156, 181.0500030517578, 185.1699981689453, 54813000, 0.0, 0.0, 'GOOGL', '2024-12-01'), (Timestamp('2024-12-11 00:00:00-0500', tz='America/New_York'), 185.30999755859375, 195.61000061035156, 184.85000610351562, 195.39999389648438, 67894100, 0.0, 0.0, 'GOOGL', '2024-12-01'), (Timestamp('2024-12-12 00:00:00-0500', tz='America/New_York'), 195.0, 195.17999267578125, 191.7100067138672, 191.9600067138672, 34817500, 0.0, 0.0, 'GOOGL', '2024-12-01'), (Timestamp('2024-12-13 00:00:00-0500', tz='America/New_York'), 191.00999450683594, 192.72999572753906, 189.63999938964844, 189.82000732421875, 25143500, 0.0, 0.0, 'GOOGL', '2024-12-01'), (Timestamp('2024-12-16 00:00:00-0500', tz='America/New_York'), 192.8699951171875, 199.0, 192.6199951171875, 196.66000366210938, 44934900, 0.0, 0.0, 'GOOGL', '2024-12-01'), (Timestamp('2024-12-17 00:00:00-0500', tz='America/New_York'), 197.25, 201.4199981689453, 194.97999572753906, 195.4199981689453, 43504000, 0.0, 0.0, 'GOOGL', '2024-12-01'), (Timestamp('2024-12-18 00:00:00-0500', tz='America/New_York'), 195.22000122070312, 197.0, 187.74000549316406, 188.39999389648438, 34166100, 0.0, 0.0, 'GOOGL', '2024-12-01'), (Timestamp('2024-12-19 00:00:00-0500', tz='America/New_York'), 191.6300048828125, 193.02999877929688, 188.3800048828125, 188.50999450683594, 32265200, 0.0, 0.0, 'GOOGL', '2024-12-01'), (Timestamp('2024-12-20 00:00:00-0500', tz='America/New_York'), 185.77999877929688, 192.88999938964844, 185.22000122070312, 191.41000366210938, 63326300, 0.0, 0.0, 'GOOGL', '2024-12-01')]\n"
     ]
    },
    {
     "ename": "PySparkTypeError",
     "evalue": "[CANNOT_ACCEPT_OBJECT_IN_TYPE] `TimestampType()` can not accept object `2024-12-02 00:00:00-05:00` in type `Timestamp`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPySparkTypeError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[103], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Create a StructType object\u001b[39;00m\n\u001b[1;32m     21\u001b[0m schema \u001b[38;5;241m=\u001b[39m StructType(new_schema)\n\u001b[0;32m---> 23\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresults\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Show the DataFrame\u001b[39;00m\n\u001b[1;32m     26\u001b[0m df\u001b[38;5;241m.\u001b[39mshow(truncate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/workspace/finalytics_spark/.venv/lib/python3.10/site-packages/pyspark/sql/session.py:1443\u001b[0m, in \u001b[0;36mSparkSession.createDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m   1438\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_pandas \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, pd\u001b[38;5;241m.\u001b[39mDataFrame):\n\u001b[1;32m   1439\u001b[0m     \u001b[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001b[39;00m\n\u001b[1;32m   1440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(SparkSession, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mcreateDataFrame(  \u001b[38;5;66;03m# type: ignore[call-overload]\u001b[39;00m\n\u001b[1;32m   1441\u001b[0m         data, schema, samplingRatio, verifySchema\n\u001b[1;32m   1442\u001b[0m     )\n\u001b[0;32m-> 1443\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_dataframe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1444\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverifySchema\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1445\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/finalytics_spark/.venv/lib/python3.10/site-packages/pyspark/sql/session.py:1485\u001b[0m, in \u001b[0;36mSparkSession._create_dataframe\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m   1483\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_createFromRDD(data\u001b[38;5;241m.\u001b[39mmap(prepare), schema, samplingRatio)\n\u001b[1;32m   1484\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1485\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_createFromLocal\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprepare\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1486\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1487\u001b[0m jrdd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mSerDeUtil\u001b[38;5;241m.\u001b[39mtoJavaArray(rdd\u001b[38;5;241m.\u001b[39m_to_java_object_rdd())\n",
      "File \u001b[0;32m/workspace/finalytics_spark/.venv/lib/python3.10/site-packages/pyspark/sql/session.py:1090\u001b[0m, in \u001b[0;36mSparkSession._createFromLocal\u001b[0;34m(self, data, schema)\u001b[0m\n\u001b[1;32m   1088\u001b[0m \u001b[38;5;66;03m# make sure data could consumed multiple times\u001b[39;00m\n\u001b[1;32m   1089\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m-> 1090\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1092\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(schema, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[1;32m   1093\u001b[0m     struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inferSchemaFromList(data, names\u001b[38;5;241m=\u001b[39mschema)\n",
      "File \u001b[0;32m/workspace/finalytics_spark/.venv/lib/python3.10/site-packages/pyspark/sql/session.py:1459\u001b[0m, in \u001b[0;36mSparkSession._create_dataframe.<locals>.prepare\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   1457\u001b[0m \u001b[38;5;129m@no_type_check\u001b[39m\n\u001b[1;32m   1458\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprepare\u001b[39m(obj):\n\u001b[0;32m-> 1459\u001b[0m     \u001b[43mverify_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "File \u001b[0;32m/workspace/finalytics_spark/.venv/lib/python3.10/site-packages/pyspark/sql/types.py:2201\u001b[0m, in \u001b[0;36m_make_type_verifier.<locals>.verify\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   2199\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mverify\u001b[39m(obj: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2200\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m verify_nullability(obj):\n\u001b[0;32m-> 2201\u001b[0m         \u001b[43mverify_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/finalytics_spark/.venv/lib/python3.10/site-packages/pyspark/sql/types.py:2174\u001b[0m, in \u001b[0;36m_make_type_verifier.<locals>.verify_struct\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   2164\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m PySparkValueError(\n\u001b[1;32m   2165\u001b[0m             error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLENGTH_SHOULD_BE_THE_SAME\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2166\u001b[0m             message_parameters\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2171\u001b[0m             },\n\u001b[1;32m   2172\u001b[0m         )\n\u001b[1;32m   2173\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m v, (_, verifier) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(obj, verifiers):\n\u001b[0;32m-> 2174\u001b[0m         \u001b[43mverifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2175\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(obj, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__dict__\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   2176\u001b[0m     d \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\n",
      "File \u001b[0;32m/workspace/finalytics_spark/.venv/lib/python3.10/site-packages/pyspark/sql/types.py:2201\u001b[0m, in \u001b[0;36m_make_type_verifier.<locals>.verify\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   2199\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mverify\u001b[39m(obj: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2200\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m verify_nullability(obj):\n\u001b[0;32m-> 2201\u001b[0m         \u001b[43mverify_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/finalytics_spark/.venv/lib/python3.10/site-packages/pyspark/sql/types.py:2195\u001b[0m, in \u001b[0;36m_make_type_verifier.<locals>.verify_default\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   2193\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mverify_default\u001b[39m(obj: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2194\u001b[0m     assert_acceptable_types(obj)\n\u001b[0;32m-> 2195\u001b[0m     \u001b[43mverify_acceptable_types\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/finalytics_spark/.venv/lib/python3.10/site-packages/pyspark/sql/types.py:2020\u001b[0m, in \u001b[0;36m_make_type_verifier.<locals>.verify_acceptable_types\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   2017\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mverify_acceptable_types\u001b[39m(obj: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2018\u001b[0m     \u001b[38;5;66;03m# subclass of them can not be fromInternal in JVM\u001b[39;00m\n\u001b[1;32m   2019\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(obj) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m _acceptable_types[_type]:\n\u001b[0;32m-> 2020\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m   2021\u001b[0m             error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCANNOT_ACCEPT_OBJECT_IN_TYPE\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2022\u001b[0m             message_parameters\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m   2023\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(dataType),\n\u001b[1;32m   2024\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobj_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(obj),\n\u001b[1;32m   2025\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobj_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(obj)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m,\n\u001b[1;32m   2026\u001b[0m             },\n\u001b[1;32m   2027\u001b[0m         )\n",
      "\u001b[0;31mPySparkTypeError\u001b[0m: [CANNOT_ACCEPT_OBJECT_IN_TYPE] `TimestampType()` can not accept object `2024-12-02 00:00:00-05:00` in type `Timestamp`."
     ]
    }
   ],
   "source": [
    "# from pyspark.sql.types import StructType, StructField, StringType, FloatType, TimestampType\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType, TimestampType, IntegerType\n",
    "\n",
    "\n",
    "print(results)\n",
    "# Define the schema for the DataFrame\n",
    "new_schema = [\n",
    "    StructField(\"Date\", TimestampType(), True),        # 'Date' column as Timestamp\n",
    "    StructField(\"Open\", FloatType(), True),            # 'Open' column as Float\n",
    "    StructField(\"High\", FloatType(), True),            # 'High' column as Float\n",
    "    StructField(\"Low\", FloatType(), True),             # 'Low' column as Float\n",
    "    StructField(\"Close\", FloatType(), True),           # 'Close' column as Float\n",
    "    StructField(\"Volume\", IntegerType(), True),              # 'Volume' column as Float\n",
    "    StructField(\"Dividends\", FloatType(), True),       # 'Dividends' column as Float\n",
    "    StructField(\"Stock Splits\", FloatType(), True),    # 'Stock Splits' column as Float\n",
    "    StructField(\"Ticker\", StringType(), True),         # 'Ticker' column as String\n",
    "    StructField(\"Start Date\", StringType(), True)      # 'Start Date' column as String\n",
    "]\n",
    "\n",
    "# Create a StructType object\n",
    "schema = StructType(new_schema)\n",
    "\n",
    "df = spark.createDataFrame(results, schema)\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "5411c16e-c7b6-4a7c-81fc-c5e3e404bbbf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'StructField' object has no attribute 'encode'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[104], line 41\u001b[0m\n\u001b[1;32m     27\u001b[0m new_schema \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     28\u001b[0m     StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m\"\u001b[39m, TimestampType(), \u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m     29\u001b[0m     StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOpen\u001b[39m\u001b[38;5;124m\"\u001b[39m, FloatType(), \u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     37\u001b[0m     StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStart Date\u001b[39m\u001b[38;5;124m\"\u001b[39m, StringType(), \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     38\u001b[0m ]\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Create a DataFrame from the converted results and schema\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconverted_results\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_schema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Show the DataFrame\u001b[39;00m\n\u001b[1;32m     44\u001b[0m df\u001b[38;5;241m.\u001b[39mshow(truncate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/workspace/finalytics_spark/.venv/lib/python3.10/site-packages/pyspark/sql/session.py:1394\u001b[0m, in \u001b[0;36mSparkSession.createDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m   1391\u001b[0m     schema \u001b[38;5;241m=\u001b[39m cast(Union[AtomicType, StructType, \u001b[38;5;28mstr\u001b[39m], _parse_datatype_string(schema))\n\u001b[1;32m   1392\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(schema, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[1;32m   1393\u001b[0m     \u001b[38;5;66;03m# Must re-encode any unicode strings to be consistent with StructField names\u001b[39;00m\n\u001b[0;32m-> 1394\u001b[0m     schema \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m schema]\n\u001b[1;32m   1396\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1397\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/finalytics_spark/.venv/lib/python3.10/site-packages/pyspark/sql/session.py:1394\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1391\u001b[0m     schema \u001b[38;5;241m=\u001b[39m cast(Union[AtomicType, StructType, \u001b[38;5;28mstr\u001b[39m], _parse_datatype_string(schema))\n\u001b[1;32m   1392\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(schema, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[1;32m   1393\u001b[0m     \u001b[38;5;66;03m# Must re-encode any unicode strings to be consistent with StructField names\u001b[39;00m\n\u001b[0;32m-> 1394\u001b[0m     schema \u001b[38;5;241m=\u001b[39m [\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m schema]\n\u001b[1;32m   1396\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1397\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'StructField' object has no attribute 'encode'"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType, TimestampType\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"YahooFinanceData\").getOrCreate()\n",
    "\n",
    "# Example result list containing tuples (with timezone info)\n",
    "results = [\n",
    "    ('2024-12-01 00:00:00-05:00', 145.22, 146.40, 144.50, 145.50, 1200000.0, 0.0, 0.0, 'AAPL', '2024-12-01'),\n",
    "    ('2024-12-02 00:00:00-05:00', 146.50, 147.60, 145.70, 146.80, 1100000.0, 0.0, 0.0, 'AAPL', '2024-12-01'),\n",
    "    ('2024-12-01 00:00:00-05:00', 101.00, 102.20, 100.50, 101.80, 1000000.0, 0.0, 0.0, 'MSFT', '2024-12-01')\n",
    "]\n",
    "\n",
    "# Convert 'Date' field (with timezone) to timestamp without timezone\n",
    "def convert_to_timestamp(date_str):\n",
    "    # Remove the timezone part and convert the string to datetime object\n",
    "    return datetime.strptime(date_str.split(' ')[0], \"%Y-%m-%d\")\n",
    "\n",
    "# Create a new list with the converted date\n",
    "converted_results = [\n",
    "    (convert_to_timestamp(row[0]), *row[1:]) for row in results\n",
    "]\n",
    "\n",
    "# Define the schema for the DataFrame\n",
    "new_schema = [\n",
    "    StructField(\"Date\", TimestampType(), True),\n",
    "    StructField(\"Open\", FloatType(), True),\n",
    "    StructField(\"High\", FloatType(), True),\n",
    "    StructField(\"Low\", FloatType(), True),\n",
    "    StructField(\"Close\", FloatType(), True),\n",
    "    StructField(\"Volume\", FloatType(), True),\n",
    "    StructField(\"Dividends\", FloatType(), True),\n",
    "    StructField(\"Stock Splits\", FloatType(), True),\n",
    "    StructField(\"Ticker\", StringType(), True),\n",
    "    StructField(\"Start Date\", StringType(), True)\n",
    "]\n",
    "\n",
    "# Create a DataFrame from the converted results and schema\n",
    "df = spark.createDataFrame(converted_results, schema=new_schema)\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "5d185b5c-9b63-4cf3-b641-5669b4616315",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+------+------+------+------+---------+---------+------------+------+----------+\n",
      "|Date                    |Open  |High  |Low   |Close |Volume   |Dividends|Stock Splits|Ticker|Start Date|\n",
      "+------------------------+------+------+------+------+---------+---------+------------+------+----------+\n",
      "|2024-12-02 00:00:00-0500|237.27|240.79|237.16|239.59|48137100 |0.0      |0.0         |AAPL  |2024-12-01|\n",
      "|2024-12-03 00:00:00-0500|239.81|242.76|238.9 |242.65|38861000 |0.0      |0.0         |AAPL  |2024-12-01|\n",
      "|2024-12-04 00:00:00-0500|242.87|244.11|241.25|243.01|44383900 |0.0      |0.0         |AAPL  |2024-12-01|\n",
      "|2024-12-05 00:00:00-0500|243.99|244.54|242.13|243.04|40033900 |0.0      |0.0         |AAPL  |2024-12-01|\n",
      "|2024-12-06 00:00:00-0500|242.91|244.63|242.08|242.84|36870600 |0.0      |0.0         |AAPL  |2024-12-01|\n",
      "|2024-12-09 00:00:00-0500|241.83|247.24|241.75|246.75|44649200 |0.0      |0.0         |AAPL  |2024-12-01|\n",
      "|2024-12-10 00:00:00-0500|246.89|248.21|245.34|247.77|36914800 |0.0      |0.0         |AAPL  |2024-12-01|\n",
      "|2024-12-11 00:00:00-0500|247.96|250.8 |246.26|246.49|45205800 |0.0      |0.0         |AAPL  |2024-12-01|\n",
      "|2024-12-12 00:00:00-0500|246.89|248.74|245.68|247.96|32777500 |0.0      |0.0         |AAPL  |2024-12-01|\n",
      "|2024-12-13 00:00:00-0500|247.82|249.29|246.24|248.13|33155300 |0.0      |0.0         |AAPL  |2024-12-01|\n",
      "|2024-12-16 00:00:00-0500|247.99|251.38|247.65|251.04|51694800 |0.0      |0.0         |AAPL  |2024-12-01|\n",
      "|2024-12-17 00:00:00-0500|250.08|253.83|249.78|253.48|51356400 |0.0      |0.0         |AAPL  |2024-12-01|\n",
      "|2024-12-18 00:00:00-0500|252.16|254.28|247.74|248.05|56774100 |0.0      |0.0         |AAPL  |2024-12-01|\n",
      "|2024-12-19 00:00:00-0500|247.5 |252.0 |247.09|249.79|60882300 |0.0      |0.0         |AAPL  |2024-12-01|\n",
      "|2024-12-20 00:00:00-0500|248.04|255.0 |245.69|254.49|146890100|0.0      |0.0         |AAPL  |2024-12-01|\n",
      "|2024-12-02 00:00:00-0500|421.57|433.0 |421.31|430.98|20207200 |0.0      |0.0         |MSFT  |2024-12-01|\n",
      "|2024-12-03 00:00:00-0500|429.84|432.47|427.74|431.2 |18302000 |0.0      |0.0         |MSFT  |2024-12-01|\n",
      "|2024-12-04 00:00:00-0500|433.03|439.67|432.63|437.42|26009400 |0.0      |0.0         |MSFT  |2024-12-01|\n",
      "|2024-12-05 00:00:00-0500|437.92|444.66|436.17|442.62|21697800 |0.0      |0.0         |MSFT  |2024-12-01|\n",
      "|2024-12-06 00:00:00-0500|442.3 |446.1 |441.77|443.57|18821000 |0.0      |0.0         |MSFT  |2024-12-01|\n",
      "+------------------------+------+------+------+------+---------+---------+------------+------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType, TimestampType, IntegerType\n",
    "import yfinance as yf\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"YahooFinanceData\").getOrCreate()\n",
    "\n",
    "# Input data: List of tickers (start_date is fixed as '2024-12-01' for all)\n",
    "tickers_data = [\n",
    "    ('AAPL', '2024-12-01'),\n",
    "    ('MSFT', '2024-12-01'),\n",
    "    ('GOOGL', '2024-12-01'),\n",
    "]\n",
    "\n",
    "# Create an RDD from the input data\n",
    "tickers_rdd = spark.sparkContext.parallelize(tickers_data)\n",
    "\n",
    "# Fetch function to retrieve all stock data fields with ticker and fixed start_date\n",
    "def fetch_stock_data_with_ticker_and_date(ticker, start_date):\n",
    "    try:\n",
    "        # Fetch stock data using yfinance\n",
    "        stock_data = yf.Ticker(ticker).history(start=start_date)\n",
    "        stock_data.reset_index(inplace=True)  # Reset index to include 'Date' as a column\n",
    "        stock_data['Date'] = stock_data['Date'].dt.strftime('%Y-%m-%d %H:%M:%S%z')\n",
    "        \n",
    "        # Convert all rows to tuples and append ticker and start_date\n",
    "        return [tuple(row) + (ticker, start_date) for row in stock_data.itertuples(index=False)]\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data for {ticker}: {e}\")\n",
    "        return []\n",
    "\n",
    "# Function to process ticker data and call the fetch function with unpacked params\n",
    "def process_ticker_data(ticker_data):\n",
    "    return fetch_stock_data_with_ticker_and_date(*ticker_data)\n",
    "\n",
    "# Use flatMap to process data in parallel and collect all rows as a single list of tuples\n",
    "results_rdd = tickers_rdd.flatMap(process_ticker_data)\n",
    "\n",
    "# Collect the data as a list of tuples\n",
    "results = results_rdd.collect()\n",
    "\n",
    "# Define the schema for the DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"Date\", StringType(), True),        # 'Date' column as Timestamp\n",
    "    StructField(\"Open\", FloatType(), True),            # 'Open' column as Float\n",
    "    StructField(\"High\", FloatType(), True),            # 'High' column as Float\n",
    "    StructField(\"Low\", FloatType(), True),             # 'Low' column as Float\n",
    "    StructField(\"Close\", FloatType(), True),           # 'Close' column as Float\n",
    "    StructField(\"Volume\", IntegerType(), True),            # 'Volume' column as Int\n",
    "    StructField(\"Dividends\", FloatType(), True),       # 'Dividends' column as Float\n",
    "    StructField(\"Stock Splits\", FloatType(), True),    # 'Stock Splits' column as Float\n",
    "    StructField(\"Ticker\", StringType(), True),         # 'Ticker' column as String\n",
    "    StructField(\"Start Date\", StringType(), True)      # 'Start Date' column as String\n",
    "])\n",
    "\n",
    "# Create a DataFrame using the schema\n",
    "df = spark.createDataFrame(results, schema)\n",
    "\n",
    "# Show the DataFrame content\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "4aa0ae5a-7595-436c-b8c2-80d14f6b5028",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price            Close        High         Low        Open    Volume\n",
      "Ticker            AAPL        AAPL        AAPL        AAPL      AAPL\n",
      "Date                                                                \n",
      "2023-12-18  194.935013  195.671411  193.442325  195.134035  55751900\n",
      "2023-12-19  195.979889  195.989835  194.935005  195.203693  40714100\n",
      "2023-12-20  193.880188  196.716285  193.880188  195.940089  52242800\n",
      "2023-12-21  193.730896  196.119205  192.556656  195.143987  46482500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "\n",
    "# Get data\n",
    "data = yf.download(\"AAPL\", start=\"2023-12-18\", end=\"2023-12-22\")\n",
    "\n",
    "# Convert the index to datetime and remove timezone\n",
    "data.index = pd.to_datetime(data.index).tz_localize(None)\n",
    "\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "008a99a0-884c-40e2-be1a-ff47437f4dd6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-03 00:00:00+00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "\n",
    "from pandas import Timedelta\n",
    "\n",
    "\n",
    "\n",
    "# Download data\n",
    "\n",
    "ticker = \"AAPL\" \n",
    "\n",
    "df = yf.download(ticker, start=\"2023-01-01\", end=\"2023-12-22\")\n",
    "\n",
    "\n",
    "\n",
    "# Convert date to UTC\n",
    "\n",
    "df.index = df.index.tz_localize('UTC') \n",
    "\n",
    "\n",
    "\n",
    "# Access a specific date in UTC\n",
    "\n",
    "print(df.index[0])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b73f15ca-f170-4c77-8e42-5fc9b799f943",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/23 14:56:00 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "Processing ('AAPL', '2024-12-10')                                               \n",
      "Processing ('GOOGL', '2024-12-10')                                  (0 + 3) / 3]\n",
      "Processing ('MSFT', '2024-12-10')\n",
      "Processing ('GOOGL', '2024-12-10')                                              \n",
      "Processing ('MSFT', '2024-12-10')                                   (0 + 3) / 3]\n",
      "Processing ('AAPL', '2024-12-10')\n",
      "[Stage 8:===================>                                       (1 + 2) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+------------------+------------------+------------------+---------+---------+------------+------+--------------------+\n",
      "|               Date|              Open|              High|               Low|             Close|   Volume|Dividends|Stock Splits|Ticker|          ImportTime|\n",
      "+-------------------+------------------+------------------+------------------+------------------+---------+---------+------------+------+--------------------+\n",
      "|2024-12-10 00:00:00|246.88999938964844| 248.2100067138672|245.33999633789062|247.77000427246094| 36914800|      0.0|         0.0|  AAPL|2024-12-23 14:56:...|\n",
      "|2024-12-11 00:00:00| 247.9600067138672| 250.8000030517578|246.25999450683594|246.49000549316406| 45205800|      0.0|         0.0|  AAPL|2024-12-23 14:56:...|\n",
      "|2024-12-12 00:00:00|246.88999938964844|248.74000549316406|245.67999267578125| 247.9600067138672| 32777500|      0.0|         0.0|  AAPL|2024-12-23 14:56:...|\n",
      "|2024-12-13 00:00:00|247.82000732421875| 249.2899932861328|246.24000549316406| 248.1300048828125| 33155300|      0.0|         0.0|  AAPL|2024-12-23 14:56:...|\n",
      "|2024-12-16 00:00:00|247.99000549316406| 251.3800048828125|247.64999389648438| 251.0399932861328| 51694800|      0.0|         0.0|  AAPL|2024-12-23 14:56:...|\n",
      "|2024-12-17 00:00:00| 250.0800018310547| 253.8300018310547|249.77999877929688|253.47999572753906| 51356400|      0.0|         0.0|  AAPL|2024-12-23 14:56:...|\n",
      "|2024-12-18 00:00:00|252.16000366210938|254.27999877929688|247.74000549316406| 248.0500030517578| 56774100|      0.0|         0.0|  AAPL|2024-12-23 14:56:...|\n",
      "|2024-12-19 00:00:00|             247.5|             252.0|247.08999633789062| 249.7899932861328| 60882300|      0.0|         0.0|  AAPL|2024-12-23 14:56:...|\n",
      "|2024-12-20 00:00:00| 248.0399932861328|             255.0|245.69000244140625|254.49000549316406|146890100|      0.0|         0.0|  AAPL|2024-12-23 14:56:...|\n",
      "|2024-12-10 00:00:00| 444.3900146484375| 449.6199951171875| 441.6000061035156| 443.3299865722656| 18469500|      0.0|         0.0|  MSFT|2024-12-23 14:56:...|\n",
      "|2024-12-11 00:00:00|444.04998779296875| 450.3500061035156|444.04998779296875|  448.989990234375| 19200200|      0.0|         0.0|  MSFT|2024-12-23 14:56:...|\n",
      "|2024-12-12 00:00:00| 449.1099853515625| 456.1600036621094| 449.1099853515625|449.55999755859375| 20834800|      0.0|         0.0|  MSFT|2024-12-23 14:56:...|\n",
      "|2024-12-13 00:00:00|448.44000244140625|451.42999267578125| 445.5799865722656| 447.2699890136719| 20177800|      0.0|         0.0|  MSFT|2024-12-23 14:56:...|\n",
      "|2024-12-16 00:00:00| 447.2699890136719|452.17999267578125| 445.2799987792969| 451.5899963378906| 23598800|      0.0|         0.0|  MSFT|2024-12-23 14:56:...|\n",
      "|2024-12-17 00:00:00|  451.010009765625| 455.2900085449219|449.57000732421875| 454.4599914550781| 22733500|      0.0|         0.0|  MSFT|2024-12-23 14:56:...|\n",
      "|2024-12-18 00:00:00|451.32000732421875| 452.6499938964844| 437.0199890136719| 437.3900146484375| 24444500|      0.0|         0.0|  MSFT|2024-12-23 14:56:...|\n",
      "|2024-12-19 00:00:00| 441.6199951171875|443.17999267578125|436.32000732421875| 437.0299987792969| 22963700|      0.0|         0.0|  MSFT|2024-12-23 14:56:...|\n",
      "|2024-12-20 00:00:00| 433.1099853515625|  443.739990234375| 428.6300048828125| 436.6000061035156| 64235200|      0.0|         0.0|  MSFT|2024-12-23 14:56:...|\n",
      "|2024-12-10 00:00:00|182.85000610351562|186.36000061035156| 181.0500030517578| 185.1699981689453| 54813000|      0.0|         0.0| GOOGL|2024-12-23 14:56:...|\n",
      "|2024-12-11 00:00:00|185.30999755859375|195.61000061035156|184.85000610351562|195.39999389648438| 67894100|      0.0|         0.0| GOOGL|2024-12-23 14:56:...|\n",
      "+-------------------+------------------+------------------+------------------+------------------+---------+---------+------------+------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType, TimestampType\n",
    "import yfinance as yf\n",
    "from datetime import date, datetime, timedelta\n",
    "from io import StringIO\n",
    "import yaml\n",
    "\n",
    "# import psycopg2.extras\n",
    "# import psycopg2\n",
    "# from pgcopy import CopyManager\n",
    "\n",
    "# Define the custom exception\n",
    "class MyCustomException(Exception):\n",
    "    pass\n",
    "\n",
    "# Main class for ingestion\n",
    "class RawYFIngestion:\n",
    "    def __init__(self, equity_type, destination):\n",
    "        self.equity_type = equity_type\n",
    "        self.destination = destination\n",
    "        self.import_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S.\") + str(datetime.now().microsecond)[:3]\n",
    "        \n",
    "       # Get yfinance registered column list\n",
    "        with open(\"config.yaml\",\"r\") as file_object:\n",
    "            documents=yaml.safe_load_all(file_object)\n",
    "            for doc in documents:\n",
    "                doc_name = doc['document_name']\n",
    "                if doc_name==f\"yfinance_{equity_type}\":\n",
    "                    self.registered_col_list=doc['registered_column_list']\n",
    "\n",
    "    # Function to fetch data from Yahoo Finance\n",
    "    def fetch_yfinance_record(self, multi_param_pairs):\n",
    "        try:\n",
    "            symbol, start_date = multi_param_pairs\n",
    "            # Fetch stock data using yfinance\n",
    "            quote = yf.Ticker(symbol)\n",
    "            current_date = date.today()\n",
    "            hist = quote.history(start=start_date, end=current_date)\n",
    "            \n",
    "            # Reset index to include Date as a column and format it\n",
    "            hist.reset_index(inplace=True)\n",
    "            hist['Date'] = hist['Date'].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "            # Add symbol and import_time to each row\n",
    "            record_list = [\n",
    "                tuple(row) + (symbol, self.import_time) for row in hist.itertuples(index=False)\n",
    "            ]\n",
    "            return record_list\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching data for {symbol}: {e}\")\n",
    "            return []  # Return an empty list on error\n",
    "    \n",
    "    # Function to process the records (pass through parameters)\n",
    "    def process_yfinance_record(self, single_param_pair):\n",
    "        print(f\"Processing {single_param_pair}\")\n",
    "        return self.fetch_yfinance_record(single_param_pair)\n",
    "\n",
    "    # Parallel fetch function\n",
    "    def parallel_fetch(self, multi_param_pairs):\n",
    "        # Create Spark session\n",
    "        spark = SparkSession.builder.appName(\"YahooFinanceData\").getOrCreate()\n",
    "        \n",
    "        # Create RDD from the input parameter pairs\n",
    "        record_rdd = spark.sparkContext.parallelize(multi_param_pairs)\n",
    "        \n",
    "        # Use flatMap to return a flattened list of records\n",
    "        results_rdd = record_rdd.flatMap(self.process_yfinance_record)\n",
    "        \n",
    "        # # Collect the results from the RDD\n",
    "        # results = results_rdd.collect()\n",
    "        \n",
    "        schema = ['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Dividends', 'Stock Splits', 'Ticker', 'ImportTime']\n",
    "        # rdd_rows = spark.sparkContext.parallelize(stock_data_rows)\n",
    "        df = spark.createDataFrame(results_rdd, schema)\n",
    "        df.show()\n",
    "        \n",
    "        \n",
    "        return df\n",
    "\n",
    "# List of stock symbols and start dates\n",
    "yf_param_pairs = [\n",
    "    ('AAPL', '2024-12-10'),\n",
    "    ('MSFT', '2024-12-10'),\n",
    "    ('GOOGL', '2024-12-10'),\n",
    "]\n",
    "\n",
    "# Instantiate the class\n",
    "stock_stage = RawYFIngestion('stock', 'mytable')\n",
    "\n",
    "# Fetch data in parallel\n",
    "stock_data_rows = stock_stage.parallel_fetch(yf_param_pairs)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b748785-765b-46a8-a8ed-864642778050",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'raw_tables': {'raw.stock_eod_data': {'schema': [{'name': 'date',\n",
       "     'type': 'StringType()',\n",
       "     'nullable': False},\n",
       "    {'name': 'open', 'type': 'FloatType()', 'nullable': True},\n",
       "    {'name': 'high', 'type': 'FloatType()', 'nullable': True},\n",
       "    {'name': 'low', 'type': 'FloatType()', 'nullable': True},\n",
       "    {'name': 'close', 'type': 'FloatType()', 'nullable': True},\n",
       "    {'name': 'volume', 'type': 'IntegerType()', 'nullable': True},\n",
       "    {'name': 'dividends', 'type': 'FloatType()', 'nullable': True},\n",
       "    {'name': 'stock_splits', 'type': 'FloatType()', 'nullable': True},\n",
       "    {'name': 'symbol', 'type': 'StringType()', 'nullable': False},\n",
       "    {'name': 'import_time', 'type': 'StringType()', 'nullable': False}],\n",
       "   'partition_by': [{'field': 'date'}]}}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType\n",
    "import os\n",
    "\n",
    "## DEFINE SENSITIVE VARIABLES\n",
    "CATALOG_URI = \"http://nessie:19120/api/v1\"  # Nessie Server URI\n",
    "WAREHOUSE = \"s3://warehouse/\"               # Minio Address to Write to\n",
    "STORAGE_URI = \"http://172.22.0.3:9000\"      # Minio IP address from docker inspect\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import yaml\n",
    "\n",
    "\n",
    "def load_config(file_path):\n",
    "    \"\"\"Load the YAML configuration file.\"\"\"\n",
    "    with open(file_path, \"r\") as file:\n",
    "        config = yaml.safe_load(file)\n",
    "    return config\n",
    "\n",
    "load_config('raw_table_schema.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1cf4c2a-9ec2-44e1-a19b-5d5e755a21fe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'schema': [{'name': 'date', 'type': 'StringType()', 'nullable': False}, {'name': 'open', 'type': 'FloatType()', 'nullable': True}, {'name': 'high', 'type': 'FloatType()', 'nullable': True}, {'name': 'low', 'type': 'FloatType()', 'nullable': True}, {'name': 'close', 'type': 'FloatType()', 'nullable': True}, {'name': 'volume', 'type': 'IntegerType()', 'nullable': True}, {'name': 'dividends', 'type': 'FloatType()', 'nullable': True}, {'name': 'stock_splits', 'type': 'FloatType()', 'nullable': True}, {'name': 'symbol', 'type': 'StringType()', 'nullable': False}, {'name': 'import_time', 'type': 'StringType()', 'nullable': False}], 'partition_by': [{'field': 'date'}]}\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'StringType()'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 39\u001b[0m\n\u001b[1;32m     37\u001b[0m table_config \u001b[38;5;241m=\u001b[39m config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraw_tables\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraw.stock_eod_data\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(table_config)\n\u001b[0;32m---> 39\u001b[0m schema \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_struct_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtable_config\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mschema\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m partition_by \u001b[38;5;241m=\u001b[39m table_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpartition_by\u001b[39m\u001b[38;5;124m'\u001b[39m, [])\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Build the SQL query for creating the Iceberg table\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[8], line 19\u001b[0m, in \u001b[0;36mcreate_struct_type\u001b[0;34m(schema_config)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_struct_type\u001b[39m(schema_config):\n\u001b[0;32m---> 19\u001b[0m     fields \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     20\u001b[0m         StructField(field[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m], type_mapping[field[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m]](), field[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnullable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     21\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m field \u001b[38;5;129;01min\u001b[39;00m schema_config\n\u001b[1;32m     22\u001b[0m     ]\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m StructType(fields)\n",
      "Cell \u001b[0;32mIn[8], line 20\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_struct_type\u001b[39m(schema_config):\n\u001b[1;32m     19\u001b[0m     fields \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m---> 20\u001b[0m         StructField(field[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[43mtype_mapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfield\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtype\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m(), field[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnullable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     21\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m field \u001b[38;5;129;01min\u001b[39;00m schema_config\n\u001b[1;32m     22\u001b[0m     ]\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m StructType(fields)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'StringType()'"
     ]
    }
   ],
   "source": [
    "\n",
    "import yaml\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType, IntegerType\n",
    "\n",
    "# Load the YAML configuration file\n",
    "def load_config(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        return yaml.safe_load(f)\n",
    "\n",
    "# Map YAML types to PySpark types\n",
    "type_mapping = {\n",
    "    \"StringType\": StringType,\n",
    "    \"FloatType\": FloatType,\n",
    "    \"IntegerType\": IntegerType,\n",
    "}\n",
    "\n",
    "# Convert YAML schema to PySpark StructType\n",
    "def create_struct_type(schema_config):\n",
    "    fields = [\n",
    "        StructField(field[\"name\"], field[\"type\"], field[\"nullable\"])\n",
    "        for field in schema_config\n",
    "    ]\n",
    "    return StructType(fields)\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"IcebergTableCreator\") \\\n",
    "    .config(\"spark.sql.catalog.my_catalog\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.my_catalog.type\", \"hadoop\") \\\n",
    "    .config(\"spark.sql.catalog.my_catalog.warehouse\", \"path/to/warehouse\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load the YAML configuration for raw tables\n",
    "config = load_config('raw_table_schema.yaml')\n",
    "\n",
    "# Extract the schema and partitioning for the table\n",
    "table_config = config['raw_tables']['raw.stock_eod_data']\n",
    "print(table_config)\n",
    "schema = create_struct_type(table_config['schema'])\n",
    "partition_by = table_config.get('partition_by', [])\n",
    "\n",
    "# Build the SQL query for creating the Iceberg table\n",
    "table_name = \"my_catalog.raw.stock_eod_data\"\n",
    "schema_columns = \", \".join([f\"{field.name} {field.dataType.simpleString()}\" for field in schema.fields])\n",
    "partition_columns = \", \".join([p[\"field\"] for p in partition_by]) if partition_by else \"\"\n",
    "\n",
    "# Create the Iceberg table\n",
    "create_table_query = f\"\"\"\n",
    "CREATE TABLE {table_name} ({schema_columns})\n",
    "USING iceberg\n",
    "\"\"\"\n",
    "if partition_columns:\n",
    "    create_table_query += f\" PARTITIONED BY ({partition_columns})\"\n",
    "\n",
    "# Run the query\n",
    "spark.sql(create_table_query)\n",
    "\n",
    "print(f\"Table {table_name} created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3c0089-d61a-44cb-884b-e896c1b791dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Poetry",
   "language": "python",
   "name": "poetry-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
