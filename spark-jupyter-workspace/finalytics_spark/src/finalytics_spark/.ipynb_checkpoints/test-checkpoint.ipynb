{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c8e29c97-3bf7-4c3f-8608-65c8a39b910f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType\n",
    "\n",
    "with open(\"config.yaml\",\"r\") as file_object:\n",
    "    config_dict=yaml.load(file_object,Loader=yaml.SafeLoader)\n",
    "\n",
    "## DEFINE SENSITIVE VARIABLES\n",
    "CATALOG_URI = config_dict[\"iceberg_env\"][\"CATALOG_URI\"]    # Nessie Server URI\n",
    "WAREHOUSE = config_dict[\"iceberg_env\"][\"WAREHOUSE\"]               # Minio Address to Write to\n",
    "STORAGE_URI = config_dict[\"iceberg_env\"][\"STORAGE_URI\"]      # Minio IP address from docker inspect\n",
    "\n",
    "# Configure Spark with necessary packages and Iceberg/Nessie settings\n",
    "conf = (\n",
    "    pyspark.SparkConf()\n",
    "        .setAppName('sales_data_app')\n",
    "        # Include necessary packages\n",
    "        .set('spark.jars.packages', 'org.postgresql:postgresql:42.7.3,org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.0,org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.12:0.77.1,software.amazon.awssdk:bundle:2.24.8,software.amazon.awssdk:url-connection-client:2.24.8')\n",
    "        # Enable Iceberg and Nessie extensions\n",
    "        .set('spark.sql.extensions', 'org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions')\n",
    "        # Configure Nessie catalog\n",
    "        .set('spark.sql.catalog.nessie', 'org.apache.iceberg.spark.SparkCatalog')\n",
    "        .set('spark.sql.catalog.nessie.uri', CATALOG_URI)\n",
    "        .set('spark.sql.catalog.nessie.ref', 'main')\n",
    "        .set('spark.sql.catalog.nessie.authentication.type', 'NONE')\n",
    "        .set('spark.sql.catalog.nessie.catalog-impl', 'org.apache.iceberg.nessie.NessieCatalog')\n",
    "        # Set Minio as the S3 endpoint for Iceberg storage\n",
    "        .set('spark.sql.catalog.nessie.s3.endpoint', STORAGE_URI)\n",
    "        .set('spark.sql.catalog.nessie.warehouse', WAREHOUSE)\n",
    "        .set('spark.sql.catalog.nessie.io-impl', 'org.apache.iceberg.aws.s3.S3FileIO')\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d3bfb9-8661-4721-bc0a-f3dd17f78834",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "839a418f-a445-432c-8043-d334d15a3887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_io.TextIOWrapper name='./config.yaml' mode='w' encoding='UTF-8'>\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# import yaml\n",
    "# # Create an empty dictionary\n",
    "# config_dict=dict()\n",
    "# # Add configuration data to the dictionary\n",
    "# config_dict[\"server\"]={'port': 8080, 'host': '0.0.0.0'}\n",
    "# config_dict[\"logging\"]={'level': 'info', 'file': '/var/log/web-server.log'}\n",
    "# config_dict[\"database\"]={'url': 'postgres://user:password@host:port/database','pool': 100}\n",
    "# # Save the dictionary to a YAML file\n",
    "# with open(\"./config.yaml\",\"w\") as file_object:\n",
    "#     print (file_object)\n",
    "#     yaml.dump(config_dict,file_object)\n",
    "#     print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fc09c08b-a34a-4aa7-ac70-978a84522411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'database': {'pool': 100, 'url': 'postgres://user:password@host:port/database'}, 'logging': {'file': '/var/log/web-server.log', 'level': 'info'}, 'server': {'host': '0.0.0.0', 'port': 8080}, 'iceberg_env': {'CATALOG_URI': 'http://nessie:19120/api/v1', 'WAREHOUSE': 's3://warehouse/', 'STORAGE_URI': 'http://172.22.0.3:9000'}}\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "with open(\"./config.yaml\",\"r\") as file_object:\n",
    "    data=yaml.load(file_object,Loader=yaml.SafeLoader)\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "baea7bb3-64a3-4cd1-b0d0-c54eb02b12f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "# Create an empty dictionary\n",
    "config_dict=dict()\n",
    "# Add configuration data to the dictionary\n",
    "config_dict[\"server\"]={'port': 8080, 'host': '0.0.0.0'}\n",
    "config_dict[\"logging\"]={'level': 'info', 'file': '/var/log/web-server.log'}\n",
    "config_dict[\"database\"]={'url': 'postgres://user:password@host:port/database','pool': 100}\n",
    "# Create another dictionary\n",
    "details_dict= {\"Website Name\":\"HoneyBadger\",\"Author\":\"Aditya\", \"Topic\":\"Configuration Files\", \"Content Type\":\"Blog\"}\n",
    "x_dict= {\"Website Name\":\"HoneyBadger\",\"Author\":\"Aditya\", \"Topic\":\"Configuration Files\", \"Content Type\":\"Blog\"}\n",
    "list_of_dicts=[config_dict,details_dict, x_dict]\n",
    "# Save data to a YAML file\n",
    "with open(\"web-server-details-1.yaml\",\"w\") as file_object:\n",
    "    yaml.dump_all(list_of_dicts,file_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3100b031-7748-460b-963f-16c51f4b185d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'database': {'pool': 100, 'url': 'postgres://user:password@host:port/database'}, 'logging': {'file': '/var/log/web-server.log', 'level': 'info'}, 'server': {'host': '0.0.0.0', 'port': 8080}, 'iceberg_env': {'CATALOG_URI': 'http://nessie:19120/api/v1', 'WAREHOUSE': 's3://warehouse/', 'STORAGE_URI': 'http://172.22.0.3:9000'}}\n",
      "http://nessie:19120/api/v1\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "with open(\"config.yaml\",\"r\") as file_object:\n",
    "    config_dict=yaml.load(file_object,Loader=yaml.SafeLoader)\n",
    "print(config_dict)\n",
    "print(config_dict[\"iceberg_env\"][\"STORAGE_URI\"])\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "86fd244c-cf7a-40d7-b987-ec829fd90e68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/12/21 19:32:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType, TimestampType\n",
    "import yfinance as yf\n",
    "from pyspark.sql import Row\n",
    "from multiprocessing import Pool\n",
    "\n",
    "# Initialize PySpark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"StockDataLoader\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define a schema for the stock data\n",
    "schema = StructType([\n",
    "    StructField(\"Ticker\", StringType(), True),\n",
    "    StructField(\"Date\", TimestampType(), True),\n",
    "    StructField(\"Open\", FloatType(), True),\n",
    "    StructField(\"High\", FloatType(), True),\n",
    "    StructField(\"Low\", FloatType(), True),\n",
    "    StructField(\"Close\", FloatType(), True),\n",
    "    StructField(\"Adj Close\", FloatType(), True),\n",
    "    StructField(\"Volume\", FloatType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3fa57384-05ba-4aa6-be47-fa44039ae014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fetch stock data for a single ticker\n",
    "from datetime import date, datetime, timedelta\n",
    "\n",
    "\n",
    "def fetch_stock_data(symbol):\n",
    "    try:\n",
    "        quote = yf.Ticker(symbol)\n",
    "        start_date = '2024-12-01'\n",
    "        current_date = date.today()\n",
    "        hist = quote.history(start=start_date, end=current_date)\n",
    "        import_time=datetime.now()\n",
    "        # if hist.empty:\n",
    "        #     sql_script = f\"UPDATE fin.stock_symbol SET is_valid= false WHERE symbol='{symbol}';\"\n",
    "        #     # print(sql_script)\n",
    "        #     self.execute_sql_script(sql_script)\n",
    "\n",
    "        # Reset index to include the Date column in the DataFrame\n",
    "        hist.reset_index(inplace=True)\n",
    "        # # get column list with extra fields\n",
    "        column_list = [x.lower().replace(\" \", \"_\") for x in hist.columns]\n",
    "        extra_field_list = ['symbol', 'import_time']\n",
    "        column_list.extend(extra_field_list)\n",
    "\n",
    "        # get records with appended extra fields\n",
    "        hist_records_map = hist.itertuples(index=False)\n",
    "        record_list = [tuple(row) + (symbol,) + (import_time,) for row in hist_records_map]\n",
    "\n",
    "        return column_list, record_list\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data for {ticker}: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "af29e04b-276e-4e63-bb4f-66437dc9586f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['date', 'open', 'high', 'low', 'close', 'volume', 'dividends', 'stock_splits', 'symbol', 'import_time']\n"
     ]
    }
   ],
   "source": [
    "column_list, record_list=fetch_stock_data('C')\n",
    "print(column_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5f8da32b-3482-4bfd-bbfe-f22e28ccc617",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel_fetch(tickers):\n",
    "    with Pool(processes=4) as pool:  # Adjust the number of processes based on your machine's capacity\n",
    "        results = pool.map(fetch_stock_data, tickers)\n",
    "    return [row for sublist in results for row in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2b0ce568-1c9d-4a89-9e41-914aef2b2e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching data for AAPL: 'Adj Close'Error fetching data for MSFT: 'Adj Close'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching data for AMZN: 'Adj Close'\n",
      "Error fetching data for GOOGL: 'Adj Close'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+----+----+---+-----+---------+------+\n",
      "|Ticker|Date|Open|High|Low|Close|Adj Close|Volume|\n",
      "+------+----+----+----+---+-----+---------+------+\n",
      "+------+----+----+----+---+-----+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# List of tickers to fetch\n",
    "tickers = [\"AAPL\", \"MSFT\", \"GOOGL\", \"AMZN\"]  # Add more tickers as needed\n",
    "\n",
    "# Fetch data in parallel\n",
    "stock_data_rows = parallel_fetch(tickers)\n",
    "\n",
    "# Convert the data to a Spark DataFrame\n",
    "stock_df = spark.createDataFrame(stock_data_rows, schema=schema)\n",
    "\n",
    "# Show some rows\n",
    "stock_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2ea2ad7f-df78-468e-be27-c3338630a0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date, datetime\n",
    "def get_yfinance_record(symbol):\n",
    "    try:\n",
    "        quote = yf.Ticker(symbol)\n",
    "        start_date = '2024-12-01'\n",
    "        current_date = date.today()\n",
    "        print(current_date)\n",
    "        # print(start_date+timedelta(days=1))\n",
    "\n",
    "        hist = quote.history(start=start_date, end=current_date)\n",
    "        print(hist)\n",
    "        # if hist.empty:\n",
    "        #     sql_script = f\"UPDATE fin.stock_symbol SET is_valid= false WHERE symbol='{symbol}';\"\n",
    "        #     # print(sql_script)\n",
    "        #     self.execute_sql_script(sql_script)\n",
    "\n",
    "        # Reset index to include the Date column in the DataFrame\n",
    "        hist.reset_index(inplace=True)\n",
    "\n",
    "        # # get column list with extra fields\n",
    "        # column_list = [x.lower().replace(\" \", \"_\") for x in hist.columns]\n",
    "        # extra_field_list = ['symbol', 'import_time']\n",
    "        # column_list.extend(extra_field_list)\n",
    "        # print(column_list)\n",
    "\n",
    "        # # get records with extra fields\n",
    "        # hist_records_map = hist.itertuples(index=False)\n",
    "\n",
    "        # record_list = [tuple(row) + (symbol,) + (self.import_time,) for row in hist_records_map]\n",
    "\n",
    "        # print(record_list)\n",
    "\n",
    "        # return column_list, record_list\n",
    "    except Exception as e:\n",
    "        print(\"x\")\n",
    "        # print(f\"An error occurred: {e}\")\n",
    "        # if \"delisted\" in e:\n",
    "        #     print(\"abc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0de2a1d9-48ba-41ef-8be7-b2b0dce8b8e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-21\n",
      "                                Open       High        Low      Close  \\\n",
      "Date                                                                    \n",
      "2024-12-02 00:00:00-05:00  71.260002  71.650002  70.879997  71.389999   \n",
      "2024-12-03 00:00:00-05:00  72.190002  72.800003  71.269997  71.419998   \n",
      "2024-12-04 00:00:00-05:00  71.500000  71.720001  70.500000  71.500000   \n",
      "2024-12-05 00:00:00-05:00  71.830002  72.849998  71.639999  72.230003   \n",
      "2024-12-06 00:00:00-05:00  72.309998  72.599998  71.709999  72.150002   \n",
      "2024-12-09 00:00:00-05:00  72.300003  72.800003  71.839996  71.860001   \n",
      "2024-12-10 00:00:00-05:00  72.000000  73.379997  71.580002  72.500000   \n",
      "2024-12-11 00:00:00-05:00  73.000000  73.260002  71.269997  71.959999   \n",
      "2024-12-12 00:00:00-05:00  71.860001  72.330002  71.410004  71.430000   \n",
      "2024-12-13 00:00:00-05:00  71.709999  71.910004  70.760002  71.010002   \n",
      "2024-12-16 00:00:00-05:00  71.269997  71.769997  70.830002  71.489998   \n",
      "2024-12-17 00:00:00-05:00  70.900002  71.349998  70.800003  71.120003   \n",
      "2024-12-18 00:00:00-05:00  71.180000  71.470001  67.919998  68.120003   \n",
      "2024-12-19 00:00:00-05:00  69.209999  69.839996  68.320000  68.419998   \n",
      "2024-12-20 00:00:00-05:00  68.290001  70.260002  68.029999  69.190002   \n",
      "\n",
      "                             Volume  Dividends  Stock Splits  \n",
      "Date                                                          \n",
      "2024-12-02 00:00:00-05:00  11932600        0.0           0.0  \n",
      "2024-12-03 00:00:00-05:00  17544200        0.0           0.0  \n",
      "2024-12-04 00:00:00-05:00  12331900        0.0           0.0  \n",
      "2024-12-05 00:00:00-05:00  14307600        0.0           0.0  \n",
      "2024-12-06 00:00:00-05:00   8781100        0.0           0.0  \n",
      "2024-12-09 00:00:00-05:00  11957500        0.0           0.0  \n",
      "2024-12-10 00:00:00-05:00  16833400        0.0           0.0  \n",
      "2024-12-11 00:00:00-05:00  22533600        0.0           0.0  \n",
      "2024-12-12 00:00:00-05:00   8893000        0.0           0.0  \n",
      "2024-12-13 00:00:00-05:00   9819900        0.0           0.0  \n",
      "2024-12-16 00:00:00-05:00  11833800        0.0           0.0  \n",
      "2024-12-17 00:00:00-05:00  12639000        0.0           0.0  \n",
      "2024-12-18 00:00:00-05:00  18028000        0.0           0.0  \n",
      "2024-12-19 00:00:00-05:00  13464300        0.0           0.0  \n",
      "2024-12-20 00:00:00-05:00  27718800        0.0           0.0  \n"
     ]
    }
   ],
   "source": [
    "get_yfinance_record('C')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0819f47b-0737-453a-b4a7-dfcd27ad010c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Poetry",
   "language": "python",
   "name": "poetry-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
