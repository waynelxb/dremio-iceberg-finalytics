{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc71f1ce-97ce-4e88-a374-747acfc540fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import nbimporter\n",
    "from datetime import datetime, date\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType\n",
    "from raw_yfinance_ingestion import RawYFIngestion  \n",
    "from lab_registered_tables import RegisteredTables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8e29c97-3bf7-4c3f-8608-65c8a39b910f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.postgresql#postgresql added as a dependency\n",
      "org.apache.iceberg#iceberg-spark-runtime-3.5_2.12 added as a dependency\n",
      "org.projectnessie.nessie-integrations#nessie-spark-extensions-3.5_2.12 added as a dependency\n",
      "software.amazon.awssdk#bundle added as a dependency\n",
      "software.amazon.awssdk#url-connection-client added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-ed0c753e-3d6b-415b-8d8b-29ac3dd92fc3;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.postgresql#postgresql;42.7.3 in central\n",
      "\tfound org.checkerframework#checker-qual;3.42.0 in central\n",
      "\tfound org.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.5.0 in central\n",
      "\tfound org.projectnessie.nessie-integrations#nessie-spark-extensions-3.5_2.12;0.77.1 in central\n",
      "\tfound software.amazon.awssdk#bundle;2.24.8 in central\n",
      "\tfound software.amazon.awssdk#url-connection-client;2.24.8 in central\n",
      "\tfound software.amazon.awssdk#utils;2.24.8 in central\n",
      "\tfound org.reactivestreams#reactive-streams;1.0.4 in central\n",
      "\tfound software.amazon.awssdk#annotations;2.24.8 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      "\tfound software.amazon.awssdk#http-client-spi;2.24.8 in central\n",
      "\tfound software.amazon.awssdk#metrics-spi;2.24.8 in central\n",
      ":: resolution report :: resolve 437ms :: artifacts dl 19ms\n",
      "\t:: modules in use:\n",
      "\torg.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.5.0 from central in [default]\n",
      "\torg.checkerframework#checker-qual;3.42.0 from central in [default]\n",
      "\torg.postgresql#postgresql;42.7.3 from central in [default]\n",
      "\torg.projectnessie.nessie-integrations#nessie-spark-extensions-3.5_2.12;0.77.1 from central in [default]\n",
      "\torg.reactivestreams#reactive-streams;1.0.4 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\tsoftware.amazon.awssdk#annotations;2.24.8 from central in [default]\n",
      "\tsoftware.amazon.awssdk#bundle;2.24.8 from central in [default]\n",
      "\tsoftware.amazon.awssdk#http-client-spi;2.24.8 from central in [default]\n",
      "\tsoftware.amazon.awssdk#metrics-spi;2.24.8 from central in [default]\n",
      "\tsoftware.amazon.awssdk#url-connection-client;2.24.8 from central in [default]\n",
      "\tsoftware.amazon.awssdk#utils;2.24.8 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   12  |   0   |   0   |   0   ||   12  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-ed0c753e-3d6b-415b-8d8b-29ac3dd92fc3\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 12 already retrieved (0kB/13ms)\n",
      "24/12/24 01:48:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/12/24 01:48:36 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "24/12/24 01:48:36 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "with open(\"config_connections.yaml\",\"r\") as file:\n",
    "    config=yaml.safe_load(file)\n",
    "    catalog_uri = config['docker_env']['catalog_uri'] \n",
    "    warehouse = config['docker_env']['warehouse']     # Minio Address to Write to\n",
    "    storage_uri = config['docker_env']['storage_uri'] # Minio IP address from docker inspec\n",
    "\n",
    "# Configure Spark with necessary packages and Iceberg/Nessie settings\n",
    "conf = (\n",
    "    pyspark.SparkConf()\n",
    "        .setAppName('finalytics_app')\n",
    "        # Include necessary packages\n",
    "        .set('spark.jars.packages',\n",
    "             'org.postgresql:postgresql:42.7.3,'\n",
    "             'org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.0,'\n",
    "             'org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.12:0.77.1,'\n",
    "             'software.amazon.awssdk:bundle:2.24.8,'\n",
    "             'software.amazon.awssdk:url-connection-client:2.24.8')\n",
    "        # Enable Iceberg and Nessie extensions\n",
    "        .set('spark.sql.extensions', \n",
    "             'org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,'\n",
    "             'org.projectnessie.spark.extensions.NessieSparkSessionExtensions')\n",
    "        # Configure Nessie catalog\n",
    "        .set('spark.sql.catalog.nessie', 'org.apache.iceberg.spark.SparkCatalog')\n",
    "        .set('spark.sql.catalog.nessie.uri', catalog_uri)\n",
    "        .set('spark.sql.catalog.nessie.ref', 'main')\n",
    "        .set('spark.sql.catalog.nessie.authentication.type', 'NONE')\n",
    "        .set('spark.sql.catalog.nessie.catalog-impl', 'org.apache.iceberg.nessie.NessieCatalog')\n",
    "        # Set Minio as the S3 endpoint for Iceberg storage\n",
    "        .set('spark.sql.catalog.nessie.s3.endpoint', storage_uri)\n",
    "        .set('spark.sql.catalog.nessie.warehouse', warehouse)\n",
    "        .set('spark.sql.catalog.nessie.io-impl', 'org.apache.iceberg.aws.s3.S3FileIO')       \n",
    ")   \n",
    "\n",
    "# Start Spark session\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "cf37b93e-929a-4089-bdf0-2b203ed87394",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S.\") + str(datetime.now().microsecond)[:3]\n",
    "zone='raw'\n",
    "sink_table='raw.stock_eod_yfinance'\n",
    "config_file_path='registered_table_schemas.yaml'\n",
    "rt=RegisteredTables(zone, sink_table, config_file_path)\n",
    "registered_column_list = rt.get_column_list()\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "43f1668e-6c02-40a4-8d8b-4c1e522420fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fetch_yfinance_record(multi_param_pairs):\n",
    "    try:\n",
    "        symbol, start_date = multi_param_pairs\n",
    "        # Fetch stock data using yfinance\n",
    "        quote = yf.Ticker(symbol)\n",
    "        current_date = date.today()\n",
    "        hist = quote.history(start=start_date, end=current_date)\n",
    "\n",
    "        # Reset index to include Date as a column and format it\n",
    "        hist.reset_index(inplace=True)\n",
    "        hist['Date'] = hist['Date'].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "        # Add symbol and import_time to each row\n",
    "        record_list = [\n",
    "            tuple(row) + (symbol, import_time) for row in hist.itertuples(index=False)\n",
    "        ]\n",
    "\n",
    "        return record_list\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data for {symbol}: {e}\")\n",
    "        return []  # Return an empty list on error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "afab4b13-8598-414f-8e80-14ec3828ed3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to process the records (pass through parameters)\n",
    "def process_yfinance_record(single_param_pair):\n",
    "    # print(f\"Processing {single_param_pair}\")\n",
    "    return fetch_yfinance_record(single_param_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "fc24f51b-ca74-4232-ae7c-eda5bafa83ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Parallel fetch function\n",
    "def parallel_fetch(multi_param_pairs):\n",
    "    # Create RDD from the input parameter pairs\n",
    "    record_rdd = spark.sparkContext.parallelize(multi_param_pairs)\n",
    "\n",
    "    # Use flatMap to return a flattened list of records\n",
    "    results_rdd = record_rdd.flatMap(process_yfinance_record)\n",
    "\n",
    "    # Collect the results from the RDD and convert to a list of tuples\n",
    "    # results = results_rdd.collect()        \n",
    "    df = spark.createDataFrame(results_rdd, registered_column_list)   \n",
    "    df.show()\n",
    "    return df\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "78b02da3-a584-4dbd-b44b-ccd3d5bbed1a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 12:======================================>                   (2 + 1) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+------------------+------------------+------------------+---------+---------+------------+------+--------------------+\n",
      "|               date|              open|              high|               low|             close|   volume|dividends|stock_splits|symbol|         import_time|\n",
      "+-------------------+------------------+------------------+------------------+------------------+---------+---------+------------+------+--------------------+\n",
      "|2024-12-10 00:00:00|246.88999938964844| 248.2100067138672|245.33999633789062|247.77000427246094| 36914800|      0.0|         0.0|  AAPL|2024-12-24 01:35:...|\n",
      "|2024-12-11 00:00:00| 247.9600067138672| 250.8000030517578|246.25999450683594|246.49000549316406| 45205800|      0.0|         0.0|  AAPL|2024-12-24 01:35:...|\n",
      "|2024-12-12 00:00:00|246.88999938964844|248.74000549316406|245.67999267578125| 247.9600067138672| 32777500|      0.0|         0.0|  AAPL|2024-12-24 01:35:...|\n",
      "|2024-12-13 00:00:00|247.82000732421875| 249.2899932861328|246.24000549316406| 248.1300048828125| 33155300|      0.0|         0.0|  AAPL|2024-12-24 01:35:...|\n",
      "|2024-12-16 00:00:00|247.99000549316406| 251.3800048828125|247.64999389648438| 251.0399932861328| 51694800|      0.0|         0.0|  AAPL|2024-12-24 01:35:...|\n",
      "|2024-12-17 00:00:00| 250.0800018310547| 253.8300018310547|249.77999877929688|253.47999572753906| 51356400|      0.0|         0.0|  AAPL|2024-12-24 01:35:...|\n",
      "|2024-12-18 00:00:00|252.16000366210938|254.27999877929688|247.74000549316406| 248.0500030517578| 56774100|      0.0|         0.0|  AAPL|2024-12-24 01:35:...|\n",
      "|2024-12-19 00:00:00|             247.5|             252.0|247.08999633789062| 249.7899932861328| 60882300|      0.0|         0.0|  AAPL|2024-12-24 01:35:...|\n",
      "|2024-12-20 00:00:00| 248.0399932861328|             255.0|245.69000244140625|254.49000549316406|146890100|      0.0|         0.0|  AAPL|2024-12-24 01:35:...|\n",
      "|2024-12-23 00:00:00|254.77000427246094|255.64999389648438| 253.4499969482422|255.27000427246094| 37277559|      0.0|         0.0|  AAPL|2024-12-24 01:35:...|\n",
      "|2024-12-10 00:00:00| 444.3900146484375| 449.6199951171875| 441.6000061035156| 443.3299865722656| 18469500|      0.0|         0.0|  MSFT|2024-12-24 01:35:...|\n",
      "|2024-12-11 00:00:00|444.04998779296875| 450.3500061035156|444.04998779296875|  448.989990234375| 19200200|      0.0|         0.0|  MSFT|2024-12-24 01:35:...|\n",
      "|2024-12-12 00:00:00| 449.1099853515625| 456.1600036621094| 449.1099853515625|449.55999755859375| 20834800|      0.0|         0.0|  MSFT|2024-12-24 01:35:...|\n",
      "|2024-12-13 00:00:00|448.44000244140625|451.42999267578125| 445.5799865722656| 447.2699890136719| 20177800|      0.0|         0.0|  MSFT|2024-12-24 01:35:...|\n",
      "|2024-12-16 00:00:00| 447.2699890136719|452.17999267578125| 445.2799987792969| 451.5899963378906| 23598800|      0.0|         0.0|  MSFT|2024-12-24 01:35:...|\n",
      "|2024-12-17 00:00:00|  451.010009765625| 455.2900085449219|449.57000732421875| 454.4599914550781| 22733500|      0.0|         0.0|  MSFT|2024-12-24 01:35:...|\n",
      "|2024-12-18 00:00:00|451.32000732421875| 452.6499938964844| 437.0199890136719| 437.3900146484375| 24444500|      0.0|         0.0|  MSFT|2024-12-24 01:35:...|\n",
      "|2024-12-19 00:00:00| 441.6199951171875|443.17999267578125|436.32000732421875| 437.0299987792969| 22963700|      0.0|         0.0|  MSFT|2024-12-24 01:35:...|\n",
      "|2024-12-20 00:00:00| 433.1099853515625|  443.739990234375| 428.6300048828125| 436.6000061035156| 64235200|      0.0|         0.0|  MSFT|2024-12-24 01:35:...|\n",
      "|2024-12-23 00:00:00|436.69000244140625| 437.6499938964844| 432.8299865722656|            435.25| 16193103|      0.0|         0.0|  MSFT|2024-12-24 01:35:...|\n",
      "+-------------------+------------------+------------------+------------------+------------------+---------+---------+------------+------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# List of stock symbols and start dates\n",
    "\n",
    "import_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S.\") + str(datetime.now().microsecond)[:3]\n",
    "zone='raw'\n",
    "sink_table='raw.stock_eod_yfinance'\n",
    "config_file_path='registered_table_schemas.yaml'\n",
    "rt=RegisteredTables(zone, sink_table, config_file_path)\n",
    "registered_column_list = rt.get_column_list()\n",
    "\n",
    "yf_param_pairs = [\n",
    "    ('AAPL', '2024-12-10'),\n",
    "    ('MSFT', '2024-12-10'),\n",
    "    ('GOOGL', '2024-12-10'),\n",
    "]\n",
    "\n",
    "# Instantiate the class\n",
    "\n",
    "# Fetch data in parallel\n",
    "stock_data_rows = parallel_fetch(yf_param_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21d3bfb9-8661-4721-bc0a-f3dd17f78834",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Main class for ingestion\n",
    "class RawYFIngestion:\n",
    "    # Basic attributes of the class\n",
    "    def __init__(self, equity_type, zone, sink_table, config_file_path):\n",
    "        \n",
    "        self.equity_type = equity_type\n",
    "        self.zone=zone\n",
    "        self.sink_table = sink_table\n",
    "        self.config_file_path = config_file_path\n",
    "        self.import_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S.\") + str(datetime.now().microsecond)[:3]\n",
    "        rt=RegisteredTables(self.zone, self.sink_table, self.config_file_path)\n",
    "        self.registered_column_list = rt.get_column_list()      \n",
    "        \n",
    "    # Function to fetch data from Yahoo Finance\n",
    "    def fetch_yfinance_record(self, multi_param_pairs):\n",
    "        try:\n",
    "            symbol, start_date = multi_param_pairs\n",
    "            # Fetch stock data using yfinance\n",
    "            quote = yf.Ticker(symbol)\n",
    "            current_date = date.today()\n",
    "            hist = quote.history(start=start_date, end=current_date)\n",
    "            \n",
    "            # Reset index to include Date as a column and format it\n",
    "            hist.reset_index(inplace=True)\n",
    "            hist['Date'] = hist['Date'].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "            # Add symbol and import_time to each row\n",
    "            record_list = [\n",
    "                tuple(row) + (symbol, self.import_time) for row in hist.itertuples(index=False)\n",
    "            ]\n",
    "            \n",
    "            return record_list\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching data for {symbol}: {e}\")\n",
    "            return []  # Return an empty list on error\n",
    "    \n",
    "    # Function to process the records (pass through parameters)\n",
    "    def process_yfinance_record(self, single_param_pair):\n",
    "        # print(f\"Processing {single_param_pair}\")\n",
    "        return self.fetch_yfinance_record(single_param_pair)\n",
    "\n",
    "    # Parallel fetch function\n",
    "    def parallel_fetch(self, multi_param_pairs):        \n",
    "      \n",
    "        # Create RDD from the input parameter pairs\n",
    "        record_rdd = spark.sparkContext.parallelize(multi_param_pairs)\n",
    "        \n",
    "        # Use flatMap to return a flattened list of records\n",
    "        results_rdd = record_rdd.flatMap(self.process_yfinance_record)\n",
    "        \n",
    "        # Collect the results from the RDD and convert to a list of tuples\n",
    "        # results = results_rdd.collect()        \n",
    "        df = spark.createDataFrame(results_rdd, self.registered_column_list)   \n",
    "        df.show()\n",
    "        return df\n",
    " \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f7997dfc-652a-4cf8-bc0b-13d0020639ae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/workspace/finalytics_spark/.venv/lib/python3.10/site-packages/pyspark/serializers.py\", line 459, in dumps\n",
      "    return cloudpickle.dumps(obj, pickle_protocol)\n",
      "  File \"/workspace/finalytics_spark/.venv/lib/python3.10/site-packages/pyspark/cloudpickle/cloudpickle_fast.py\", line 73, in dumps\n",
      "    cp.dump(obj)\n",
      "  File \"/workspace/finalytics_spark/.venv/lib/python3.10/site-packages/pyspark/cloudpickle/cloudpickle_fast.py\", line 632, in dump\n",
      "    return Pickler.dump(self, obj)\n",
      "  File \"/workspace/finalytics_spark/.venv/lib/python3.10/site-packages/pyspark/context.py\", line 466, in __getnewargs__\n",
      "    raise PySparkRuntimeError(\n",
      "pyspark.errors.exceptions.base.PySparkRuntimeError: [CONTEXT_ONLY_VALID_ON_DRIVER] It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.\n"
     ]
    },
    {
     "ename": "PicklingError",
     "evalue": "Could not serialize object: PySparkRuntimeError: [CONTEXT_ONLY_VALID_ON_DRIVER] It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPySparkRuntimeError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m/workspace/finalytics_spark/.venv/lib/python3.10/site-packages/pyspark/serializers.py:459\u001b[0m, in \u001b[0;36mCloudPickleSerializer.dumps\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 459\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcloudpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mPickleError:\n",
      "File \u001b[0;32m/workspace/finalytics_spark/.venv/lib/python3.10/site-packages/pyspark/cloudpickle/cloudpickle_fast.py:73\u001b[0m, in \u001b[0;36mdumps\u001b[0;34m(obj, protocol, buffer_callback)\u001b[0m\n\u001b[1;32m     70\u001b[0m cp \u001b[38;5;241m=\u001b[39m CloudPickler(\n\u001b[1;32m     71\u001b[0m     file, protocol\u001b[38;5;241m=\u001b[39mprotocol, buffer_callback\u001b[38;5;241m=\u001b[39mbuffer_callback\n\u001b[1;32m     72\u001b[0m )\n\u001b[0;32m---> 73\u001b[0m \u001b[43mcp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m file\u001b[38;5;241m.\u001b[39mgetvalue()\n",
      "File \u001b[0;32m/workspace/finalytics_spark/.venv/lib/python3.10/site-packages/pyspark/cloudpickle/cloudpickle_fast.py:632\u001b[0m, in \u001b[0;36mCloudPickler.dump\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 632\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/workspace/finalytics_spark/.venv/lib/python3.10/site-packages/pyspark/context.py:466\u001b[0m, in \u001b[0;36mSparkContext.__getnewargs__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getnewargs__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NoReturn:\n\u001b[1;32m    465\u001b[0m     \u001b[38;5;66;03m# This method is called when attempting to pickle SparkContext, which is always an error:\u001b[39;00m\n\u001b[0;32m--> 466\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkRuntimeError(\n\u001b[1;32m    467\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCONTEXT_ONLY_VALID_ON_DRIVER\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    468\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{},\n\u001b[1;32m    469\u001b[0m     )\n",
      "\u001b[0;31mPySparkRuntimeError\u001b[0m: [CONTEXT_ONLY_VALID_ON_DRIVER] It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m stock_stage \u001b[38;5;241m=\u001b[39m RawYFIngestion(spark, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstock\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraw.stock_eod_yfinance\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mregistered_table_schemas.yaml\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Fetch data in parallel\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m stock_data_rows \u001b[38;5;241m=\u001b[39m \u001b[43mstock_stage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparallel_fetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43myf_param_pairs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 55\u001b[0m, in \u001b[0;36mRawYFIngestion.parallel_fetch\u001b[0;34m(self, multi_param_pairs)\u001b[0m\n\u001b[1;32m     51\u001b[0m results_rdd \u001b[38;5;241m=\u001b[39m record_rdd\u001b[38;5;241m.\u001b[39mflatMap(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_yfinance_record)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# Collect the results from the RDD and convert to a list of tuples\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# results = results_rdd.collect()        \u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresults_rdd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mregistered_column_list\u001b[49m\u001b[43m)\u001b[49m   \n\u001b[1;32m     56\u001b[0m df\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "File \u001b[0;32m/workspace/finalytics_spark/.venv/lib/python3.10/site-packages/pyspark/sql/session.py:1443\u001b[0m, in \u001b[0;36mSparkSession.createDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m   1438\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_pandas \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, pd\u001b[38;5;241m.\u001b[39mDataFrame):\n\u001b[1;32m   1439\u001b[0m     \u001b[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001b[39;00m\n\u001b[1;32m   1440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(SparkSession, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mcreateDataFrame(  \u001b[38;5;66;03m# type: ignore[call-overload]\u001b[39;00m\n\u001b[1;32m   1441\u001b[0m         data, schema, samplingRatio, verifySchema\n\u001b[1;32m   1442\u001b[0m     )\n\u001b[0;32m-> 1443\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_dataframe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1444\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverifySchema\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1445\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/finalytics_spark/.venv/lib/python3.10/site-packages/pyspark/sql/session.py:1483\u001b[0m, in \u001b[0;36mSparkSession._create_dataframe\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m   1480\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[1;32m   1482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, RDD):\n\u001b[0;32m-> 1483\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_createFromRDD\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprepare\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1484\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1485\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_createFromLocal(\u001b[38;5;28mmap\u001b[39m(prepare, data), schema)\n",
      "File \u001b[0;32m/workspace/finalytics_spark/.venv/lib/python3.10/site-packages/pyspark/sql/session.py:1056\u001b[0m, in \u001b[0;36mSparkSession._createFromRDD\u001b[0;34m(self, rdd, schema, samplingRatio)\u001b[0m\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m \u001b[38;5;124;03mCreate an RDD for DataFrame from an existing RDD, returns the RDD and schema.\u001b[39;00m\n\u001b[1;32m   1054\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1055\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(schema, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m-> 1056\u001b[0m     struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inferSchema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrdd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1057\u001b[0m     converter \u001b[38;5;241m=\u001b[39m _create_converter(struct)\n\u001b[1;32m   1058\u001b[0m     tupled_rdd \u001b[38;5;241m=\u001b[39m rdd\u001b[38;5;241m.\u001b[39mmap(converter)\n",
      "File \u001b[0;32m/workspace/finalytics_spark/.venv/lib/python3.10/site-packages/pyspark/sql/session.py:996\u001b[0m, in \u001b[0;36mSparkSession._inferSchema\u001b[0;34m(self, rdd, samplingRatio, names)\u001b[0m\n\u001b[1;32m    975\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_inferSchema\u001b[39m(\n\u001b[1;32m    976\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    977\u001b[0m     rdd: RDD[Any],\n\u001b[1;32m    978\u001b[0m     samplingRatio: Optional[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    979\u001b[0m     names: Optional[List[\u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    980\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m StructType:\n\u001b[1;32m    981\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    982\u001b[0m \u001b[38;5;124;03m    Infer schema from an RDD of Row, dict, or tuple.\u001b[39;00m\n\u001b[1;32m    983\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    994\u001b[0m \u001b[38;5;124;03m    :class:`pyspark.sql.types.StructType`\u001b[39;00m\n\u001b[1;32m    995\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 996\u001b[0m     first \u001b[38;5;241m=\u001b[39m \u001b[43mrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfirst\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    997\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(first, Sized) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(first) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    998\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m PySparkValueError(\n\u001b[1;32m    999\u001b[0m             error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCANNOT_INFER_EMPTY_SCHEMA\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1000\u001b[0m             message_parameters\u001b[38;5;241m=\u001b[39m{},\n\u001b[1;32m   1001\u001b[0m         )\n",
      "File \u001b[0;32m/workspace/finalytics_spark/.venv/lib/python3.10/site-packages/pyspark/rdd.py:2888\u001b[0m, in \u001b[0;36mRDD.first\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2862\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfirst\u001b[39m(\u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRDD[T]\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m   2863\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2864\u001b[0m \u001b[38;5;124;03m    Return the first element in this RDD.\u001b[39;00m\n\u001b[1;32m   2865\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2886\u001b[0m \u001b[38;5;124;03m    ValueError: RDD is empty\u001b[39;00m\n\u001b[1;32m   2887\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2888\u001b[0m     rs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2889\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m rs:\n\u001b[1;32m   2890\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m rs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/workspace/finalytics_spark/.venv/lib/python3.10/site-packages/pyspark/rdd.py:2855\u001b[0m, in \u001b[0;36mRDD.take\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   2852\u001b[0m         taken \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2854\u001b[0m p \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(partsScanned, \u001b[38;5;28mmin\u001b[39m(partsScanned \u001b[38;5;241m+\u001b[39m numPartsToTry, totalParts))\n\u001b[0;32m-> 2855\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunJob\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtakeUpToNumLeft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2857\u001b[0m items \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m res\n\u001b[1;32m   2858\u001b[0m partsScanned \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m numPartsToTry\n",
      "File \u001b[0;32m/workspace/finalytics_spark/.venv/lib/python3.10/site-packages/pyspark/context.py:2510\u001b[0m, in \u001b[0;36mSparkContext.runJob\u001b[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[1;32m   2508\u001b[0m mappedRDD \u001b[38;5;241m=\u001b[39m rdd\u001b[38;5;241m.\u001b[39mmapPartitions(partitionFunc)\n\u001b[1;32m   2509\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2510\u001b[0m sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonRDD\u001b[38;5;241m.\u001b[39mrunJob(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsc\u001b[38;5;241m.\u001b[39msc(), \u001b[43mmappedRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m, partitions)\n\u001b[1;32m   2511\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, mappedRDD\u001b[38;5;241m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[0;32m/workspace/finalytics_spark/.venv/lib/python3.10/site-packages/pyspark/rdd.py:5470\u001b[0m, in \u001b[0;36mPipelinedRDD._jrdd\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   5467\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   5468\u001b[0m     profiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 5470\u001b[0m wrapped_func \u001b[38;5;241m=\u001b[39m \u001b[43m_wrap_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5471\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prev_jrdd_deserializer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd_deserializer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofiler\u001b[49m\n\u001b[1;32m   5472\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5474\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   5475\u001b[0m python_rdd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonRDD(\n\u001b[1;32m   5476\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prev_jrdd\u001b[38;5;241m.\u001b[39mrdd(), wrapped_func, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreservesPartitioning, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_barrier\n\u001b[1;32m   5477\u001b[0m )\n",
      "File \u001b[0;32m/workspace/finalytics_spark/.venv/lib/python3.10/site-packages/pyspark/rdd.py:5268\u001b[0m, in \u001b[0;36m_wrap_function\u001b[0;34m(sc, func, deserializer, serializer, profiler)\u001b[0m\n\u001b[1;32m   5266\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m serializer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mserializer should not be empty\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   5267\u001b[0m command \u001b[38;5;241m=\u001b[39m (func, profiler, deserializer, serializer)\n\u001b[0;32m-> 5268\u001b[0m pickled_command, broadcast_vars, env, includes \u001b[38;5;241m=\u001b[39m \u001b[43m_prepare_for_python_RDD\u001b[49m\u001b[43m(\u001b[49m\u001b[43msc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5269\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   5270\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sc\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mSimplePythonFunction(\n\u001b[1;32m   5271\u001b[0m     \u001b[38;5;28mbytearray\u001b[39m(pickled_command),\n\u001b[1;32m   5272\u001b[0m     env,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5277\u001b[0m     sc\u001b[38;5;241m.\u001b[39m_javaAccumulator,\n\u001b[1;32m   5278\u001b[0m )\n",
      "File \u001b[0;32m/workspace/finalytics_spark/.venv/lib/python3.10/site-packages/pyspark/rdd.py:5251\u001b[0m, in \u001b[0;36m_prepare_for_python_RDD\u001b[0;34m(sc, command)\u001b[0m\n\u001b[1;32m   5248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_prepare_for_python_RDD\u001b[39m(sc: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSparkContext\u001b[39m\u001b[38;5;124m\"\u001b[39m, command: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\u001b[38;5;28mbytes\u001b[39m, Any, Any, Any]:\n\u001b[1;32m   5249\u001b[0m     \u001b[38;5;66;03m# the serialized command will be compressed by broadcast\u001b[39;00m\n\u001b[1;32m   5250\u001b[0m     ser \u001b[38;5;241m=\u001b[39m CloudPickleSerializer()\n\u001b[0;32m-> 5251\u001b[0m     pickled_command \u001b[38;5;241m=\u001b[39m \u001b[43mser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5252\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   5253\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(pickled_command) \u001b[38;5;241m>\u001b[39m sc\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mgetBroadcastThreshold(sc\u001b[38;5;241m.\u001b[39m_jsc):  \u001b[38;5;66;03m# Default 1M\u001b[39;00m\n\u001b[1;32m   5254\u001b[0m         \u001b[38;5;66;03m# The broadcast will have same life cycle as created PythonRDD\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/finalytics_spark/.venv/lib/python3.10/site-packages/pyspark/serializers.py:469\u001b[0m, in \u001b[0;36mCloudPickleSerializer.dumps\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    467\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not serialize object: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (e\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, emsg)\n\u001b[1;32m    468\u001b[0m print_exec(sys\u001b[38;5;241m.\u001b[39mstderr)\n\u001b[0;32m--> 469\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mPicklingError(msg)\n",
      "\u001b[0;31mPicklingError\u001b[0m: Could not serialize object: PySparkRuntimeError: [CONTEXT_ONLY_VALID_ON_DRIVER] It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063."
     ]
    }
   ],
   "source": [
    "\n",
    "# List of stock symbols and start dates\n",
    "yf_param_pairs = [\n",
    "    ('AAPL', '2024-12-10'),\n",
    "    ('MSFT', '2024-12-10'),\n",
    "    ('GOOGL', '2024-12-10'),\n",
    "]\n",
    "\n",
    "# Instantiate the class\n",
    "stock_stage = RawYFIngestion(spark, 'stock', 'raw', 'raw.stock_eod_yfinance', 'registered_table_schemas.yaml')\n",
    "\n",
    "# Fetch data in parallel\n",
    "stock_data_rows = stock_stage.parallel_fetch(yf_param_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc09c08b-a34a-4aa7-ac70-978a84522411",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Main class for ingestion\n",
    "class RawYFIngestion:\n",
    "    # Basic attributes of the class\n",
    "    def __init__(self, spark, equity_type, zone, sink_table, config_file_path):\n",
    "        self.spark=spark\n",
    "        self.equity_type = equity_type\n",
    "        self.zone=zone\n",
    "        self.sink_table = sink_table\n",
    "        self.config_file_path = config_file_path\n",
    "        self.import_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S.\") + str(datetime.now().microsecond)[:3]\n",
    "        rt=RegisteredTables(self.zone, self.sink_table, self.config_file_path)\n",
    "        self.registered_column_list = rt.get_column_list()\n",
    "        self.registered_struct_type = rt.get_struct_type()        \n",
    "        \n",
    "    # Function to fetch data from Yahoo Finance\n",
    "    def fetch_yfinance_record(self, multi_param_pairs):\n",
    "        try:\n",
    "            symbol, start_date = multi_param_pairs\n",
    "            # Fetch stock data using yfinance\n",
    "            quote = yf.Ticker(symbol)\n",
    "            current_date = date.today()\n",
    "            hist = quote.history(start=start_date, end=current_date)\n",
    "            \n",
    "            # Reset index to include Date as a column and format it\n",
    "            hist.reset_index(inplace=True)\n",
    "            hist['Date'] = hist['Date'].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "            # Add symbol and import_time to each row\n",
    "            record_list = [\n",
    "                tuple(row) + (symbol, self.import_time) for row in hist.itertuples(index=False)\n",
    "            ]\n",
    "            \n",
    "            return record_list\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching data for {symbol}: {e}\")\n",
    "            return []  # Return an empty list on error\n",
    "    \n",
    "    # Function to process the records (pass through parameters)\n",
    "    def process_yfinance_record(self, single_param_pair):\n",
    "        # print(f\"Processing {single_param_pair}\")\n",
    "        return self.fetch_yfinance_record(single_param_pair)\n",
    "\n",
    "    # Parallel fetch function\n",
    "    def parallel_fetch(self, multi_param_pairs):        \n",
    "      \n",
    "        # Create RDD from the input parameter pairs\n",
    "        record_rdd = self.spark.sparkContext.parallelize(multi_param_pairs)\n",
    "        \n",
    "        # Use flatMap to return a flattened list of records\n",
    "        results_rdd = record_rdd.flatMap(self.process_yfinance_record)\n",
    "        \n",
    "        # Collect the results from the RDD and convert to a list of tuples\n",
    "        # results = results_rdd.collect()        \n",
    "        df = self.spark.createDataFrame(results_rdd, self.registered_column_list)   \n",
    "        df.show()\n",
    "        return df\n",
    " \n",
    "\n",
    "    \n",
    "\n",
    "# # List of stock symbols and start dates\n",
    "# yf_param_pairs = [\n",
    "#     ('AAPL', '2024-12-10'),\n",
    "#     ('MSFT', '2024-12-10'),\n",
    "#     ('GOOGL', '2024-12-10'),\n",
    "# ]\n",
    "\n",
    "# # Instantiate the class\n",
    "# stock_stage = RawYFIngestion('stock', 'raw', 'raw.stock_eod_yfinance', 'registered_table_schemas.yaml')\n",
    "\n",
    "# # Fetch data in parallel\n",
    "# stock_data_rows = stock_stage.parallel_fetch(yf_param_pairs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baea7bb3-64a3-4cd1-b0d0-c54eb02b12f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "# Create an empty dictionary\n",
    "config_dict=dict()\n",
    "# Add configuration data to the dictionary\n",
    "config_dict[\"server\"]={'port': 8080, 'host': '0.0.0.0'}\n",
    "config_dict[\"logging\"]={'level': 'info', 'file': '/var/log/web-server.log'}\n",
    "config_dict[\"database\"]={'url': 'postgres://user:password@host:port/database','pool': 100}\n",
    "# Create another dictionary\n",
    "details_dict= {\"Website Name\":\"HoneyBadger\",\"Author\":\"Aditya\", \"Topic\":\"Configuration Files\", \"Content Type\":\"Blog\"}\n",
    "x_dict= {\"Website Name\":\"HoneyBadger\",\"Author\":\"Aditya\", \"Topic\":\"Configuration Files\", \"Content Type\":\"Blog\"}\n",
    "list_of_dicts=[config_dict,details_dict, x_dict]\n",
    "# Save data to a YAML file\n",
    "with open(\"web-server-details-1.yaml\",\"w\") as file_object:\n",
    "    yaml.dump_all(list_of_dicts,file_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3100b031-7748-460b-963f-16c51f4b185d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "with open(\"config.yaml\",\"r\") as file_object:\n",
    "    config_dict=yaml.load(file_object,Loader=yaml.SafeLoader)\n",
    "print(config_dict)\n",
    "print(config_dict[\"iceberg_env\"][\"STORAGE_URI\"])\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fd244c-cf7a-40d7-b987-ec829fd90e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType, TimestampType\n",
    "import yfinance as yf\n",
    "from pyspark.sql import Row\n",
    "from multiprocessing import Pool\n",
    "\n",
    "# Initialize PySpark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"StockDataLoader\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define a schema for the stock data\n",
    "schema = StructType([\n",
    "    StructField(\"Ticker\", StringType(), True),\n",
    "    StructField(\"Date\", TimestampType(), True),\n",
    "    StructField(\"Open\", FloatType(), True),\n",
    "    StructField(\"High\", FloatType(), True),\n",
    "    StructField(\"Low\", FloatType(), True),\n",
    "    StructField(\"Close\", FloatType(), True),\n",
    "    StructField(\"Adj Close\", FloatType(), True),\n",
    "    StructField(\"Volume\", FloatType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa57384-05ba-4aa6-be47-fa44039ae014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fetch stock data for a single ticker\n",
    "from datetime import date, datetime, timedelta\n",
    "\n",
    "\n",
    "def fetch_stock_data(symbol):\n",
    "    try:\n",
    "        quote = yf.Ticker(symbol)\n",
    "        start_date = '2024-12-01'\n",
    "        current_date = date.today()\n",
    "        hist = quote.history(start=start_date, end=current_date)\n",
    "        import_time=datetime.now()\n",
    "        # if hist.empty:\n",
    "        #     sql_script = f\"UPDATE fin.stock_symbol SET is_valid= false WHERE symbol='{symbol}';\"\n",
    "        #     # print(sql_script)\n",
    "        #     self.execute_sql_script(sql_script)\n",
    "\n",
    "        # Reset index to include the Date column in the DataFrame\n",
    "        hist.reset_index(inplace=True)\n",
    "        # # get column list with extra fields\n",
    "        column_list = [x.lower().replace(\" \", \"_\") for x in hist.columns]\n",
    "        extra_field_list = ['symbol', 'import_time']\n",
    "        column_list.extend(extra_field_list)\n",
    "\n",
    "        # get records with appended extra fields\n",
    "        hist_records_map = hist.itertuples(index=False)\n",
    "        record_list = [tuple(row) + (symbol,) + (import_time,) for row in hist_records_map]\n",
    "\n",
    "        return column_list, record_list\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data for {ticker}: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af29e04b-276e-4e63-bb4f-66437dc9586f",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_list, record_list=fetch_stock_data('C')\n",
    "print(column_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8da32b-3482-4bfd-bbfe-f22e28ccc617",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel_fetch(tickers):\n",
    "    with Pool(processes=4) as pool:  # Adjust the number of processes based on your machine's capacity\n",
    "        results = pool.map(fetch_stock_data, tickers)\n",
    "    return [row for sublist in results for row in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0ce568-1c9d-4a89-9e41-914aef2b2e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of tickers to fetch\n",
    "tickers = [\"AAPL\", \"MSFT\", \"GOOGL\", \"AMZN\"]  # Add more tickers as needed\n",
    "\n",
    "# Fetch data in parallel\n",
    "stock_data_rows = parallel_fetch(tickers)\n",
    "\n",
    "# Convert the data to a Spark DataFrame\n",
    "stock_df = spark.createDataFrame(stock_data_rows, schema=schema)\n",
    "\n",
    "# Show some rows\n",
    "stock_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea2ad7f-df78-468e-be27-c3338630a0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date, datetime\n",
    "def get_yfinance_record(symbol):\n",
    "    try:\n",
    "        quote = yf.Ticker(symbol)\n",
    "        start_date = '2024-12-01'\n",
    "        current_date = date.today()\n",
    "        print(current_date)\n",
    "        # print(start_date+timedelta(days=1))\n",
    "\n",
    "        hist = quote.history(start=start_date, end=current_date)\n",
    "        print(hist)\n",
    "        # if hist.empty:\n",
    "        #     sql_script = f\"UPDATE fin.stock_symbol SET is_valid= false WHERE symbol='{symbol}';\"\n",
    "        #     # print(sql_script)\n",
    "        #     self.execute_sql_script(sql_script)\n",
    "\n",
    "        # Reset index to include the Date column in the DataFrame\n",
    "        hist.reset_index(inplace=True)\n",
    "\n",
    "        # # get column list with extra fields\n",
    "        # column_list = [x.lower().replace(\" \", \"_\") for x in hist.columns]\n",
    "        # extra_field_list = ['symbol', 'import_time']\n",
    "        # column_list.extend(extra_field_list)\n",
    "        # print(column_list)\n",
    "\n",
    "        # # get records with extra fields\n",
    "        # hist_records_map = hist.itertuples(index=False)\n",
    "\n",
    "        # record_list = [tuple(row) + (symbol,) + (self.import_time,) for row in hist_records_map]\n",
    "\n",
    "        # print(record_list)\n",
    "\n",
    "        # return column_list, record_list\n",
    "    except Exception as e:\n",
    "        print(\"x\")\n",
    "        # print(f\"An error occurred: {e}\")\n",
    "        # if \"delisted\" in e:\n",
    "        #     print(\"abc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de2a1d9-48ba-41ef-8be7-b2b0dce8b8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_yfinance_record('C')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0819f47b-0737-453a-b4a7-dfcd27ad010c",
   "metadata": {},
   "outputs": [],
   "source": [
    "list1 = [1, 2, 3, 4, 5]\n",
    "list2 = [3, 4, 5]\n",
    "\n",
    "# Convert lists to sets\n",
    "set1 = set(list1)\n",
    "set2 = set(list2)\n",
    "\n",
    "# Find the difference between the sets\n",
    "difference1 = set1 - set2  # Elements in list1 but not in list2\n",
    "difference2 = set2 - set1  # Elements in list2 but not in list1\n",
    "\n",
    "\n",
    "print(len(difference1))\n",
    "print(\"Difference (list1 - list2):\", difference1)\n",
    "print(len(difference2))\n",
    "print(\"Difference (list2 - list1):\", difference2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304cdfc5-9860-4c28-9048-c0164ca50aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit\n",
    "import yfinance as yf\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"YahooFinanceData\").getOrCreate()\n",
    "\n",
    "# Input Data: Tickers with their respective start dates\n",
    "tickers_data = [\n",
    "    {\"ticker\": \"AAPL\", \"start_date\": \"2020-01-01\"},\n",
    "    {\"ticker\": \"MSFT\", \"start_date\": \"2019-01-01\"},\n",
    "    {\"ticker\": \"GOOGL\", \"start_date\": \"2021-01-01\"},\n",
    "]\n",
    "\n",
    "# Create RDD from input data\n",
    "tickers_rdd = spark.sparkContext.parallelize(tickers_data)\n",
    "\n",
    "# Function to fetch Yahoo Finance data\n",
    "def fetch_data(params):\n",
    "    ticker = params[\"ticker\"]\n",
    "    start_date = params[\"start_date\"]\n",
    "    try:\n",
    "        data = yf.download(ticker, start=start_date)\n",
    "        data.reset_index(inplace=True)\n",
    "        data[\"Ticker\"] = ticker  # Add a column for ticker\n",
    "        return data.to_dict(\"records\")  # Convert to a list of dictionaries\n",
    "    except Exception as e:\n",
    "        return [{\"Ticker\": ticker, \"Error\": str(e)}]\n",
    "\n",
    "# Fetch data in parallel\n",
    "results = tickers_rdd.flatMap(fetch_data).collect()\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "flattened_results = [item for sublist in results for item in sublist]\n",
    "df = spark.createDataFrame(flattened_results)\n",
    "\n",
    "# Show results\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83236d6c-adb3-4f93-89f7-5dbeec1a8e27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "from datetime import date, datetime, timedelta\n",
    "# import psycopg2.extras\n",
    "# from joblib import Parallel, delayed\n",
    "# import psycopg2\n",
    "import multiprocessing\n",
    "# from pgcopy import CopyManager\n",
    "from io import StringIO\n",
    "import yaml\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# with open(\"config.yaml\",\"r\") as file_object:\n",
    "#     documents=yaml.safe_load_all(file_object)\n",
    "#     for doc in documents:\n",
    "#         doc_name = doc['document_name']\n",
    "#         if doc_name=='yfinance_stock':\n",
    "#             registered_col_list=doc['column_list']\n",
    "\n",
    "#         if doc_name=='iceberg_env':\n",
    "#             CATALOG_URI = doc['catalog_uri'] # Nessie Server URI\n",
    "#             WAREHOUSE = doc['warehouse']     # Minio Address to Write to\n",
    "#             STORAGE_URI = doc['storage_uri'] # Minio IP address from docker inspec\n",
    "\n",
    "\n",
    "\n",
    "class MyCustomException(Exception):\n",
    "    pass\n",
    "\n",
    "class YFinanceStageIngestion:\n",
    "    def __init__(self, equity_type, destination):\n",
    "        self.equity_type = equity_type\n",
    "        self.destination = destination\n",
    "        self.import_time=datetime.now()\n",
    "        \n",
    "        # Get yfinance stock data registered column list\n",
    "        with open(\"config.yaml\",\"r\") as file_object:\n",
    "            documents=yaml.safe_load_all(file_object)\n",
    "            for doc in documents:\n",
    "                doc_name = doc['document_name']\n",
    "                if doc_name==f\"yfinance_{equity_type}\":\n",
    "                    self.registered_col_list=doc['registered_column_list']\n",
    "\n",
    "    \n",
    "        # self.import_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S.\") + str(datetime.now().microsecond)[:3]\n",
    "    \n",
    "    def fetch_yfinance_data(self, args):\n",
    "        try:\n",
    "            symbol = args[\"symbol\"]\n",
    "            start_date = args[\"start_date\"]\n",
    "            \n",
    "            quote = yf.Ticker(symbol)\n",
    "            # start_date = '2024-12-01'\n",
    "            current_date = date.today()\n",
    "            hist = quote.history(start=start_date, end=current_date)\n",
    "            # import_time=datetime.now()\n",
    "    \n",
    "            # Reset index to include the Date column in the DataFrame\n",
    "            hist.reset_index(inplace=True)\n",
    "            \n",
    "            # Standardize the hist column name by lowering the case of the original column name and replacing space with underscore\n",
    "            # This standardized column names are reginstered in configuration file\n",
    "            standardized_column_list = [x.lower().replace(\" \", \"_\") for x in hist.columns]  \n",
    "            \n",
    "            # Add symbol and import_time in column list\n",
    "            extra_field_list = ['symbol', 'import_time']\n",
    "            standardized_column_list.extend(extra_field_list)\n",
    "            \n",
    "            # Check whether the standardized column names match the registered ones   \n",
    "            set_standardized = set(standardized_column_list)\n",
    "            set_registered = set(self.registered_column_list)            \n",
    "            if set_standardized!=set_registered:\n",
    "                raise MyCustomException(f\"Error: standardized_column_list {str(standardized_column_list)} does not match registered_column_list {str(registered_column_list)}!\")\n",
    "\n",
    "            # Add symbol and import_time in each record\n",
    "            hist_records_map = hist.itertuples(index=False)            \n",
    "            record_list = [tuple(row) + (symbol,) + (self.import_time,) for row in hist_records_map]  \n",
    "            \n",
    "            return standardized_column_list, record_list\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching data for {symbol}: {e}\")\n",
    "            return []\n",
    "\n",
    "\n",
    "    def parallel_fetch(self, param_pairs):\n",
    "        yfinance_data_rdd = spark.sparkContext.parallelize(param_pairs)\n",
    "        results = yfinance_data_rdd.flatMap(self.fetch_yfinance_data).collect()\n",
    "        flattened_results = [item for sublist in results for item in sublist]\n",
    "        df = spark.createDataFrame(flattened_results)\n",
    "        df.show(truncate=False)\n",
    "            \n",
    "            \n",
    "\n",
    "stock_stage = YFinanceStageIngestion('stock', 'mytable')\n",
    "        \n",
    "# List of tickers to fetch\n",
    "stock_param_pairs = [(\"AAPL\", \"2024-12-1\"), (\"MSFT\", \"2024-12-5\"), (\"GOOGL\", \"2024-12-9\")]  # Add more tickers as needed\n",
    "\n",
    "stock_stage.parallel_fetch(stock_param_pairs)\n",
    "\n",
    "# Fetch data in parallel\n",
    "stock_data_rows = parallel_fetch(stock_param_pairs)\n",
    "\n",
    "# # Convert the data to a Spark DataFrame\n",
    "# stock_df = spark.createDataFrame(stock_data_rows, schema=schema)\n",
    "\n",
    "# # Show some rows\n",
    "# stock_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009a8d50-9ff1-45c9-8155-092bef2b8a10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa474609-2cbb-43a6-80e5-49026aeb044a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit\n",
    "import yfinance as yf\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"YahooFinanceData\").getOrCreate()\n",
    "\n",
    "# Input Data: Tickers with their respective start dates\n",
    "tickers_data = [\n",
    "    {\"ticker\": \"AAPL\", \"start_date\": \"2020-01-01\"},\n",
    "    {\"ticker\": \"MSFT\", \"start_date\": \"2019-01-01\"},\n",
    "    {\"ticker\": \"GOOGL\", \"start_date\": \"2021-01-01\"},\n",
    "]\n",
    "\n",
    "# Create RDD from input data\n",
    "tickers_rdd = spark.sparkContext.parallelize(tickers_data)\n",
    "\n",
    "# Function to fetch Yahoo Finance data\n",
    "def fetch_data(params):\n",
    "    ticker = params[\"ticker\"]\n",
    "    start_date = params[\"start_date\"]\n",
    "    try:\n",
    "        quote = yf.Ticker(ticker)\n",
    "        hist = quote.history(start=start_date, end=current_date)\n",
    "        hist.reset_index(inplace=True)\n",
    "        return hist.to_dict(\"records\")  # Convert to a list of dictionaries\n",
    "    except Exception as e:\n",
    "        return [{\"Ticker\": ticker, \"Error\": str(e)}]\n",
    "\n",
    "# Fetch data in parallel\n",
    "results = tickers_rdd.flatMap(fetch_data).collect()\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "flattened_results = [item for sublist in results for item in sublist]\n",
    "df = spark.createDataFrame(flattened_results)\n",
    "\n",
    "# Show results\n",
    "df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a3930d-cf0d-45c6-8cdd-0c86390a6437",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba4ceb6-906b-4e34-8f51-9e646440b579",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import yfinance as yf\n",
    "import numpy as np\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, DateType\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"YahooFinanceData\").getOrCreate()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "symbol = 'aasdfljksadfjlksd'\n",
    "start_date = \"\"\n",
    "\n",
    "quote = yf.Ticker('AAPL')\n",
    "# start_date = '2024-12-01'\n",
    "current_date = date.today()\n",
    "hist = quote.history()\n",
    "# import_time=datetime.now()\n",
    "\n",
    "# Reset index to include the Date column in the DataFrame\n",
    "hist.reset_index(inplace=True)\n",
    "\n",
    "# Standardize the hist column name by lowering the case of the original column name and replacing space with underscore\n",
    "# This standardized column names are reginstered in configuration file\n",
    "standardized_column_list = [x.lower().replace(\" \", \"_\") for x in hist.columns]  \n",
    "\n",
    "# Add symbol and import_time in column list\n",
    "extra_field_list = ['symbol', 'import_time']\n",
    "standardized_column_list.extend(extra_field_list)\n",
    "print(standardized_column_list)\n",
    "\n",
    "# # Check whether the standardized column names match the registered ones   \n",
    "# set_standardized = set(standardized_column_list)\n",
    "# set_registered = set(self.registered_column_list)            \n",
    "# if set_standardized!=set_registered:\n",
    "#     raise MyCustomException(f\"Error: standardized_column_list {str(standardized_column_list)} does not match registered_column_list {str(registered_column_list)}!\")\n",
    "\n",
    "# # Add symbol and import_time in each record\n",
    "# hist_records_map = hist.itertuples(index=False)            \n",
    "# record_list = [tuple(row) + (symbol,) + (self.import_time,) for row in hist_records_map]  \n",
    "\n",
    "# return standardized_column_list, record_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad98a3a-3105-4726-88fd-cefa466a52fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType, TimestampType, IntegerType\n",
    "import yfinance as yf\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"YahooFinanceData\").getOrCreate()\n",
    "\n",
    "# Input data: List of tickers (start_date is fixed as '2024-12-01' for all)\n",
    "tickers_data = [\n",
    "    ('AAPL', '2024-12-01'),\n",
    "    ('MSFT', '2024-12-01'),\n",
    "    ('GOOGL', '2024-12-01'),\n",
    "]\n",
    "\n",
    "# Create an RDD from the input data\n",
    "tickers_rdd = spark.sparkContext.parallelize(tickers_data)\n",
    "\n",
    "# Fetch function to retrieve all stock data fields with ticker and fixed start_date\n",
    "def fetch_stock_data_with_ticker_and_date(ticker, start_date):\n",
    "    try:\n",
    "        # Fetch stock data using yfinance\n",
    "        stock_data = yf.Ticker(ticker).history(start=start_date)\n",
    "        stock_data.reset_index(inplace=True)  # Reset index to include 'Date' as a column\n",
    "        stock_data['Date'] = stock_data['Date'].dt.strftime('%Y-%m-%d %H:%M:%S%z')\n",
    "        \n",
    "        # Convert all rows to tuples and append ticker and start_date\n",
    "        return [tuple(row) + (ticker, start_date) for row in stock_data.itertuples(index=False)]\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data for {ticker}: {e}\")\n",
    "        return []\n",
    "\n",
    "# Function to process ticker data and call the fetch function with unpacked params\n",
    "def process_ticker_data(ticker_data):\n",
    "    print(ticker_data)\n",
    "    return fetch_stock_data_with_ticker_and_date(*ticker_data)\n",
    "\n",
    "# Use flatMap to process data in parallel and collect all rows as a single list of tuples\n",
    "results_rdd = tickers_rdd.flatMap(process_ticker_data)\n",
    "\n",
    "# Collect the data as a list of tuples\n",
    "results = results_rdd.collect()\n",
    "\n",
    "# Define the schema for the DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"Date\", StringType(), True),        # 'Date' column as Timestamp\n",
    "    StructField(\"Open\", FloatType(), True),            # 'Open' column as Float\n",
    "    StructField(\"High\", FloatType(), True),            # 'High' column as Float\n",
    "    StructField(\"Low\", FloatType(), True),             # 'Low' column as Float\n",
    "    StructField(\"Close\", FloatType(), True),           # 'Close' column as Float\n",
    "    StructField(\"Volume\", IntegerType(), True),            # 'Volume' column as Int\n",
    "    StructField(\"Dividends\", FloatType(), True),       # 'Dividends' column as Float\n",
    "    StructField(\"Stock Splits\", FloatType(), True),    # 'Stock Splits' column as Float\n",
    "    StructField(\"Ticker\", StringType(), True),         # 'Ticker' column as String\n",
    "    StructField(\"Start Date\", StringType(), True)      # 'Start Date' column as String\n",
    "])\n",
    "\n",
    "# Create a DataFrame using the schema\n",
    "df = spark.createDataFrame(results, schema)\n",
    "\n",
    "# Show the DataFrame content\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a179b8-b0c3-4e90-9c01-3a660e1decbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import yfinance as yf\n",
    "import numpy as np\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"YahooFinanceData\").getOrCreate()\n",
    "\n",
    "# Input data: List of tuples with tickers and start dates\n",
    "tickers_data = [\n",
    "    ('AAPL', '2024-12-01'),\n",
    "    ('MSFT', '2024-12-01'),\n",
    "    ('GOOGL', '2024-12-01'),\n",
    "]\n",
    "\n",
    "# Create an RDD from the input data\n",
    "tickers_rdd = spark.sparkContext.parallelize(tickers_data)\n",
    "\n",
    "# Function to fetch daily data for a ticker using yf.Ticker\n",
    "def fetch_data(params):\n",
    "    ticker, start_date = params\n",
    "    try:\n",
    "        # Fetch the ticker object\n",
    "        quote = yf.Ticker(ticker)\n",
    "        \n",
    "        # Get historical market data\n",
    "        data = quote.history(start=start_date)\n",
    "        \n",
    "        # Reset index to include the 'Date' column\n",
    "        data.reset_index(inplace=True)\n",
    "        \n",
    "        \n",
    "        rows = data.apply(lambda row: tuple(row), axis=1).tolist()       \n",
    "\n",
    "        return rows\n",
    "    except Exception as e:\n",
    "        # Return empty list on error\n",
    "        return []\n",
    "\n",
    "# Use flatMap to fetch data in parallel\n",
    "results_rdd = tickers_rdd.flatMap(fetch_data)\n",
    "\n",
    "# Create the DataFrame by inferring schema from the RDD\n",
    "df = spark.createDataFrame(results_rdd)\n",
    "\n",
    "# Show the results\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e5f9ba-dc0f-4938-8816-ab462bd8b8da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import numpy as np\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, DateType\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType, TimestampType, IntegerType\n",
    "import yfinance as yf\n",
    "\n",
    "\n",
    "from datetime import date, datetime, timedelta\n",
    "\n",
    "class MyCustomException(Exception):\n",
    "    pass\n",
    "\n",
    "class YFinanceStageIngestion:\n",
    "    def __init__(self, equity_type, destination):\n",
    "        self.equity_type = equity_type\n",
    "        self.destination = destination\n",
    "        self.import_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S.\") + str(datetime.now().microsecond)[:3]      \n",
    "        \n",
    "   \n",
    "    def fetch_yfinance_record(self, params):\n",
    "        try:\n",
    "            symbol, start_date = params      \n",
    "            quote = yf.Ticker(symbol)\n",
    "            current_date = date.today()\n",
    "            hist = quote.history(start=start_date, end=current_date)\n",
    "   \n",
    "            # Reset index to include the Date column in the DataFrame\n",
    "            hist.reset_index(inplace=True)\n",
    "            hist['Date'] = hist['Date'].dt.strftime('%Y-%m-%d %H:%M:%S%z')\n",
    "\n",
    "            # Add symbol and import_time in each record\n",
    "            hist_records_map = hist.itertuples(index=False)            \n",
    "            record_list = [tuple(row) + (symbol, self.import_time) for row in hist_records_map]\n",
    "            record_list=[]\n",
    "            return record_list\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching data for {symbol}: {e}\")\n",
    "            return []    \n",
    "    \n",
    "    def process_yfinance_record(self, param):\n",
    "        print(param)\n",
    "        return self.fetch_yfinance_record(*param)\n",
    "\n",
    "        \n",
    "    def parallel_fetch(self, param_pairs):       \n",
    "        spark = SparkSession.builder.appName(\"YahooFinanceData\").getOrCreate()\n",
    "        record_rdd = spark.sparkContext.parallelize(param_pairs)        \n",
    "        results_rdd = record_rdd.flatMap(self.process_yfinance_record)  \n",
    "        results = results_rdd.collect()\n",
    "        print(results)\n",
    "        \n",
    "\n",
    "stock_param_pairs = [\n",
    "    ('AAPL', '2024-12-10'),\n",
    "    ('MSFT', '2024-12-10'),\n",
    "    ('GOOGL', '2024-12-10'),\n",
    "]\n",
    "      \n",
    "stock_stage = YFinanceStageIngestion('stock', 'mytable')\n",
    "        \n",
    "\n",
    "# Fetch data in parallel\n",
    "stock_data_rows = stock_stage.parallel_fetch(stock_param_pairs)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Input data: List of tickers (start_date is fixed as '2024-12-01' for all)\n",
    "# tickers_data = [\n",
    "#     ('AAPL', '2024-12-01'),\n",
    "#     ('MSFT', '2024-12-01'),\n",
    "#     ('GOOGL', '2024-12-01'),\n",
    "# ]\n",
    "\n",
    "# # Create an RDD from the input data\n",
    "# tickers_rdd = spark.sparkContext.parallelize(tickers_data)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65748f6-d437-4a7f-b2d2-3b3e00992ef0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType, TimestampType\n",
    "import yfinance as yf\n",
    "from datetime import datetime, date\n",
    "\n",
    "# Define the custom exception\n",
    "class MyCustomException(Exception):\n",
    "    pass\n",
    "\n",
    "# Main class for ingestion\n",
    "class YFinanceStageIngestion:\n",
    "    def __init__(self, equity_type, destination):\n",
    "        self.equity_type = equity_type\n",
    "        self.destination = destination\n",
    "        self.import_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S.\") + str(datetime.now().microsecond)[:3]      \n",
    "\n",
    "    # Function to fetch data from Yahoo Finance\n",
    "    def fetch_yfinance_record(self, params):\n",
    "        try:\n",
    "            symbol, start_date = params\n",
    "            # Fetch stock data using yfinance\n",
    "            quote = yf.Ticker(symbol)\n",
    "            current_date = date.today()\n",
    "            hist = quote.history(start=start_date, end=current_date)\n",
    "            \n",
    "            # Reset index to include Date as a column and format it\n",
    "            hist.reset_index(inplace=True)\n",
    "            hist['Date'] = hist['Date'].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "            # Add symbol and import_time to each row\n",
    "            record_list = [\n",
    "                tuple(row) + (symbol, self.import_time) for row in hist.itertuples(index=False)\n",
    "            ]\n",
    "            return record_list\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching data for {symbol}: {e}\")\n",
    "            return []  # Return an empty list on error\n",
    "    \n",
    "    # Function to process the records (pass through parameters)\n",
    "    def process_yfinance_record(self, param):\n",
    "        print(f\"Processing {param}\")\n",
    "        return self.fetch_yfinance_record(param)\n",
    "\n",
    "    # Parallel fetch function\n",
    "    def parallel_fetch(self, param_pairs):\n",
    "        # Create Spark session\n",
    "        spark = SparkSession.builder.appName(\"YahooFinanceData\").getOrCreate()\n",
    "        \n",
    "        # Create RDD from the input parameter pairs\n",
    "        record_rdd = spark.sparkContext.parallelize(param_pairs)\n",
    "        \n",
    "        # Use flatMap to return a flattened list of records\n",
    "        results_rdd = record_rdd.flatMap(self.process_yfinance_record)\n",
    "        \n",
    "        # Collect the results from the RDD\n",
    "        results = results_rdd.collect()\n",
    "        return results\n",
    "\n",
    "# List of stock symbols and start dates\n",
    "stock_param_pairs = [\n",
    "    ('AAPL', '2024-12-10'),\n",
    "    ('MSFT', '2024-12-10'),\n",
    "    ('GOOGL', '2024-12-10'),\n",
    "]\n",
    "\n",
    "# Instantiate the class\n",
    "stock_stage = YFinanceStageIngestion('stock', 'mytable')\n",
    "\n",
    "# Fetch data in parallel\n",
    "stock_data_rows = stock_stage.parallel_fetch(stock_param_pairs)\n",
    "\n",
    "spark = SparkSession.builder.appName(\"YahooFinanceDatax\").getOrCreate()\n",
    "\n",
    "\n",
    "# You can also load the result into a DataFrame if required\n",
    "from pyspark.sql import Row\n",
    "schema = ['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Dividends', 'Stock Splits', 'Ticker', 'ImportTime']\n",
    "rdd_rows = spark.sparkContext.parallelize(stock_data_rows)\n",
    "df = spark.createDataFrame(rdd_rows, schema)\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1a4b76-06bc-4799-9df5-54651bc9ab3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import yfinance as yf\n",
    "import numpy as np\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"YahooFinanceData\").getOrCreate()\n",
    "\n",
    "# Input data: List of tuples with tickers and start dates\n",
    "tickers_data = [\n",
    "    ('AAPL', '2024-12-01'),\n",
    "    ('MSFT', '2024-12-01'),\n",
    "    ('GOOGL', '2024-12-01'),\n",
    "]\n",
    "\n",
    "# Create an RDD from the input data\n",
    "tickers_rdd = spark.sparkContext.parallelize(tickers_data)\n",
    "\n",
    "# Function to fetch daily data for a ticker using yf.Ticker\n",
    "def fetch_data(params):\n",
    "    ticker, start_date = params\n",
    "    try:\n",
    "        # Fetch the ticker object\n",
    "        quote = yf.Ticker(ticker)\n",
    "        \n",
    "        # Get historical market data\n",
    "        data = quote.history(start=start_date)\n",
    "        \n",
    "        # Reset index to include the 'Date' column\n",
    "        data.reset_index(inplace=True)\n",
    "        \n",
    "        \n",
    "        rows = data.apply(lambda row: tuple(row), axis=1).tolist()       \n",
    "\n",
    "        return rows\n",
    "    except Exception as e:\n",
    "        # Return empty list on error\n",
    "        return []\n",
    "\n",
    "# Use flatMap to fetch data in parallel\n",
    "results_rdd = tickers_rdd.flatMap(fetch_data)\n",
    "\n",
    "# Create the DataFrame by inferring schema from the RDD\n",
    "df = spark.createDataFrame(results_rdd)\n",
    "\n",
    "# Show the results\n",
    "df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7a2b46-da60-4d92-a6bd-5e7a0bdd0a99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import yfinance as yf\n",
    "import numpy as np\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"YahooFinanceData\").getOrCreate()\n",
    "\n",
    "# Input data: List of tuples with tickers and start dates\n",
    "tickers_data = [\n",
    "    ('AAPL', '2020-01-01'),\n",
    "    ('MSFT', '2019-01-01'),\n",
    "    ('GOOGL', '2021-01-01'),\n",
    "]\n",
    "\n",
    "# Create an RDD from the input data\n",
    "tickers_rdd = spark.sparkContext.parallelize(tickers_data)\n",
    "\n",
    "# Function to fetch daily data for a ticker using yf.Ticker\n",
    "def fetch_data(params):\n",
    "    ticker, start_date = params\n",
    "    try:\n",
    "        # Fetch the ticker object\n",
    "        quote = yf.Ticker(ticker)\n",
    "        \n",
    "        # Get historical market data\n",
    "        data = quote.history(start=start_date)\n",
    "        \n",
    "        # Reset index to include the 'Date' column\n",
    "        data.reset_index(inplace=True)\n",
    "        \n",
    "        # Convert the DataFrame to a list of tuples\n",
    "        rows = data.apply(lambda row: tuple(row), axis=1).tolist()\n",
    "        \n",
    "        return rows\n",
    "    except Exception as e:\n",
    "        # Return empty list on error\n",
    "        print(f\"Error fetching data for {ticker}: {e}\")\n",
    "        return []\n",
    "\n",
    "# Use flatMap to fetch data in parallel\n",
    "results_rdd = tickers_rdd.flatMap(fetch_data)\n",
    "\n",
    "# Extract column names dynamically from the data\n",
    "# After resetting the index, the columns should include 'Date', 'Open', 'Close', etc.\n",
    "data = next(iter(results_rdd))  # Get the first element to infer columns\n",
    "columns = [col for col in data[0]._fields]  # Extract column names dynamically from the first row\n",
    "\n",
    "# Create the DataFrame by inferring schema from the RDD\n",
    "df = spark.createDataFrame(results_rdd, schema=columns)\n",
    "\n",
    "# Show the results\n",
    "df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3335d545-6ffa-4492-9e56-a1fc59d97de0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "symbol='C'\n",
    "quote = yf.Ticker(symbol)\n",
    "start_date ='2024-12-11'\n",
    "current_date = date.today()\n",
    "# print(start_date+timedelta(days=1))\n",
    "\n",
    "hist = quote.history(start=start_date, end=current_date)\n",
    "import_time=datetime.now()\n",
    "\n",
    "# Reset index to include the Date column in the DataFrame\n",
    "hist.reset_index(inplace=True)\n",
    "\n",
    "# get column list with extra fields\n",
    "column_list = [x.lower().replace(\" \", \"_\") for x in hist.columns]\n",
    "extra_field_list = ['symbol', 'import_time']\n",
    "column_list.extend(extra_field_list)\n",
    "# print(column_list)\n",
    "\n",
    "# get records with extra fields\n",
    "hist_records_map = hist.itertuples(index=False)\n",
    "\n",
    "record_list = [tuple(row) + (symbol,) + (import_time,) for row in hist_records_map]\n",
    "\n",
    "print(record_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a7c322-c74c-41ba-83a6-75f6b9945276",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"YahooFinanceData\").getOrCreate()\n",
    "\n",
    "# Input data: List of tuples with ticker and start date\n",
    "tickers_data = [\n",
    "    ('AAPL', '2024-01-01'),\n",
    "    ('MSFT', '2024-01-01'),\n",
    "    ('GOOGL', '2024-01-01'),\n",
    "]\n",
    "\n",
    "# Create an RDD from the input data\n",
    "tickers_rdd = spark.sparkContext.parallelize(tickers_data)\n",
    "\n",
    "# General fetch function\n",
    "# def fetch_stock_data(ticker, start_date):\n",
    "#     try:\n",
    "#         # Fetch the stock data using yfinance\n",
    "#         stock_data = yf.Ticker(ticker).history(start=start_date)\n",
    "#         stock_data.reset_index(inplace=True)  # Reset index to get 'Date' as a column\n",
    "        \n",
    "#         # Convert the DataFrame into a list of tuples\n",
    "#         rows = [\n",
    "#             (\n",
    "#                 row['Date'].strftime('%Y-%m-%d') if isinstance(row['Date'], pd.Timestamp) else row['Date'],\n",
    "#                 float(row['Open']) if not np.isnan(row['Open']) else None,\n",
    "#                 float(row['High']) if not np.isnan(row['High']) else None,\n",
    "#                 float(row['Low']) if not np.isnan(row['Low']) else None,\n",
    "#                 float(row['Close']) if not np.isnan(row['Close']) else None,\n",
    "#                 float(row['Volume']) if not np.isnan(row['Volume']) else None,\n",
    "#             )\n",
    "#             for _, row in stock_data.iterrows()\n",
    "#         ]\n",
    "#         return rows\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error fetching data for {ticker}: {e}\")\n",
    "#         return []\n",
    "\n",
    "def fetch_stock_data(ticker, start_date):\n",
    "    try:\n",
    "        # Fetch stock data using yfinance\n",
    "        stock_data = yf.Ticker(ticker).history(start=start_date)\n",
    "        stock_data.reset_index(inplace=True)  # Reset index to include 'Date' as a column\n",
    "        \n",
    "        # Convert all fields from the DataFrame into a list of tuples\n",
    "        data_tuples = [tuple(row) for row in stock_data.itertuples(index=False)]\n",
    "        \n",
    "        return data_tuples\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data for {ticker}: {e}\")\n",
    "        return []\n",
    "\n",
    "# Wrapper function for map\n",
    "def wrapper(params, fetch_fn):\n",
    "    ticker, start_date = params\n",
    "    return fetch_fn(ticker, start_date)\n",
    "\n",
    "# Use map with fetch function as parameter\n",
    "results_rdd = tickers_rdd.map(lambda params: wrapper(params, fetch_stock_data))\n",
    "\n",
    "# Flatten the results RDD\n",
    "flattened_rdd = results_rdd.flatMap(lambda x: x)\n",
    "\n",
    "# Define schema for the final DataFrame\n",
    "schema = [\"date\", \"open\", \"high\", \"low\", \"close\", \"volume\"]\n",
    "\n",
    "# Create a DataFrame from the flattened RDD\n",
    "stock_df = spark.createDataFrame(flattened_rdd, schema=schema)\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "stock_df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5280dd4-9d20-47e2-ab5e-5bf9dbc1bd9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def fetch_stock_data_as_tuples(ticker, start_date):\n",
    "    try:\n",
    "        # Fetch the stock data using yfinance\n",
    "        stock_data = yf.Ticker(ticker).history(start=start_date)\n",
    "        stock_data.reset_index(inplace=True)  # Reset index to include 'Date' as a column\n",
    "\n",
    "        # Convert the DataFrame into a list of tuples\n",
    "        data_tuples = [\n",
    "            (\n",
    "                row['Date'].strftime('%Y-%m-%d') if isinstance(row['Date'], pd.Timestamp) else row['Date'],\n",
    "                float(row['Open']) if not np.isnan(row['Open']) else None,\n",
    "                float(row['High']) if not np.isnan(row['High']) else None,\n",
    "                float(row['Low']) if not np.isnan(row['Low']) else None,\n",
    "                float(row['Close']) if not np.isnan(row['Close']) else None,\n",
    "                float(row['Volume']) if not np.isnan(row['Volume']) else None,\n",
    "            )\n",
    "            for _, row in stock_data.iterrows()\n",
    "        ]\n",
    "        return data_tuples\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data for {ticker}: {e}\")\n",
    "        return []\n",
    "\n",
    "# Example usage\n",
    "ticker = \"AAPL\"\n",
    "start_date = \"2020-01-01\"\n",
    "stock_tuples = fetch_stock_data_as_tuples(ticker, start_date)\n",
    "\n",
    "# Print the first few tuples\n",
    "print(stock_tuples[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68996a6b-1320-4a9e-93d8-ed04aa9af57c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "\n",
    "def fetch_all_fields_as_tuples(ticker, start_date):\n",
    "    try:\n",
    "        # Fetch stock data using yfinance\n",
    "        stock_data = yf.Ticker(ticker).history(start=start_date)\n",
    "        stock_data.reset_index(inplace=True)  # Reset index to include 'Date' as a column\n",
    "        \n",
    "        # Convert all fields from the DataFrame into a list of tuples\n",
    "        data_tuples = [tuple(row) for row in stock_data.itertuples(index=False)]\n",
    "        \n",
    "        return data_tuples\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data for {ticker}: {e}\")\n",
    "        return []\n",
    "\n",
    "# Example usage\n",
    "ticker = \"AAPL\"\n",
    "start_date = \"2020-01-01\"\n",
    "stock_tuples = fetch_all_fields_as_tuples(ticker, start_date)\n",
    "\n",
    "# Print the first few tuples\n",
    "print(stock_tuples[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5430ed4-ed33-4be7-89c4-b38d0e6fa4be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import yfinance as yf\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"YahooFinanceData\").getOrCreate()\n",
    "\n",
    "# Input data: List of tuples with tickers and start dates\n",
    "tickers_data = [\n",
    "    ('AAPL', '2020-01-01'),\n",
    "    ('MSFT', '2019-01-01'),\n",
    "    ('GOOGL', '2021-01-01'),\n",
    "]\n",
    "\n",
    "# Create an RDD from the input data\n",
    "tickers_rdd = spark.sparkContext.parallelize(tickers_data)\n",
    "\n",
    "# Fetch function to retrieve all stock data fields\n",
    "def fetch_stock_data_all_fields(ticker, start_date):\n",
    "    try:\n",
    "        # Fetch stock data using yfinance\n",
    "        stock_data = yf.Ticker(ticker).history(start=start_date)\n",
    "        stock_data.reset_index(inplace=True)  # Reset index to include 'Date' as a column\n",
    "        \n",
    "        # Convert all rows to tuples\n",
    "        return [tuple(row) for row in stock_data.itertuples(index=False)]\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data for {ticker}: {e}\")\n",
    "        return []\n",
    "\n",
    "# Use map instead of flatMap\n",
    "results_rdd = tickers_rdd.map(lambda params: fetch_stock_data_all_fields(params[0], params[1]))\n",
    "\n",
    "# Collect and print results\n",
    "results = results_rdd.collect()\n",
    "print(results)\n",
    "# for result in results:\n",
    "#     print(result[:3])  # Print the first 3 rows for each ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc05701-dba2-46ad-9b12-2f466121554f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import yfinance as yf\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"YahooFinanceData\").getOrCreate()\n",
    "\n",
    "# Input data: List of tickers (start_date is fixed as '2024-12-01' for all)\n",
    "tickers_data = [\n",
    "    ('AAPL', '2024-12-01'),\n",
    "    ('MSFT', '2024-12-01'),\n",
    "    ('GOOGL', '2024-12-01'),\n",
    "]\n",
    "\n",
    "# Create an RDD from the input data\n",
    "tickers_rdd = spark.sparkContext.parallelize(tickers_data)\n",
    "\n",
    "# Fetch function to retrieve all stock data fields with ticker and fixed start_date\n",
    "def fetch_stock_data_with_ticker_and_date(ticker, start_date):\n",
    "    try:\n",
    "        # Fetch stock data using yfinance\n",
    "        stock_data = yf.Ticker(ticker).history(start=start_date)\n",
    "        stock_data.reset_index(inplace=True)  # Reset index to include 'Date' as a column\n",
    "        \n",
    "        # Convert all rows to tuples and append ticker and start_date\n",
    "        return [tuple(row) + (ticker, start_date) for row in stock_data.itertuples(index=False)]\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data for {ticker}: {e}\")\n",
    "        return []\n",
    "\n",
    "# Use map to process data in parallel\n",
    "results_rdd = tickers_rdd.map(lambda params: fetch_stock_data_with_ticker_and_date(params[0], params[1]))\n",
    "\n",
    "# Collect and print results\n",
    "results = results_rdd.collect()\n",
    "for ticker_data in results:\n",
    "    print(ticker_data[:3])  # Print the first 3 rows for each ticker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2563b2b2-b2b2-4637-b0ac-1e76e475d50a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import yfinance as yf\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"YahooFinanceData\").getOrCreate()\n",
    "\n",
    "# Input data: List of tickers (start_date is fixed as '2024-12-01' for all)\n",
    "tickers_data = [\n",
    "    ('AAPL', '2024-12-01'),\n",
    "    ('MSFT', '2024-12-01'),\n",
    "    ('GOOGL', '2024-12-01'),\n",
    "]\n",
    "\n",
    "# Create an RDD from the input data\n",
    "tickers_rdd = spark.sparkContext.parallelize(tickers_data)\n",
    "\n",
    "# Fetch function to retrieve all stock data fields with ticker and fixed start_date\n",
    "def fetch_stock_data_with_ticker_and_date(ticker, start_date):\n",
    "    try:\n",
    "        # Fetch stock data using yfinance\n",
    "        stock_data = yf.Ticker(ticker).history(start=start_date)\n",
    "        stock_data.reset_index(inplace=True)  # Reset index to include 'Date' as a column\n",
    "        \n",
    "        # Convert all rows to tuples and append ticker and start_date\n",
    "        return [tuple(row) + (ticker, start_date) for row in stock_data.itertuples(index=False)]\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data for {ticker}: {e}\")\n",
    "        return []\n",
    "\n",
    "# Use flatMap to process data in parallel and collect all rows as tuples\n",
    "results_rdd = tickers_rdd.flatMap(lambda params: fetch_stock_data_with_ticker_and_date(params[0], params[1]))\n",
    "\n",
    "# Collect the data as a list of tuples\n",
    "results = results_rdd.collect()\n",
    "\n",
    "# Define the schema based on the yfinance columns and additional fields\n",
    "columns = [\n",
    "    \"Date\", \"Open\", \"High\", \"Low\", \"Close\", \"Volume\", \"Dividends\", \"Stock Splits\",\n",
    "    \"Ticker\", \"Start_Date\"\n",
    "]\n",
    "\n",
    "# Create a Spark DataFrame\n",
    "df = spark.createDataFrame(results, schema=columns)\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0051d310-8180-48db-82fe-a272a45af39c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import yfinance as yf\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"YahooFinanceData\").getOrCreate()\n",
    "\n",
    "# Input data: List of tickers (start_date is fixed as '2024-12-01' for all)\n",
    "tickers_data = [\n",
    "    ('AAPL', '2024-12-01'),\n",
    "    ('MSFT', '2024-12-01'),\n",
    "    ('GOOGL', '2024-12-01'),\n",
    "]\n",
    "\n",
    "# Create an RDD from the input data\n",
    "tickers_rdd = spark.sparkContext.parallelize(tickers_data)\n",
    "\n",
    "# Fetch function to retrieve all stock data fields with ticker and fixed start_date\n",
    "def fetch_stock_data_with_ticker_and_date(ticker, start_date):\n",
    "    try:\n",
    "        # Fetch stock data using yfinance\n",
    "        stock_data = yf.Ticker(ticker).history(start=start_date)\n",
    "        stock_data.reset_index(inplace=True)  # Reset index to include 'Date' as a column\n",
    "        \n",
    "        # Convert 'Date' to string format and append ticker and start_date\n",
    "        result = []\n",
    "        for row in stock_data.itertuples(index=False):\n",
    "            row_date = row[0].strftime('%Y-%m-%d')  # Convert Date to string\n",
    "            result.append(tuple([row_date] + list(row[1:]) + [ticker, start_date]))\n",
    "        \n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data for {ticker}: {e}\")\n",
    "        return []\n",
    "\n",
    "# Use flatMap to process data in parallel and collect all rows as tuples\n",
    "results_rdd = tickers_rdd.flatMap(lambda params: fetch_stock_data_with_ticker_and_date(params[0], params[1]))\n",
    "\n",
    "# Collect the data as a list of tuples\n",
    "results = results_rdd.collect()\n",
    "\n",
    "# Define the schema based on the yfinance columns and additional fields\n",
    "columns = [\n",
    "    \"Date\", \"Open\", \"High\", \"Low\", \"Close\", \"Volume\", \"Dividends\", \"Stock Splits\",\n",
    "    \"Ticker\", \"Start_Date\"\n",
    "]\n",
    "\n",
    "# Create a Spark DataFrame\n",
    "df = spark.createDataFrame(results, schema=columns)\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2039ff18-3704-41a3-bc7a-208b2073b0c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import yfinance as yf\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"YahooFinanceData\").getOrCreate()\n",
    "\n",
    "# Input data: List of tickers (start_date is fixed as '2024-12-01' for all)\n",
    "tickers_data = [\n",
    "    ('AAPL', '2024-12-01'),\n",
    "    ('MSFT', '2024-12-01'),\n",
    "    ('GOOGL', '2024-12-01'),\n",
    "]\n",
    "\n",
    "# Create an RDD from the input data\n",
    "tickers_rdd = spark.sparkContext.parallelize(tickers_data)\n",
    "\n",
    "# Fetch function to retrieve all stock data fields with ticker and fixed start_date\n",
    "def fetch_stock_data_with_ticker_and_date(ticker, start_date):\n",
    "    try:\n",
    "        # Fetch stock data using yfinance\n",
    "        stock_data = yf.Ticker(ticker).history(start=start_date)\n",
    "        stock_data.reset_index(inplace=True)  # Reset index to include 'Date' as a column\n",
    "        \n",
    "        # Convert all rows to tuples and append ticker and start_date\n",
    "        return [tuple(row) + (ticker, start_date) for row in stock_data.itertuples(index=False)]\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data for {ticker}: {e}\")\n",
    "        return []\n",
    "\n",
    "# Use flatMap to process data in parallel and collect all rows as a single list of tuples\n",
    "results_rdd = tickers_rdd.flatMap(lambda params: fetch_stock_data_with_ticker_and_date(params[0], params[1]))\n",
    "\n",
    "# Collect the data as a list of tuples\n",
    "results = results_rdd.collect()\n",
    "print(results)\n",
    "\n",
    "# Print the first 3 tuples to check the result\n",
    "for row in results[:3]:\n",
    "    print(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1175deeb-8a83-47e7-81ab-171bbd59af7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import yfinance as yf\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"YahooFinanceData\").getOrCreate()\n",
    "\n",
    "# Input data: List of tickers (start_date is fixed as '2024-12-01' for all)\n",
    "tickers_data = [\n",
    "    ('AAPL', '2024-12-10'),\n",
    "    ('MSFT', '2024-12-10'),\n",
    "    ('GOOGL', '2024-12-10'),\n",
    "]\n",
    "\n",
    "# Create an RDD from the input data\n",
    "tickers_rdd = spark.sparkContext.parallelize(tickers_data)\n",
    "\n",
    "# Fetch function to retrieve all stock data fields with ticker and fixed start_date\n",
    "def fetch_stock_data_with_ticker_and_date(ticker, start_date):\n",
    "    try:\n",
    "        # Fetch stock data using yfinance\n",
    "        stock_data = yf.Ticker(ticker).history(start=start_date)\n",
    "        stock_data.reset_index(inplace=True)  # Reset index to include 'Date' as a column\n",
    "\n",
    "        stock_data['Date'] = stock_data['Date'].dt.strftime('%Y-%m-%d %H:%M:%S%z')\n",
    "        \n",
    "        \n",
    "        # Convert all rows to tuples and append ticker and start_date\n",
    "        return [tuple(row) + (ticker, start_date) for row in stock_data.itertuples(index=False)]\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data for {ticker}: {e}\")\n",
    "        return []\n",
    "\n",
    "# Function to process ticker data and call the fetch function with unpacked params\n",
    "def process_ticker_data(ticker_data):\n",
    "    return fetch_stock_data_with_ticker_and_date(*ticker_data)\n",
    "\n",
    "# Use flatMap to process data in parallel and collect all rows as a single list of tuples\n",
    "results_rdd = tickers_rdd.flatMap(process_ticker_data)\n",
    "\n",
    "# Collect the data as a list of tuples\n",
    "results = results_rdd.collect()\n",
    "print(results)\n",
    "# # Print the first 3 tuples to check the result\n",
    "# for row in results[:10]:\n",
    "#     print(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338e7d35-b37e-4bd6-a21e-d973310eab39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "\n",
    "def fetch_stock_data_with_ticker_and_date(ticker, start_date):\n",
    "    try:\n",
    "        # Fetch stock data using yfinance\n",
    "        stock_data = yf.Ticker(ticker).history(start=start_date)\n",
    "        stock_data.reset_index(inplace=True)  # Reset index to include 'Date' as a column\n",
    "        \n",
    "        if stock_data['Date'].dt.tz is not None:\n",
    "            stock_data['Date'] = stock_data['Date'].dt.tz_localize(None)\n",
    "        # Convert all rows to tuples, include ticker and start_date, dynamically using all fields\n",
    "        return [\n",
    "            tuple(row) + (ticker, start_date)  # Add ticker and start_date to the tuple\n",
    "            for row in stock_data.itertuples(index=False)\n",
    "        ]\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data for {ticker}: {e}\")\n",
    "        return []\n",
    "\n",
    "    \n",
    "x=fetch_stock_data_with_ticker_and_date('C', '2024-12-11')\n",
    "print(x)\n",
    "                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e29cc9c-a208-4bde-a560-d738ca3d2e72",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType, TimestampType, IntegerType\n",
    "import yfinance as yf\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"YahooFinanceData\").getOrCreate()\n",
    "\n",
    "# Define the schema for the DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"Date\", StringType(), True),\n",
    "    StructField(\"Open\", FloatType(), True),\n",
    "    StructField(\"High\", FloatType(), True),\n",
    "    StructField(\"Low\", FloatType(), True),\n",
    "    StructField(\"Close\", FloatType(), True),\n",
    "    StructField(\"Volume\", IntegerType(), True),\n",
    "    StructField(\"Dividends\", FloatType(), True),\n",
    "    StructField(\"Stock Splits\", FloatType(), True),\n",
    "    StructField(\"Ticker\", StringType(), True),\n",
    "    StructField(\"Start Date\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Input data: List of tickers and start date\n",
    "tickers_data = [\n",
    "    ('AAPL', '2024-12-01'),\n",
    "    ('MSFT', '2024-12-01'),\n",
    "    ('GOOGL', '2024-12-01')\n",
    "]\n",
    "\n",
    "# Fetch function to retrieve stock data\n",
    "def fetch_stock_data_with_ticker_and_date(ticker, start_date):\n",
    "    try:\n",
    "        # Fetch stock data using yfinance\n",
    "        stock_data = yf.Ticker(ticker).history(start=start_date)\n",
    "        stock_data.reset_index(inplace=True)  # Reset index to include 'Date' as a column\n",
    "        \n",
    "        \n",
    "        stock_data['Date'] = stock_data['Date'].dt.strftime('%Y-%m-%d %H:%M:%S%z')\n",
    "        \n",
    "        # Convert all rows to tuples, include ticker and start_date\n",
    "        return [\n",
    "            tuple(row) + (ticker, start_date)  # Add ticker and start_date to the tuple\n",
    "            for row in stock_data.itertuples(index=False)\n",
    "        ]\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data for {ticker}: {e}\")\n",
    "        return []\n",
    "\n",
    "# Create an RDD from the tickers data\n",
    "tickers_rdd = spark.sparkContext.parallelize(tickers_data)\n",
    "\n",
    "# Fetch the stock data for all tickers using map and collect the results\n",
    "results_rdd = tickers_rdd.flatMap(lambda params: fetch_stock_data_with_ticker_and_date(params[0], params[1]))\n",
    "\n",
    "# Collect the results as a list of tuples\n",
    "results = results_rdd.collect()\n",
    "\n",
    "# Create a Spark DataFrame from the results\n",
    "df = spark.createDataFrame(results, schema)\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d4c836-9cca-4a70-96c9-6929e3c4871e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql.types import StructType, StructField, StringType, FloatType, TimestampType\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType, TimestampType, IntegerType\n",
    "\n",
    "\n",
    "print(results)\n",
    "# Define the schema for the DataFrame\n",
    "new_schema = [\n",
    "    StructField(\"Date\", TimestampType(), True),        # 'Date' column as Timestamp\n",
    "    StructField(\"Open\", FloatType(), True),            # 'Open' column as Float\n",
    "    StructField(\"High\", FloatType(), True),            # 'High' column as Float\n",
    "    StructField(\"Low\", FloatType(), True),             # 'Low' column as Float\n",
    "    StructField(\"Close\", FloatType(), True),           # 'Close' column as Float\n",
    "    StructField(\"Volume\", IntegerType(), True),              # 'Volume' column as Float\n",
    "    StructField(\"Dividends\", FloatType(), True),       # 'Dividends' column as Float\n",
    "    StructField(\"Stock Splits\", FloatType(), True),    # 'Stock Splits' column as Float\n",
    "    StructField(\"Ticker\", StringType(), True),         # 'Ticker' column as String\n",
    "    StructField(\"Start Date\", StringType(), True)      # 'Start Date' column as String\n",
    "]\n",
    "\n",
    "# Create a StructType object\n",
    "schema = StructType(new_schema)\n",
    "\n",
    "df = spark.createDataFrame(results, schema)\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5411c16e-c7b6-4a7c-81fc-c5e3e404bbbf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType, TimestampType\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"YahooFinanceData\").getOrCreate()\n",
    "\n",
    "# Example result list containing tuples (with timezone info)\n",
    "results = [\n",
    "    ('2024-12-01 00:00:00-05:00', 145.22, 146.40, 144.50, 145.50, 1200000.0, 0.0, 0.0, 'AAPL', '2024-12-01'),\n",
    "    ('2024-12-02 00:00:00-05:00', 146.50, 147.60, 145.70, 146.80, 1100000.0, 0.0, 0.0, 'AAPL', '2024-12-01'),\n",
    "    ('2024-12-01 00:00:00-05:00', 101.00, 102.20, 100.50, 101.80, 1000000.0, 0.0, 0.0, 'MSFT', '2024-12-01')\n",
    "]\n",
    "\n",
    "# Convert 'Date' field (with timezone) to timestamp without timezone\n",
    "def convert_to_timestamp(date_str):\n",
    "    # Remove the timezone part and convert the string to datetime object\n",
    "    return datetime.strptime(date_str.split(' ')[0], \"%Y-%m-%d\")\n",
    "\n",
    "# Create a new list with the converted date\n",
    "converted_results = [\n",
    "    (convert_to_timestamp(row[0]), *row[1:]) for row in results\n",
    "]\n",
    "\n",
    "# Define the schema for the DataFrame\n",
    "new_schema = [\n",
    "    StructField(\"Date\", TimestampType(), True),\n",
    "    StructField(\"Open\", FloatType(), True),\n",
    "    StructField(\"High\", FloatType(), True),\n",
    "    StructField(\"Low\", FloatType(), True),\n",
    "    StructField(\"Close\", FloatType(), True),\n",
    "    StructField(\"Volume\", FloatType(), True),\n",
    "    StructField(\"Dividends\", FloatType(), True),\n",
    "    StructField(\"Stock Splits\", FloatType(), True),\n",
    "    StructField(\"Ticker\", StringType(), True),\n",
    "    StructField(\"Start Date\", StringType(), True)\n",
    "]\n",
    "\n",
    "# Create a DataFrame from the converted results and schema\n",
    "df = spark.createDataFrame(converted_results, schema=new_schema)\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d185b5c-9b63-4cf3-b641-5669b4616315",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType, TimestampType, IntegerType\n",
    "import yfinance as yf\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"YahooFinanceData\").getOrCreate()\n",
    "\n",
    "# Input data: List of tickers (start_date is fixed as '2024-12-01' for all)\n",
    "tickers_data = [\n",
    "    ('AAPL', '2024-12-01'),\n",
    "    ('MSFT', '2024-12-01'),\n",
    "    ('GOOGL', '2024-12-01'),\n",
    "]\n",
    "\n",
    "# Create an RDD from the input data\n",
    "tickers_rdd = spark.sparkContext.parallelize(tickers_data)\n",
    "\n",
    "# Fetch function to retrieve all stock data fields with ticker and fixed start_date\n",
    "def fetch_stock_data_with_ticker_and_date(ticker, start_date):\n",
    "    try:\n",
    "        # Fetch stock data using yfinance\n",
    "        stock_data = yf.Ticker(ticker).history(start=start_date)\n",
    "        stock_data.reset_index(inplace=True)  # Reset index to include 'Date' as a column\n",
    "        stock_data['Date'] = stock_data['Date'].dt.strftime('%Y-%m-%d %H:%M:%S%z')\n",
    "        \n",
    "        # Convert all rows to tuples and append ticker and start_date\n",
    "        return [tuple(row) + (ticker, start_date) for row in stock_data.itertuples(index=False)]\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data for {ticker}: {e}\")\n",
    "        return []\n",
    "\n",
    "# Function to process ticker data and call the fetch function with unpacked params\n",
    "def process_ticker_data(ticker_data):\n",
    "    return fetch_stock_data_with_ticker_and_date(*ticker_data)\n",
    "\n",
    "# Use flatMap to process data in parallel and collect all rows as a single list of tuples\n",
    "results_rdd = tickers_rdd.flatMap(process_ticker_data)\n",
    "\n",
    "# Collect the data as a list of tuples\n",
    "results = results_rdd.collect()\n",
    "\n",
    "# Define the schema for the DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"Date\", StringType(), True),        # 'Date' column as Timestamp\n",
    "    StructField(\"Open\", FloatType(), True),            # 'Open' column as Float\n",
    "    StructField(\"High\", FloatType(), True),            # 'High' column as Float\n",
    "    StructField(\"Low\", FloatType(), True),             # 'Low' column as Float\n",
    "    StructField(\"Close\", FloatType(), True),           # 'Close' column as Float\n",
    "    StructField(\"Volume\", IntegerType(), True),            # 'Volume' column as Int\n",
    "    StructField(\"Dividends\", FloatType(), True),       # 'Dividends' column as Float\n",
    "    StructField(\"Stock Splits\", FloatType(), True),    # 'Stock Splits' column as Float\n",
    "    StructField(\"Ticker\", StringType(), True),         # 'Ticker' column as String\n",
    "    StructField(\"Start Date\", StringType(), True)      # 'Start Date' column as String\n",
    "])\n",
    "\n",
    "# Create a DataFrame using the schema\n",
    "df = spark.createDataFrame(results, schema)\n",
    "\n",
    "# Show the DataFrame content\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa0ae5a-7595-436c-b8c2-80d14f6b5028",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "\n",
    "# Get data\n",
    "data = yf.download(\"AAPL\", start=\"2023-12-18\", end=\"2023-12-22\")\n",
    "\n",
    "# Convert the index to datetime and remove timezone\n",
    "data.index = pd.to_datetime(data.index).tz_localize(None)\n",
    "\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008a99a0-884c-40e2-be1a-ff47437f4dd6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "\n",
    "from pandas import Timedelta\n",
    "\n",
    "\n",
    "\n",
    "# Download data\n",
    "\n",
    "ticker = \"AAPL\" \n",
    "\n",
    "df = yf.download(ticker, start=\"2023-01-01\", end=\"2023-12-22\")\n",
    "\n",
    "\n",
    "\n",
    "# Convert date to UTC\n",
    "\n",
    "df.index = df.index.tz_localize('UTC') \n",
    "\n",
    "\n",
    "\n",
    "# Access a specific date in UTC\n",
    "\n",
    "print(df.index[0])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73f15ca-f170-4c77-8e42-5fc9b799f943",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType, TimestampType\n",
    "import yfinance as yf\n",
    "from datetime import date, datetime, timedelta\n",
    "from io import StringIO\n",
    "import yaml\n",
    "\n",
    "# import psycopg2.extras\n",
    "# import psycopg2\n",
    "# from pgcopy import CopyManager\n",
    "\n",
    "# Define the custom exception\n",
    "class MyCustomException(Exception):\n",
    "    pass\n",
    "\n",
    "# Main class for ingestion\n",
    "class RawYFIngestion:\n",
    "    def __init__(self, equity_type, destination):\n",
    "        self.equity_type = equity_type\n",
    "        self.destination = destination\n",
    "        self.import_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S.\") + str(datetime.now().microsecond)[:3]\n",
    "        \n",
    "       # Get yfinance registered column list\n",
    "        with open(\"config.yaml\",\"r\") as file_object:\n",
    "            documents=yaml.safe_load_all(file_object)\n",
    "            for doc in documents:\n",
    "                doc_name = doc['document_name']\n",
    "                if doc_name==f\"yfinance_{equity_type}\":\n",
    "                    self.registered_col_list=doc['registered_column_list']\n",
    "\n",
    "    # Function to fetch data from Yahoo Finance\n",
    "    def fetch_yfinance_record(self, multi_param_pairs):\n",
    "        try:\n",
    "            symbol, start_date = multi_param_pairs\n",
    "            # Fetch stock data using yfinance\n",
    "            quote = yf.Ticker(symbol)\n",
    "            current_date = date.today()\n",
    "            hist = quote.history(start=start_date, end=current_date)\n",
    "            \n",
    "            # Reset index to include Date as a column and format it\n",
    "            hist.reset_index(inplace=True)\n",
    "            hist['Date'] = hist['Date'].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "            # Add symbol and import_time to each row\n",
    "            record_list = [\n",
    "                tuple(row) + (symbol, self.import_time) for row in hist.itertuples(index=False)\n",
    "            ]\n",
    "            return record_list\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching data for {symbol}: {e}\")\n",
    "            return []  # Return an empty list on error\n",
    "    \n",
    "    # Function to process the records (pass through parameters)\n",
    "    def process_yfinance_record(self, single_param_pair):\n",
    "        print(f\"Processing {single_param_pair}\")\n",
    "        return self.fetch_yfinance_record(single_param_pair)\n",
    "\n",
    "    # Parallel fetch function\n",
    "    def parallel_fetch(self, multi_param_pairs):\n",
    "        # Create Spark session\n",
    "        spark = SparkSession.builder.appName(\"YahooFinanceData\").getOrCreate()\n",
    "        \n",
    "        # Create RDD from the input parameter pairs\n",
    "        record_rdd = spark.sparkContext.parallelize(multi_param_pairs)\n",
    "        \n",
    "        # Use flatMap to return a flattened list of records\n",
    "        results_rdd = record_rdd.flatMap(self.process_yfinance_record)\n",
    "        \n",
    "        # # Collect the results from the RDD\n",
    "        # results = results_rdd.collect()\n",
    "        \n",
    "        schema = ['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Dividends', 'Stock Splits', 'Ticker', 'ImportTime']\n",
    "        # rdd_rows = spark.sparkContext.parallelize(stock_data_rows)\n",
    "        df = spark.createDataFrame(results_rdd, schema)\n",
    "        df.show()\n",
    "        \n",
    "        \n",
    "        return df\n",
    "\n",
    "# List of stock symbols and start dates\n",
    "yf_param_pairs = [\n",
    "    ('AAPL', '2024-12-10'),\n",
    "    ('MSFT', '2024-12-10'),\n",
    "    ('GOOGL', '2024-12-10'),\n",
    "]\n",
    "\n",
    "# Instantiate the class\n",
    "stock_stage = RawYFIngestion('stock', 'mytable')\n",
    "\n",
    "# Fetch data in parallel\n",
    "stock_data_rows = stock_stage.parallel_fetch(yf_param_pairs)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b748785-765b-46a8-a8ed-864642778050",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType\n",
    "import os\n",
    "\n",
    "## DEFINE SENSITIVE VARIABLES\n",
    "CATALOG_URI = \"http://nessie:19120/api/v1\"  # Nessie Server URI\n",
    "WAREHOUSE = \"s3://warehouse/\"               # Minio Address to Write to\n",
    "STORAGE_URI = \"http://172.22.0.3:9000\"      # Minio IP address from docker inspect\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import yaml\n",
    "\n",
    "\n",
    "def load_config(file_path):\n",
    "    \"\"\"Load the YAML configuration file.\"\"\"\n",
    "    with open(file_path, \"r\") as file:\n",
    "        config = yaml.safe_load(file)\n",
    "    return config\n",
    "\n",
    "load_config('raw_table_schema.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cf4c2a-9ec2-44e1-a19b-5d5e755a21fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType, IntegerType\n",
    "\n",
    "# Load the YAML configuration file\n",
    "def load_config(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        return yaml.safe_load(f)\n",
    "\n",
    "# Map YAML types to PySpark types\n",
    "type_mapping = {\n",
    "    \"StringType\": StringType,\n",
    "    \"FloatType\": FloatType,\n",
    "    \"IntegerType\": IntegerType,\n",
    "}\n",
    "\n",
    "# Convert YAML schema to PySpark StructType\n",
    "def create_struct_type(schema_config):\n",
    "    fields = [\n",
    "        StructField(field[\"name\"], type_mapping[field[\"type\"]](), field[\"nullable\"])\n",
    "        for field in schema_config\n",
    "    ]\n",
    "    return StructType(fields)\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"IcebergTableCreator\") \\\n",
    "    .config(\"spark.sql.catalog.my_catalog\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.my_catalog.type\", \"hadoop\") \\\n",
    "    .config(\"spark.sql.catalog.my_catalog.warehouse\", \"path/to/warehouse\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load the YAML configuration for raw tables\n",
    "config = load_config('raw_table_schemas.yaml')\n",
    "\n",
    "# Extract the schema and partitioning for the table\n",
    "table_config = config['raw_tables']['raw.stock_eod_data']\n",
    "schema = create_struct_type(table_config['schema'])\n",
    "partition_by = table_config.get('partition_by', [])\n",
    "\n",
    "# Build the SQL query for creating the Iceberg table\n",
    "table_name = \"my_catalog.raw.stock_eod_data\"\n",
    "schema_columns = \", \".join([f\"{field.name} {field.dataType.simpleString()}\" for field in schema.fields])\n",
    "partition_columns = \", \".join([p[\"field\"] for p in partition_by]) if partition_by else \"\"\n",
    "\n",
    "# Create the Iceberg table\n",
    "create_table_query = f\"\"\"\n",
    "CREATE TABLE {table_name} ({schema_columns})\n",
    "USING iceberg\n",
    "\"\"\"\n",
    "if partition_columns:\n",
    "    create_table_query += f\" PARTITIONED BY ({partition_columns})\"\n",
    "\n",
    "print(create_table_query)\n",
    "# # Run the query\n",
    "# spark.sql(create_table_query)\n",
    "\n",
    "# print(f\"Table {table_name} created successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3c0089-d61a-44cb-884b-e896c1b791dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35912ec-4e56-4723-9e8b-09c66cbe59ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "config_file_path='table_schemas.yaml'\n",
    "zone='raw'\n",
    "table='raw.stock_eod_data'\n",
    "with open(config_file_path, 'r') as f:\n",
    "    config=yaml.safe_load(f)\n",
    "print(config[zone][table])\n",
    "\n",
    "\n",
    "\n",
    "# self.yaml_table_schema=config['schema']?\n",
    "\n",
    "#     def __init__(self, config_file_path, zone, table):\n",
    "#         self.config_file_path=config_file_path\n",
    "#         self.zone=zone\n",
    "#         self.table=table\n",
    "#         with open(self.config_file_path, 'r') as f:\n",
    "#             config=yaml.safe_load(f)\n",
    "#         self.yaml_table_schema=config['schema']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b7b751-ab25-478f-9755-3ff38df9cb5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType, IntegerType\n",
    "\n",
    "# Load the YAML configuration file\n",
    "def load_config(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        return yaml.safe_load(f)\n",
    "\n",
    "# Map YAML types to PySpark types\n",
    "type_mapping = {\n",
    "    \"StringType\": StringType,\n",
    "    \"FloatType\": FloatType,\n",
    "    \"IntegerType\": IntegerType,\n",
    "}\n",
    "\n",
    "# Convert YAML schema to PySpark StructType\n",
    "def create_struct_type(schema_config):\n",
    "    print(schema_config)\n",
    "    fields = [\n",
    "        StructField(field[\"name\"], type_mapping[field[\"type\"]](), field[\"nullable\"])\n",
    "        for field in schema_config\n",
    "    ]\n",
    "    return StructType(fields)\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CreateDataFrameFromYAML\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load the schema from YAML file\n",
    "config = load_config('table_schemas.yaml')\n",
    "\n",
    "# Extract the schema\n",
    "schema = create_struct_type(config['raw']['raw.stock_eod_data']['schema'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ed87ba-d3b6-49be-a75b-e45f002d5e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def parallel_fetch(self, multi_param_pairs):        \n",
    "      \n",
    "        # Create RDD from the input parameter pairs\n",
    "        \n",
    "        # record_rdd = self.spark.sparkContext.parallelize(multi_param_pairs)\n",
    "        \n",
    "        all_records = []  # Collect all results in a list\n",
    "        for pair in multi_param_pairs:\n",
    "            records = self.fetch_yfinance_record(pair)  # Fetch data on the driver\n",
    "            all_records.extend(records)\n",
    "        # Use flatMap to return a flattened list of records\n",
    "        # results_rdd = record_rdd.flatMap(self.process_yfinance_record)\n",
    "        results_rdd = self.spark.sparkContext.parallelize(all_records)\n",
    "        \n",
    "        # Collect the results from the RDD and convert to a list of tuples\n",
    "        # results = results_rdd.collect()        \n",
    "        df = self.spark.createDataFrame(results_rdd, self.registered_column_list)   \n",
    "        df.show()\n",
    "        return df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Poetry",
   "language": "python",
   "name": "poetry-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
