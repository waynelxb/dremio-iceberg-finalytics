{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc71f1ce-97ce-4e88-a374-747acfc540fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import nbimporter\n",
    "from datetime import datetime, date\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType,  DateType, TimestampType\n",
    "from pyspark.sql.functions import to_date, to_timestamp\n",
    "from lab_table_manager import TableManager\n",
    "import yfinance as yf\n",
    "import time\n",
    "import random\n",
    "from lab_pg_database_manager import PGDatabaseManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8e29c97-3bf7-4c3f-8608-65c8a39b910f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.postgresql#postgresql added as a dependency\n",
      "org.apache.iceberg#iceberg-spark-runtime-3.5_2.12 added as a dependency\n",
      "org.projectnessie.nessie-integrations#nessie-spark-extensions-3.5_2.12 added as a dependency\n",
      "software.amazon.awssdk#bundle added as a dependency\n",
      "software.amazon.awssdk#url-connection-client added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-62283837-6afd-4e53-9051-7102958a1c83;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.postgresql#postgresql;42.7.3 in central\n",
      "\tfound org.checkerframework#checker-qual;3.42.0 in central\n",
      "\tfound org.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.5.0 in central\n",
      "\tfound org.projectnessie.nessie-integrations#nessie-spark-extensions-3.5_2.12;0.77.1 in central\n",
      "\tfound software.amazon.awssdk#bundle;2.24.8 in central\n",
      "\tfound software.amazon.awssdk#url-connection-client;2.24.8 in central\n",
      "\tfound software.amazon.awssdk#utils;2.24.8 in central\n",
      "\tfound org.reactivestreams#reactive-streams;1.0.4 in central\n",
      "\tfound software.amazon.awssdk#annotations;2.24.8 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      "\tfound software.amazon.awssdk#http-client-spi;2.24.8 in central\n",
      "\tfound software.amazon.awssdk#metrics-spi;2.24.8 in central\n",
      ":: resolution report :: resolve 758ms :: artifacts dl 25ms\n",
      "\t:: modules in use:\n",
      "\torg.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.5.0 from central in [default]\n",
      "\torg.checkerframework#checker-qual;3.42.0 from central in [default]\n",
      "\torg.postgresql#postgresql;42.7.3 from central in [default]\n",
      "\torg.projectnessie.nessie-integrations#nessie-spark-extensions-3.5_2.12;0.77.1 from central in [default]\n",
      "\torg.reactivestreams#reactive-streams;1.0.4 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\tsoftware.amazon.awssdk#annotations;2.24.8 from central in [default]\n",
      "\tsoftware.amazon.awssdk#bundle;2.24.8 from central in [default]\n",
      "\tsoftware.amazon.awssdk#http-client-spi;2.24.8 from central in [default]\n",
      "\tsoftware.amazon.awssdk#metrics-spi;2.24.8 from central in [default]\n",
      "\tsoftware.amazon.awssdk#url-connection-client;2.24.8 from central in [default]\n",
      "\tsoftware.amazon.awssdk#utils;2.24.8 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   12  |   0   |   0   |   0   ||   12  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-62283837-6afd-4e53-9051-7102958a1c83\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 12 already retrieved (0kB/16ms)\n",
      "25/01/04 22:54:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"cfg_connections.yaml\",\"r\") as file:\n",
    "    config=yaml.safe_load(file)\n",
    "    catalog_uri = config['docker_env']['catalog_uri'] \n",
    "    warehouse = config['docker_env']['warehouse']     # Minio Address to Write to\n",
    "    storage_uri = config['docker_env']['storage_uri'] # Minio IP address from docker inspec\n",
    "\n",
    "# Configure Spark with necessary packages and Iceberg/Nessie settings\n",
    "conf = (\n",
    "    pyspark.SparkConf()\n",
    "        .setAppName('finalytics_app')\n",
    "        # Include necessary packages\n",
    "        .set('spark.jars.packages',\n",
    "             'org.postgresql:postgresql:42.7.3,'\n",
    "             'org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.0,'\n",
    "             'org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.12:0.77.1,'\n",
    "             'software.amazon.awssdk:bundle:2.24.8,'\n",
    "             'software.amazon.awssdk:url-connection-client:2.24.8')\n",
    "        # Enable Iceberg and Nessie extensions\n",
    "        .set('spark.sql.extensions', \n",
    "             'org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,'\n",
    "             'org.projectnessie.spark.extensions.NessieSparkSessionExtensions')\n",
    "        # Configure Nessie catalog\n",
    "        .set('spark.sql.catalog.nessie', 'org.apache.iceberg.spark.SparkCatalog')\n",
    "        .set('spark.sql.catalog.nessie.uri', catalog_uri)\n",
    "        .set('spark.sql.catalog.nessie.ref', 'main')\n",
    "        .set('spark.sql.catalog.nessie.authentication.type', 'NONE')\n",
    "        .set('spark.sql.catalog.nessie.catalog-impl', 'org.apache.iceberg.nessie.NessieCatalog')\n",
    "        # Set Minio as the S3 endpoint for Iceberg storage\n",
    "        .set('spark.sql.catalog.nessie.s3.endpoint', storage_uri)\n",
    "        .set('spark.sql.catalog.nessie.warehouse', warehouse)\n",
    "        .set('spark.sql.catalog.nessie.io-impl', 'org.apache.iceberg.aws.s3.S3FileIO')       \n",
    ")   \n",
    "\n",
    "# Start Spark session\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()  \n",
    "# Create the \"raw\" namespace\n",
    "spark.sql(\"CREATE NAMESPACE IF NOT EXISTS nessie.raw;\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43f1668e-6c02-40a4-8d8b-4c1e522420fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fetch_yfinance_record(symbol_date_pairs):\n",
    "    try:\n",
    "        symbol, start_date = symbol_date_pairs\n",
    "        # Fetch stock data using yfinance\n",
    "        quote = yf.Ticker(symbol)\n",
    "        current_date = date.today()\n",
    "        hist = quote.history(start=start_date, end=current_date)\n",
    "\n",
    "        # Reset index to include Date as a column and format it\n",
    "        hist.reset_index(inplace=True)\n",
    "        hist[\"Date\"] = hist[\"Date\"].dt.date\n",
    "        \n",
    "        # limit and stablize the fields of hist\n",
    "        hist = hist[['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Dividends', 'Stock Splits']]\n",
    "        \n",
    "        # Add symbol and import_time to each row\n",
    "        record_list = [\n",
    "            tuple(row) + (symbol, import_time) for row in hist.itertuples(index=False)\n",
    "        ]\n",
    "        random_sleep_time = random.uniform(0.1, 0.9)\n",
    "        time.sleep(random_sleep_time)\n",
    "\n",
    "        # print(record_list)\n",
    "        return record_list\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data for {symbol}: {e}\")\n",
    "        return []  # Return an empty list on error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa7a8628-9f57-478a-9ca5-5e500fceb7a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def parallel_fetch_yfinance_record(symbol_date_pairs, record_schema):\n",
    "    try:\n",
    "        # Distribute (symbol, start_date) pairs across Spark workers\n",
    "        record_rdd = spark.sparkContext.parallelize(symbol_date_pairs)\n",
    "\n",
    "        \n",
    "        # Fetch data in parallel\n",
    "        mapped_record_rdd = record_rdd.flatMap(fetch_yfinance_record)\n",
    "\n",
    "        # Convert RDD to DataFrame\n",
    "        result_df = spark.createDataFrame(mapped_record_rdd, schema=record_schema)\n",
    "        # result_df = spark.createDataFrame(mapped_record_rdd)\n",
    "\n",
    "        # Show or save the results\n",
    "        # result_df.show()\n",
    "        return result_df\n",
    "    except Exception as e:\n",
    "        print(f\"Error paralleling fetch: {e}\")\n",
    "        return spark.createDataFrame([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78b02da3-a584-4dbd-b44b-ccd3d5bbed1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_iceberg_raw_eod_yfinance(symbol_date_pairs, iceberg_sink_table, schema_config_file):\n",
    "    try: \n",
    "        table_manager=TableManager(schema_config_file)\n",
    "        schema_struct_type=table_manager.get_struct_type(iceberg_sink_table)   \n",
    "\n",
    "        # print(schema_struct_type)\n",
    "        create_table_script = table_manager.get_create_table_query(iceberg_sink_table)\n",
    "        spark.sql(create_table_script)\n",
    "\n",
    "        df_source=parallel_fetch_yfinance_record(symbol_date_pairs, schema_struct_type)\n",
    "        \n",
    "        # df_source.writeTo(iceberg_sink_table).append()\n",
    "        df_source.write.mode(\"overwrite\").saveAsTable(iceberg_sink_table)    \n",
    "        # df_source.writeTo(iceberg_sink_table).overwritePartitions() \n",
    "\n",
    "        print(f\"{iceberg_sink_table} has been loaded\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading lceberg raw table: {e}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2dff869-fe5d-4be5-8762-3ea44b8ce947",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pg_finalytics(iceberg_source_table, pg_url, pg_driver, pg_sink_table, pg_truncate_script, pg_merge_script):   \n",
    "    try:     \n",
    "        df_source=spark.read.table(iceberg_source_table)          \n",
    "        finalytics.execute_sql_script(pg_truncate_script)\n",
    "        \n",
    "        # Write DataFrame to PostgreSQL\n",
    "        df_source.write.jdbc(url=pg_url, table=pg_sink_table, mode=\"append\", properties={\"driver\": pg_driver})        \n",
    "        finalytics.execute_sql_script(pg_merge_script)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading pg finalytics: {e}\")        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519c3d0f-0086-4ca7-bb71-974aaedc0d9f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load nessie.raw.stock_eod_yfinance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87ce60ae-b25b-44e2-a2a3-c33c05aa5172",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\n",
      "SLF4J: Defaulting to no-operation (NOP) logger implementation\n",
      "SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n",
      "$NYCB: possibly delisted; no timezone found                         (0 + 4) / 4]\n",
      "Error fetching data for NYCB: Can only use .dt accessor with datetimelike values\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nessie.raw.stock_eod_yfinance has been loaded\n"
     ]
    }
   ],
   "source": [
    "# symbol_start_date_pairs = [\n",
    "#     ('PRMW', '2024-12-20'),\n",
    "#     ('CATC', '2024-12-20'),\n",
    "#     ('SQSP', '2024-12-20'),\n",
    "# ]\n",
    "\n",
    "# Get finalytics connetion info\n",
    "conn_config_file='cfg_connections.yaml'\n",
    "finalytics=PGDatabaseManager(conn_config_file, 'finalytics')\n",
    "pg_url=finalytics.jdbc_url\n",
    "pg_driver=finalytics.driver\n",
    "\n",
    "# Get symbol_start_date_pairs from finalytics\n",
    "query=\"select symbol, start_date from fin.vw_etl_stock_eod_start_date limit 5\"\n",
    "# query=\"select symbol, start_date from fin.vw_etl_stock_eod_start_date\"\n",
    "symbol_start_date_pairs=finalytics.get_sql_script_result_list(query)\n",
    "# print(symbol_start_date_pairs)\n",
    "\n",
    "\n",
    "# Get iceberg table config info\n",
    "table_schema_config_file='cfg_table_schemas.yaml'\n",
    "iceberg_table='nessie.raw.stock_eod_yfinance'\n",
    "\n",
    "# Set global import_time\n",
    "import_time = datetime.now().isoformat()\n",
    "\n",
    "# Load nessie.raw.stock_eod_yfinance \n",
    "load_iceberg_raw_eod_yfinance(symbol_start_date_pairs, iceberg_table, table_schema_config_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06a9dbb-80b6-49fb-8d87-d75e62c60416",
   "metadata": {},
   "source": [
    "## Load finalytics.stage.stock_eod_quote_yahoo and merge into fin.stock_eod_quote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ff6a1d9-c5fa-4697-9b7b-b257048855eb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "pg_table = \"stage.stock_eod_quote_yahoo\"  # Replace with the PostgreSQL table name\n",
    "\n",
    "pg_truncate_script = f\"truncate table {pg_table};\"\n",
    "\n",
    "pg_merge_script = \"call fin.usp_load_stock_eod();\"\n",
    "\n",
    "load_pg_finalytics(iceberg_table, pg_url, pg_driver, pg_table,  pg_truncate_script, pg_merge_script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3904a34-d834-4d28-87b6-21f2b271fbb3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|       4|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f'select count(*) from {iceberg_table}').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06376922-1939-4c55-bde2-0be33707d47a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-----+------+-----+--------+---------+------------+------+--------------------------+\n",
      "|date      |open |high |low   |close|volume  |dividends|stock_splits|symbol|import_time               |\n",
      "+----------+-----+-----+------+-----+--------+---------+------------+------+--------------------------+\n",
      "|2024-11-14|1.9  |1.725|1.725 |1.725|129682  |0.0      |0.0         |EIGR  |2025-01-04T22:55:14.743363|\n",
      "|2024-11-26|44.66|44.72|44.565|44.63|1429608 |0.0      |0.0         |VSTO  |2025-01-04T22:55:14.743363|\n",
      "|2024-11-21|29.11|29.37|28.43 |28.55|36027438|0.0      |0.0         |MRO   |2025-01-04T22:55:14.743363|\n",
      "|2024-12-10|12.86|14.08|12.42 |12.42|6127    |0.0      |0.0         |THCPU |2025-01-04T22:55:14.743363|\n",
      "+----------+-----+-----+------+-----+--------+---------+------------+------+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select * from nessie.raw.stock_eod_yfinance').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a96c0a5-9315-4e5f-a4ed-6c717efd9a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(symbol_start_date_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fef35cd0-02d5-45aa-9c49-e182de44137b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         Date       Open       High        Low      Close  \\\n",
      "0   2024-01-02 00:00:00-05:00  49.458668  51.242268  49.352616  51.136215   \n",
      "1   2024-01-03 00:00:00-05:00  51.338675  52.418473  50.355286  51.714676   \n",
      "2   2024-01-04 00:00:00-05:00  51.984625  52.784836  51.733959  51.840012   \n",
      "3   2024-01-05 00:00:00-05:00  52.090682  52.707710  51.955708  52.379917   \n",
      "4   2024-01-08 00:00:00-05:00  52.052121  52.148530  51.116936  52.071400   \n",
      "..                        ...        ...        ...        ...        ...   \n",
      "249 2024-12-27 00:00:00-05:00  70.860001  71.529999  70.540001  71.000000   \n",
      "250 2024-12-30 00:00:00-05:00  70.180000  70.830002  69.790001  70.389999   \n",
      "251 2024-12-31 00:00:00-05:00  70.550003  70.959999  70.150002  70.389999   \n",
      "252 2025-01-02 00:00:00-05:00  70.940002  71.160004  69.650002  69.940002   \n",
      "253 2025-01-03 00:00:00-05:00  70.879997  71.089996  69.849998  71.000000   \n",
      "\n",
      "       Volume  Dividends  Stock Splits  \n",
      "0    24784900        0.0           0.0  \n",
      "1    30897600        0.0           0.0  \n",
      "2    23714300        0.0           0.0  \n",
      "3    17776500        0.0           0.0  \n",
      "4    17359500        0.0           0.0  \n",
      "..        ...        ...           ...  \n",
      "249   7743100        0.0           0.0  \n",
      "250   6664400        0.0           0.0  \n",
      "251   6393000        0.0           0.0  \n",
      "252   9827400        0.0           0.0  \n",
      "253  11342900        0.0           0.0  \n",
      "\n",
      "[254 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "symbol=\"c\"\n",
    "start_date =\"2024-1-1\"\n",
    "# Fetch stock data using yfinance\n",
    "quote = yf.Ticker(symbol)\n",
    "current_date = date.today()\n",
    "hist = quote.history(start=start_date, end=current_date)\n",
    "\n",
    "# Reset index to include Date as a column and format it\n",
    "hist.reset_index(inplace=True)\n",
    "print(hist)\n",
    "# hist[\"Date\"] = hist[\"Date\"].dt.date\n",
    "# print(hist)\n",
    "\n",
    "# # limit and stablize the fields of hist\n",
    "# hist = hist[['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Dividends', 'Stock Splits']]\n",
    "\n",
    "# # Add symbol and import_time to each row\n",
    "# record_list = [\n",
    "#     tuple(row) + (symbol, import_time) for row in hist.itertuples(index=False)\n",
    "# ]\n",
    "\n",
    "# # print(record_list)\n",
    "# print(record_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed854b11-963d-4b43-a6a6-beadd8f6bec7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "164553c4-1a10-44e9-a4a7-8f9fb57d230c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "from datetime import date\n",
    "import time\n",
    "import random\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, DateType, DoubleType, StringType\n",
    "\n",
    "# Define the schema for the DataFrame\n",
    "record_schema = StructType([\n",
    "    StructField(\"Date\", DateType(), True),\n",
    "    StructField(\"Open\", DoubleType(), True),\n",
    "    StructField(\"High\", DoubleType(), True),\n",
    "    StructField(\"Low\", DoubleType(), True),\n",
    "    StructField(\"Close\", DoubleType(), True),\n",
    "    StructField(\"Volume\", DoubleType(), True),\n",
    "    StructField(\"Dividends\", DoubleType(), True),\n",
    "    StructField(\"Stock Splits\", DoubleType(), True),\n",
    "    StructField(\"Symbol\", StringType(), True),\n",
    "    StructField(\"ImportTime\", StringType(), True)\n",
    "])\n",
    "\n",
    "def xfetch_yfinance_record(symbol_date_pair):\n",
    "    try:\n",
    "        symbol, start_date = symbol_date_pair\n",
    "        # Fetch stock data using yfinance\n",
    "        quote = yf.Ticker(symbol)\n",
    "        current_date = date.today()\n",
    "        hist = quote.history(start=start_date, end=current_date)\n",
    "\n",
    "        # Reset index to include Date as a column and format it\n",
    "        hist.reset_index(inplace=True)\n",
    "        hist[\"Date\"] = hist[\"Date\"].dt.date\n",
    "        \n",
    "        # Limit and stabilize the fields of hist\n",
    "        hist = hist[['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Dividends', 'Stock Splits']]\n",
    "        print(hist)\n",
    "        \n",
    "        # Add symbol and import_time to each row\n",
    "        import_time = str(date.today())\n",
    "        record_list = [\n",
    "            tuple(row) + (symbol, import_time) for row in hist.itertuples(index=False)\n",
    "        ]\n",
    "        \n",
    "        random_sleep_time = random.uniform(0.1, 0.9)\n",
    "        time.sleep(random_sleep_time)\n",
    "     \n",
    "        return record_list\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data for {symbol}: {e}\")\n",
    "        return []  # Return an empty list on error\n",
    "\n",
    "def xparallel_fetch_yfinance_record(symbol_date_pairs, record_schema):\n",
    "    try:\n",
    "        # Initialize Spark session\n",
    "\n",
    "        connection_config_file=\"cfg_connections.yaml\"\n",
    "        spark_app_name=\"YFinanceDataFetcher\"\n",
    "        spark=create_spark_session(connection_config_file, spark_app_name)\n",
    "        \n",
    "        # spark = SparkSession.builder.appName(\"YFinanceDataFetcher\").getOrCreate()\n",
    "        \n",
    "        # Distribute (symbol, start_date) pairs across Spark workers\n",
    "        record_rdd = spark.sparkContext.parallelize(symbol_date_pairs)\n",
    "        \n",
    "        # Fetch data in parallel using mapPartitions to avoid broadcasting Spark session\n",
    "        mapped_record_rdd = record_rdd.mapPartitions(lambda partition: [record for pair in partition for record in xfetch_yfinance_record(pair)])\n",
    "        \n",
    "        # Convert RDD to DataFrame on the driver node\n",
    "        result_df = spark.createDataFrame(mapped_record_rdd, schema=record_schema)  \n",
    "\n",
    "        return result_df\n",
    "    except Exception as e:\n",
    "        print(f\"Error paralleling fetch: {e}\")\n",
    "        return spark.createDataFrame([], schema=record_schema)\n",
    "    finally:\n",
    "        spark.stop()\n",
    "\n",
    "# Example usage\n",
    "symbol_date_pairs = [(\"AAPL\", \"2023-01-01\"), (\"GOOGL\", \"2023-01-01\")]\n",
    "result_df = xparallel_fetch_yfinance_record(symbol_date_pairs, record_schema)\n",
    "# result_df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Poetry",
   "language": "python",
   "name": "poetry-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
