{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc71f1ce-97ce-4e88-a374-747acfc540fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import nbimporter\n",
    "from datetime import datetime, date\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType,  DateType, TimestampType\n",
    "from lab_table_manager import TableManager\n",
    "import yfinance as yf\n",
    "from lab_finalytics_database import FinalyticsDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e29c97-3bf7-4c3f-8608-65c8a39b910f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"cfg_connections.yaml\",\"r\") as file:\n",
    "    config=yaml.safe_load(file)\n",
    "    catalog_uri = config['docker_env']['catalog_uri'] \n",
    "    warehouse = config['docker_env']['warehouse']     # Minio Address to Write to\n",
    "    storage_uri = config['docker_env']['storage_uri'] # Minio IP address from docker inspec\n",
    "\n",
    "# Configure Spark with necessary packages and Iceberg/Nessie settings\n",
    "conf = (\n",
    "    pyspark.SparkConf()\n",
    "        .setAppName('finalytics_app')\n",
    "        # Include necessary packages\n",
    "        .set('spark.jars.packages',\n",
    "             'org.postgresql:postgresql:42.7.3,'\n",
    "             'org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.0,'\n",
    "             'org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.12:0.77.1,'\n",
    "             'software.amazon.awssdk:bundle:2.24.8,'\n",
    "             'software.amazon.awssdk:url-connection-client:2.24.8')\n",
    "        # Enable Iceberg and Nessie extensions\n",
    "        .set('spark.sql.extensions', \n",
    "             'org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,'\n",
    "             'org.projectnessie.spark.extensions.NessieSparkSessionExtensions')\n",
    "        # Configure Nessie catalog\n",
    "        .set('spark.sql.catalog.nessie', 'org.apache.iceberg.spark.SparkCatalog')\n",
    "        .set('spark.sql.catalog.nessie.uri', catalog_uri)\n",
    "        .set('spark.sql.catalog.nessie.ref', 'main')\n",
    "        .set('spark.sql.catalog.nessie.authentication.type', 'NONE')\n",
    "        .set('spark.sql.catalog.nessie.catalog-impl', 'org.apache.iceberg.nessie.NessieCatalog')\n",
    "        # Set Minio as the S3 endpoint for Iceberg storage\n",
    "        .set('spark.sql.catalog.nessie.s3.endpoint', storage_uri)\n",
    "        .set('spark.sql.catalog.nessie.warehouse', warehouse)\n",
    "        .set('spark.sql.catalog.nessie.io-impl', 'org.apache.iceberg.aws.s3.S3FileIO')       \n",
    ")   \n",
    "\n",
    "# Start Spark session\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()  \n",
    "# Create the \"sales\" namespace\n",
    "spark.sql(\"CREATE NAMESPACE IF NOT EXISTS nessie.raw;\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f1668e-6c02-40a4-8d8b-4c1e522420fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fetch_yfinance_record(symbol_date_pairs):\n",
    "    try:\n",
    "        symbol, start_date = symbol_date_pairs\n",
    "        # Fetch stock data using yfinance\n",
    "        quote = yf.Ticker(symbol)\n",
    "        current_date = date.today()\n",
    "        hist = quote.history(start=start_date, end=current_date)\n",
    "\n",
    "        # Reset index to include Date as a column and format it\n",
    "        hist.reset_index(inplace=True)\n",
    "        # hist['Date'] = hist['Date'].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        hist['Date'] = hist['Date'].dt.strftime('%Y-%m-%d').date()\n",
    "        \n",
    "        # Add symbol and import_time to each row\n",
    "        record_list = [\n",
    "            tuple(row) + (symbol, import_time) for row in hist.itertuples(index=False)\n",
    "        ]\n",
    "        \n",
    "\n",
    "        return record_list\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data for {symbol}: {e}\")\n",
    "        return []  # Return an empty list on error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7a8628-9f57-478a-9ca5-5e500fceb7a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def parallel_fetch_yfinance_record(symbol_date_pairs, record_schema):    \n",
    "    # Distribute (symbol, start_date) pairs across Spark workers\n",
    "    record_rdd = spark.sparkContext.parallelize(symbol_date_pairs)\n",
    "    \n",
    "    # Fetch data in parallel\n",
    "    mapped_record_rdd = record_rdd.flatMap(fetch_yfinance_record)\n",
    "\n",
    "    # Convert RDD to DataFrame\n",
    "    result_df = spark.createDataFrame(mapped_record_rdd, schema=record_schema)\n",
    "\n",
    "    # Show or save the results\n",
    "    # result_df.show()\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b02da3-a584-4dbd-b44b-ccd3d5bbed1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_raw_eod_yfinance(symbol_date_pairs, sink_table, schema_config_file):\n",
    "    table_manager=TableManager(schema_config_file)\n",
    "    regd_struct_type=table_manager.get_struct_type(sink_table)   \n",
    "    # regd_column_list = table_manager.get_column_list(sink_table)\n",
    "    create_table_script = table_manager.get_create_table_query(sink_table)\n",
    "    \n",
    "    df_raw_eod_yfinance=parallel_fetch_yfinance_record(symbol_date_pairs, regd_struct_type)\n",
    "    spark.sql(create_table_script)\n",
    "    # df_raw_eod_yfinance.writeTo(sink_table).append()\n",
    "    df_raw_eod_yfinance.write.mode(\"overwrite\").saveAsTable(sink_table)    \n",
    "    # df_raw_eod_yfinance.writeTo(sink_table).overwritePartitions()\n",
    "    print(f\"{sink_table} has been loaded\")\n",
    "\n",
    "         \n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e4e507-3cca-4314-a3c6-c5c6206fe818",
   "metadata": {},
   "outputs": [],
   "source": [
    "          \n",
    "# symbol_start_date_pairs = [\n",
    "#     ('AAPL', '2024-12-10'),\n",
    "#     ('MSFT', '2024-12-10'),\n",
    "#     ('GOOGL', '2024-12-10'),\n",
    "# ]\n",
    "\n",
    "\n",
    "conn_config_file='cfg_connections.yaml'\n",
    "finalytics=FinalyticsDB(conn_config_file)\n",
    "query=\"select symbol, start_date from fin.vw_etl_stock_eod_start_date limit 3\"\n",
    "symbol_start_date_pairs=finalytics.get_symbol_start_date_pairs(query)\n",
    "finalytics_url=finalytics.jdbc_url\n",
    "finalytics_driver=finalytics.driver\n",
    "\n",
    "regd_schema_config_file='cfg_registered_table_schemas.yaml'\n",
    "sink_table='nessie.raw.stock_eod_yfinance'\n",
    "\n",
    "# import_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S.\") + str(datetime.now().microsecond)[:3]\n",
    "import_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S.\")\n",
    "load_raw_eod_yfinance(symbol_start_date_pairs, sink_table, regd_schema_config_file)\n",
    "          \n",
    "    \n",
    "df=spark.read.table(sink_table)\n",
    "          \n",
    "# table_name = \"stage.stock_eod_quote_yahoo_new\"  # Replace with the PostgreSQL table name\n",
    "# # # Write Delta table DataFrame to PostgreSQL\n",
    "# df.write.jdbc(url=finalytics_url, table=table_name, mode=\"append\", properties={\"driver\": finalytics_driver})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff6a1d9-c5fa-4697-9b7b-b257048855eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "spark.sql('select count(*) from nessie.raw.stock_eod_yfinance limit 4').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06376922-1939-4c55-bde2-0be33707d47a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.sql('select * from nessie.raw.stock_eod_yfinance limit 4 order by import_time desc' ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e022f151-9324-4786-83c9-55a283c5ff19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df=spark.read.table(sink_table)\n",
    "# df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee0792d-f66c-4294-a8bd-d408408844bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "table_name = \"stage.stock_eod_quote_yahoo_new\"  # Replace with the PostgreSQL table name\n",
    "# # Write Delta table DataFrame to PostgreSQL\n",
    "df.write.jdbc(url=finalytics_url, table=table_name, mode=\"overwrite\", properties={\"driver\": finalytics_driver})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa57e00f-96ea-4db2-8a36-6d17d01d02ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Poetry",
   "language": "python",
   "name": "poetry-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
