{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "bc71f1ce-97ce-4e88-a374-747acfc540fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import nbimporter\n",
    "from datetime import datetime, date\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType,  DateType, TimestampType\n",
    "from pyspark.sql.functions import to_date, to_timestamp\n",
    "from lab_table_manager import TableManager\n",
    "import yfinance as yf\n",
    "from lab_pg_database_manager import PGDatabaseManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c8e29c97-3bf7-4c3f-8608-65c8a39b910f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# with open(\"cfg_connections.yaml\",\"r\") as file:\n",
    "#     config=yaml.safe_load(file)\n",
    "#     catalog_uri = config['docker_env']['catalog_uri'] \n",
    "#     warehouse = config['docker_env']['warehouse']     # Minio Address to Write to\n",
    "#     storage_uri = config['docker_env']['storage_uri'] # Minio IP address from docker inspec\n",
    "\n",
    "# # Configure Spark with necessary packages and Iceberg/Nessie settings\n",
    "# conf = (\n",
    "#     pyspark.SparkConf()\n",
    "#         .setAppName('finalytics_app')\n",
    "#         # Include necessary packages\n",
    "#         .set('spark.jars.packages',\n",
    "#              'org.postgresql:postgresql:42.7.3,'\n",
    "#              'org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.0,'\n",
    "#              'org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.12:0.77.1,'\n",
    "#              'software.amazon.awssdk:bundle:2.24.8,'\n",
    "#              'software.amazon.awssdk:url-connection-client:2.24.8')\n",
    "#         # Enable Iceberg and Nessie extensions\n",
    "#         .set('spark.sql.extensions', \n",
    "#              'org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,'\n",
    "#              'org.projectnessie.spark.extensions.NessieSparkSessionExtensions')\n",
    "#         # Configure Nessie catalog\n",
    "#         .set('spark.sql.catalog.nessie', 'org.apache.iceberg.spark.SparkCatalog')\n",
    "#         .set('spark.sql.catalog.nessie.uri', catalog_uri)\n",
    "#         .set('spark.sql.catalog.nessie.ref', 'main')\n",
    "#         .set('spark.sql.catalog.nessie.authentication.type', 'NONE')\n",
    "#         .set('spark.sql.catalog.nessie.catalog-impl', 'org.apache.iceberg.nessie.NessieCatalog')\n",
    "#         # Set Minio as the S3 endpoint for Iceberg storage\n",
    "#         .set('spark.sql.catalog.nessie.s3.endpoint', storage_uri)\n",
    "#         .set('spark.sql.catalog.nessie.warehouse', warehouse)\n",
    "#         .set('spark.sql.catalog.nessie.io-impl', 'org.apache.iceberg.aws.s3.S3FileIO')       \n",
    "# )   \n",
    "\n",
    "# # Start Spark session\n",
    "# spark = SparkSession.builder.config(conf=conf).getOrCreate()  \n",
    "# # Create the \"sales\" namespace\n",
    "# spark.sql(\"CREATE NAMESPACE IF NOT EXISTS nessie.raw;\").show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with open(\"cfg_connections.yaml\",\"r\") as file:\n",
    "    config=yaml.safe_load(file)\n",
    "    catalog_uri = config['docker_env']['catalog_uri'] \n",
    "    warehouse = config['docker_env']['warehouse']     # Minio Address to Write to\n",
    "    storage_uri = config['docker_env']['storage_uri'] # Minio IP address from docker inspec\n",
    "\n",
    "# Configure Spark with necessary packages and Iceberg/Nessie settings\n",
    "conf = (\n",
    "    pyspark.SparkConf()\n",
    "        .setAppName('finalytics_app')\n",
    "        # Include necessary packages\n",
    "        .set('spark.jars.packages',\n",
    "             'org.postgresql:postgresql:42.7.3,'\n",
    "             'org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.0,'\n",
    "             'org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.12:0.77.1,'\n",
    "             'software.amazon.awssdk:bundle:2.24.8,'\n",
    "             'software.amazon.awssdk:url-connection-client:2.24.8')\n",
    "        # Enable Iceberg and Nessie extensions\n",
    "        .set('spark.sql.extensions', \n",
    "             'org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,'\n",
    "             'org.projectnessie.spark.extensions.NessieSparkSessionExtensions')\n",
    "        # Configure Nessie catalog\n",
    "        .set('spark.sql.catalog.nessie', 'org.apache.iceberg.spark.SparkCatalog')\n",
    "        .set('spark.sql.catalog.nessie.uri', catalog_uri)\n",
    "        .set('spark.sql.catalog.nessie.ref', 'main')\n",
    "        .set('spark.sql.catalog.nessie.authentication.type', 'NONE')\n",
    "        .set('spark.sql.catalog.nessie.catalog-impl', 'org.apache.iceberg.nessie.NessieCatalog')\n",
    "        # Set Minio as the S3 endpoint for Iceberg storage\n",
    "        .set('spark.sql.catalog.nessie.s3.endpoint', storage_uri)\n",
    "        .set('spark.sql.catalog.nessie.warehouse', warehouse)\n",
    "        .set('spark.sql.catalog.nessie.io-impl', 'org.apache.iceberg.aws.s3.S3FileIO')       \n",
    ")   \n",
    "\n",
    "# Start Spark session\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()  \n",
    "# Create the \"sales\" namespace\n",
    "spark.sql(\"CREATE NAMESPACE IF NOT EXISTS nessie.raw;\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "43f1668e-6c02-40a4-8d8b-4c1e522420fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fetch_yfinance_record(symbol_date_pairs):\n",
    "    try:\n",
    "        symbol, start_date = symbol_date_pairs\n",
    "        # Fetch stock data using yfinance\n",
    "        quote = yf.Ticker(symbol)\n",
    "        current_date = date.today()\n",
    "        hist = quote.history(start=start_date, end=current_date)\n",
    "\n",
    "        # Reset index to include Date as a column and format it\n",
    "        hist.reset_index(inplace=True)\n",
    "        # hist['Date'] = hist['Date'].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        # hist['Date'] = hist['Date'].dt.strftime('%Y-%m-%d').date()\n",
    "        hist[\"Date\"] = hist[\"Date\"].dt.date\n",
    "        \n",
    "        # limit and stablize the fields of hist\n",
    "        hist = hist[['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Dividends', 'Stock Splits']]\n",
    "        print(hist)\n",
    "        \n",
    "        # Add symbol and import_time to each row\n",
    "        record_list = [\n",
    "            tuple(row) + (symbol, import_time) for row in hist.itertuples(index=False)\n",
    "        ]\n",
    "\n",
    "        print(record_list)\n",
    "        return record_list\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data for {symbol}: {e}\")\n",
    "        return []  # Return an empty list on error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "fa7a8628-9f57-478a-9ca5-5e500fceb7a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def parallel_fetch_yfinance_record(symbol_date_pairs, record_schema):\n",
    "    try:\n",
    "        # Distribute (symbol, start_date) pairs across Spark workers\n",
    "        record_rdd = spark.sparkContext.parallelize(symbol_date_pairs)\n",
    "\n",
    "        \n",
    "        # Fetch data in parallel\n",
    "        mapped_record_rdd = record_rdd.flatMap(fetch_yfinance_record)\n",
    "\n",
    "        # Convert RDD to DataFrame\n",
    "        result_df = spark.createDataFrame(mapped_record_rdd, schema=record_schema)\n",
    "        # result_df = spark.createDataFrame(mapped_record_rdd)\n",
    "\n",
    "        # Show or save the results\n",
    "        # result_df.show()\n",
    "        return result_df\n",
    "    except Exception as e:\n",
    "        first_element = record_rdd.first()  # Get the first element to inspect its structure\n",
    "        # num_fields = len(first_element) \n",
    "        print(first_element)\n",
    "        # print(f\"Number of fields in the RDD elements: {num_fields}\")\n",
    "        print(f\"Error paralleling fetch: {e}\")\n",
    "        # return spark.createDataFrame([], schema=record_schema)\n",
    "        return spark.createDataFrame([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "78b02da3-a584-4dbd-b44b-ccd3d5bbed1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_iceberg_raw_eod_yfinance(symbol_date_pairs, iceberg_sink_table, schema_config_file):\n",
    "    try: \n",
    "        table_manager=TableManager(schema_config_file)\n",
    "        schema_struct_type=table_manager.get_struct_type(iceberg_sink_table)   \n",
    "        # regd_column_list = table_manager.get_column_list(iceberg_sink_table)\n",
    "        # print(regd_column_list)\n",
    "        # print(schema_struct_type)\n",
    "        create_table_script = table_manager.get_create_table_query(iceberg_sink_table)\n",
    "        spark.sql(create_table_script)\n",
    "\n",
    "        df_source=parallel_fetch_yfinance_record(symbol_date_pairs, schema_struct_type)\n",
    "        \n",
    "        # df_source.writeTo(iceberg_sink_table).append()\n",
    "        df_source.write.mode(\"overwrite\").saveAsTable(iceberg_sink_table)    \n",
    "        # df_source.writeTo(iceberg_sink_table).overwritePartitions() \n",
    "\n",
    "        print(f\"{iceberg_sink_table} has been loaded\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading lceberg raw table: {e}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a2dff869-fe5d-4be5-8762-3ea44b8ce947",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pg_finalytics(iceberg_source_table, pg_sink_table, jdbc_url, pg_driver):   \n",
    "    try: \n",
    "    \n",
    "        df_source=spark.read.table(iceberg_source_table)          \n",
    "        pg_sink_table = \"stage.stock_eod_quote_yahoo\"  # Replace with the PostgreSQL table name\n",
    "\n",
    "        truncate_script = f\"truncate table {pg_sink_table};\"\n",
    "        finalytics.execute_sql_script(truncate_script)\n",
    "\n",
    "        # # Write Delta table DataFrame to PostgreSQL\n",
    "        df_source.write.jdbc(url=jdbc_url, table=pg_sink_table, mode=\"append\", properties={\"driver\": pg_driver})\n",
    "\n",
    "        pg_merge_script = \"call fin.usp_load_stock_eod();\"\n",
    "        finalytics.execute_sql_script(pg_merge_script)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading pg finalytics: {e}\")        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519c3d0f-0086-4ca7-bb71-974aaedc0d9f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load nessie.raw.stock_eod_yfinance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "87ce60ae-b25b-44e2-a2a3-c33c05aa5172",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$CATC: possibly delisted; no price data found  (1d 2024-12-20 -> 2024-12-25)/ 4]\n",
      "Error fetching data for CATC: Can only use .dt accessor with datetimelike values\n",
      "         Date        Open        High  ...    Volume  Dividends  Stock Splits 4]\n",
      "0  2024-12-20  185.779999  192.889999  ...  63462900        0.0           0.0\n",
      "1  2024-12-23  192.619995  195.100006  ...  25675000        0.0           0.0\n",
      "2  2024-12-24  194.839996  196.110001  ...  10403300        0.0           0.0\n",
      "\n",
      "[3 rows x 8 columns]\n",
      "         Date       Open       High  ...  Volume  Dividends  Stock Splits1) / 4]\n",
      "0  2024-12-20  39.959999  40.450001  ...    9311        0.0           0.0\n",
      "1  2024-12-23  40.395000  40.395000  ...       0        0.0           0.0\n",
      "\n",
      "[2 rows x 8 columns]\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nessie.raw.stock_eod_yfinance has been loaded\n"
     ]
    }
   ],
   "source": [
    "symbol_start_date_pairs = [\n",
    "    ('NDP', '2024-12-20'),\n",
    "    ('CATC', '2024-12-20'),\n",
    "    ('GOOGL', '2024-12-20'),\n",
    "]\n",
    "\n",
    "# Get finalytics connetion info\n",
    "conn_config_file='cfg_connections.yaml'\n",
    "finalytics=PGDatabaseManager(conn_config_file, 'finalytics')\n",
    "jdbc_url=finalytics.jdbc_url\n",
    "pg_driver=finalytics.driver\n",
    "\n",
    "# Get symbol_start_date_pairs from finalytics\n",
    "query=\"select symbol, start_date from fin.vw_etl_stock_eod_start_date limit 1000\"\n",
    "# symbol_start_date_pairs=finalytics.get_sql_script_result_list(query)\n",
    "\n",
    "\n",
    "# Get iceberg table config info\n",
    "table_schema_config_file='cfg_table_schemas.yaml'\n",
    "iceberg_table='nessie.raw.stock_eod_yfinance'\n",
    "\n",
    "# Set global import_time\n",
    "import_time = datetime.now().isoformat()\n",
    "\n",
    "# Load nessie.raw.stock_eod_yfinance \n",
    "load_iceberg_raw_eod_yfinance(symbol_start_date_pairs, iceberg_table, table_schema_config_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06a9dbb-80b6-49fb-8d87-d75e62c60416",
   "metadata": {},
   "source": [
    "## Load finalytics.stage.stock_eod_quote_yahoo and merge into fin.stock_eod_quote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7ff6a1d9-c5fa-4697-9b7b-b257048855eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pg_table = \"stage.stock_eod_quote_yahoo\"  # Replace with the PostgreSQL table name\n",
    "load_pg_finalytics(iceberg_table, pg_table, jdbc_url, pg_driver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c3904a34-d834-4d28-87b6-21f2b271fbb3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|       5|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select count(*) from nessie.raw.stock_eod_yfinance').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "06376922-1939-4c55-bde2-0be33707d47a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+------+------+------+--------+---------+------------+------+--------------------------+\n",
      "|date      |open  |high  |low   |close |volume  |dividends|stock_splits|symbol|import_time               |\n",
      "+----------+------+------+------+------+--------+---------+------------+------+--------------------------+\n",
      "|2024-12-20|39.96 |40.45 |39.94 |40.395|9311    |0.0      |0.0         |NDP   |2024-12-25T17:00:44.194050|\n",
      "|2024-12-23|40.395|40.395|40.395|40.395|0       |0.0      |0.0         |NDP   |2024-12-25T17:00:44.194050|\n",
      "|2024-12-20|185.78|192.89|185.22|191.41|63462900|0.0      |0.0         |GOOGL |2024-12-25T17:00:44.194050|\n",
      "|2024-12-23|192.62|195.1 |190.15|194.63|25675000|0.0      |0.0         |GOOGL |2024-12-25T17:00:44.194050|\n",
      "|2024-12-24|194.84|196.11|193.78|196.11|10403300|0.0      |0.0         |GOOGL |2024-12-25T17:00:44.194050|\n",
      "+----------+------+------+------+------+--------+---------+------------+------+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select * from nessie.raw.stock_eod_yfinance').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a96c0a5-9315-4e5f-a4ed-6c717efd9a0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Poetry",
   "language": "python",
   "name": "poetry-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
