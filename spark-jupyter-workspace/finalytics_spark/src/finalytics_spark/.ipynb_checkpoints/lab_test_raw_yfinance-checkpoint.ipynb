{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43f1668e-6c02-40a4-8d8b-4c1e522420fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import nbimporter\n",
    "from datetime import datetime, date\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType,  DateType, TimestampType\n",
    "from pyspark.sql.functions import to_date, to_timestamp\n",
    "from lab_table_manager import TableManager\n",
    "import yfinance as yf\n",
    "import time\n",
    "import random\n",
    "from lab_pg_database_manager import PGDatabaseManager\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def fetch_yfinance_record(symbol_date_pairs):\n",
    "    try:\n",
    "        symbol, start_date = symbol_date_pairs\n",
    "        # Fetch stock data using yfinance\n",
    "        quote = yf.Ticker(symbol)\n",
    "        current_date = date.today()\n",
    "        hist = quote.history(start=start_date, end=current_date)\n",
    "\n",
    "        # Reset index to include Date as a column and format it\n",
    "        hist.reset_index(inplace=True)\n",
    "        hist[\"Date\"] = hist[\"Date\"].dt.date\n",
    "        \n",
    "        # limit and stablize the fields of hist\n",
    "        hist = hist[['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Dividends', 'Stock Splits']]\n",
    "        import_time = datetime.now().isoformat()\n",
    "        # Add symbol and import_time to each row\n",
    "        record_list = [\n",
    "            tuple(row) + (symbol, import_time) for row in hist.itertuples(index=False)\n",
    "        ]\n",
    "        random_sleep_time = random.uniform(0.1, 0.9)\n",
    "        time.sleep(random_sleep_time)\n",
    "\n",
    "        # print(record_list)\n",
    "        return record_list\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data for {symbol}: {e}\")\n",
    "        return []  # Return an empty list on error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d738cd69-ec1e-4273-a825-268cce1a14c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "from datetime import date\n",
    "import time\n",
    "import random\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, DateType, DoubleType, StringType\n",
    "\n",
    "# # Define the schema for the DataFrame\n",
    "record_schema = StructType([\n",
    "    StructField(\"Date\", DateType(), True),\n",
    "    StructField(\"Open\", DoubleType(), True),\n",
    "    StructField(\"High\", DoubleType(), True),\n",
    "    StructField(\"Low\", DoubleType(), True),\n",
    "    StructField(\"Close\", DoubleType(), True),\n",
    "    StructField(\"Volume\", DoubleType(), True),\n",
    "    StructField(\"Dividends\", DoubleType(), True),\n",
    "    StructField(\"Stock Splits\", DoubleType(), True),\n",
    "    StructField(\"Symbol\", StringType(), True),\n",
    "    StructField(\"ImportTime\", StringType(), True)\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "def xparallel_fetch_yfinance_record(spark, symbol_date_pairs, record_schema):\n",
    "    try:\n",
    "        # Initialize Spark session\n",
    "\n",
    "        connection_config_file=\"cfg_connections.yaml\"\n",
    "        spark_app_name=\"YFinanceDataFetcher\"\n",
    "        # spark=create_spark_session(spark_app_name)\n",
    "        \n",
    "        # spark = SparkSession.builder.appName(\"YFinanceDataFetcher\").getOrCreate()\n",
    "        \n",
    "        # Distribute (symbol, start_date) pairs across Spark workers\n",
    "        record_rdd = spark.sparkContext.parallelize(symbol_date_pairs)\n",
    "        \n",
    "        # Fetch data in parallel using mapPartitions to avoid broadcasting Spark session\n",
    "        mapped_record_rdd = record_rdd.mapPartitions(lambda partition: [record for pair in partition for record in fetch_yfinance_record(pair)])\n",
    "        \n",
    "        # Convert RDD to DataFrame on the driver node\n",
    "        result_df = spark.createDataFrame(mapped_record_rdd)  \n",
    "\n",
    "        return result_df\n",
    "    except Exception as e:\n",
    "        print(f\"Error paralleling fetch: {e}\")\n",
    "        return spark.createDataFrame([])\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb81d18d-f8df-4805-aaf2-f5a5b48d9d52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Poetry",
   "language": "python",
   "name": "poetry-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
