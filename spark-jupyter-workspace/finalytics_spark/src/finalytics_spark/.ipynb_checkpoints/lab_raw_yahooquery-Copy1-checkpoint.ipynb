{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "639ab335-35ed-4d00-9ce9-5d708b21672e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import nbimporter\n",
    "from datetime import datetime, date\n",
    "import yfinance as yf\n",
    "import time\n",
    "import random\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType,  DateType, TimestampType\n",
    "from pyspark.sql.functions import to_date, to_timestamp\n",
    "from lab_database_manager import PgDBManager\n",
    "from lab_spark import create_spark_session\n",
    "from lab_schema_manager import SchemaManager\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47f50bb6-b8ab-401c-b268-5567c62e71b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group Date: 2024-11-15, Group Number: 1, Symbols: ['RRAC', 'CNSL', 'MITA', 'GLT', 'IBTX', 'MNTX', 'VEV', 'NUZE', 'AGR', 'ACST', 'JWSM', 'NYCB', 'PHYT', 'OCUP', 'FANH', 'NAPA', 'TTP', 'CTLT', 'THCP', 'THCPU', 'VSTO', 'MRO', 'ARC', 'GMDA', 'VIEW', 'STER', 'NSTG', 'SRCL', 'EIGR']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get finalytics connetion info\n",
    "conn_config_file='cfg_connections.yaml'\n",
    "finalytics=PgDBManager(conn_config_file, 'finalytics')\n",
    "pg_url=finalytics.jdbc_url\n",
    "pg_driver=finalytics.driver\n",
    "\n",
    "# Get symbol_start_date_pairs from finalytics\n",
    "query=\"SELECT symbol,group_start_date, group_number from fin.vw_etl_stock_eod_start_date_grouped  WHERE group_number <3;\"\n",
    "query_result=finalytics.get_sql_script_result_list(query)\n",
    "# print(query_result)\n",
    "# symbol_list = [row[0] for row in query_result]\n",
    "\n",
    "# print(symbol_list)\n",
    "\n",
    "# Initialize a defaultdict to store the symbols for each (group_date, group_number)\n",
    "grouped_symbols = defaultdict(list)\n",
    "\n",
    "# Iterate over the data to group symbols by (group_date, group_number)\n",
    "for symbol, group_date, group_number in query_result:\n",
    "    # Use a tuple of (group_date, group_number) as the key and append the symbol to the list\n",
    "    grouped_symbols[(group_date, group_number)].append(symbol)\n",
    "\n",
    "# Display the results\n",
    "for group, symbols in grouped_symbols.items():\n",
    "    group_date, group_number = group\n",
    "    print(f\"Group Date: {group_date}, Group Number: {group_number}, Symbols: {symbols}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e8df45b-6894-410e-aaef-e1a873b52c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group Date: 2024-11-15, Group Number: 1, Symbols: ['RRAC', 'CNSL']\n",
      "Group Date: 2024-11-15, Group Number: 2, Symbols: ['MITA', 'GLT']\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Given data\n",
    "data = [\n",
    "    ('RRAC', '2024-11-15', 1),\n",
    "    ('CNSL', '2024-11-15', 1),\n",
    "    ('MITA', '2024-11-15', 2),\n",
    "    ('GLT', '2024-11-15', 2)\n",
    "]\n",
    "\n",
    "# Initialize a defaultdict to store the symbols for each (group_date, group_number)\n",
    "grouped_symbols = defaultdict(list)\n",
    "\n",
    "# Iterate over the data to group symbols by (group_date, group_number)\n",
    "for symbol, group_date, group_number in data:\n",
    "    # Use a tuple of (group_date, group_number) as the key and append the symbol to the list\n",
    "    grouped_symbols[(group_date, group_number)].append(symbol)\n",
    "\n",
    "# Display the results\n",
    "for group, symbols in grouped_symbols.items():\n",
    "    group_date, group_number = group\n",
    "    print(f\"Group Date: {group_date}, Group Number: {group_number}, Symbols: {symbols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf035ce-6f19-4726-b76a-e585b021f72e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33899a61-94b9-4bc9-9822-bb9b64b647fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         date symbol        open        high         low       close  \\\n",
      "0  2025-01-06   MSFT  428.000000  434.320007  425.480011  427.850006   \n",
      "1  2025-01-07   MSFT  429.000000  430.649994  420.799988  422.369995   \n",
      "2  2025-01-06  GOOGL  193.979996  198.220001  193.850006  196.869995   \n",
      "3  2025-01-07  GOOGL  197.110001  201.000000  194.600006  195.490005   \n",
      "\n",
      "     volume                import_time  \n",
      "0  20573600 2025-01-09 04:19:23.331907  \n",
      "1  18139100 2025-01-09 04:19:23.331907  \n",
      "2  29563600 2025-01-09 04:19:23.331907  \n",
      "3  26487200 2025-01-09 04:19:23.331907  \n",
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/finalytics_spark/.venv/lib/python3.10/site-packages/yahooquery/utils/__init__.py:1470: FutureWarning: 'S' is deprecated and will be removed in a future version. Please use 's' instead of 'S'.\n",
      "  has_live_indice = index_utc[-1] >= last_trade - pd.Timedelta(2, \"S\")\n",
      "/workspace/finalytics_spark/.venv/lib/python3.10/site-packages/yahooquery/utils/__init__.py:1470: FutureWarning: 'S' is deprecated and will be removed in a future version. Please use 's' instead of 'S'.\n",
      "  has_live_indice = index_utc[-1] >= last_trade - pd.Timedelta(2, \"S\")\n"
     ]
    }
   ],
   "source": [
    "from yahooquery import Ticker\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# List of tickers\n",
    "values_list = [\"FSNB\", \"MSFT\", \"GOOGL\"]\n",
    "tickers=values_list\n",
    "# Initialize Ticker object\n",
    "data = Ticker(tickers)\n",
    "\n",
    "# Fetch historical data\n",
    "hist_data = data.history(start=\"2025-01-06\", end=\"2025-01-08\", interval=\"1d\")\n",
    "\n",
    "# Reshape the data to include a 'Ticker' column\n",
    "hist_data.reset_index(inplace=True)  # Reset index to make Ticker and Date columns\n",
    "\n",
    "# # Select only required columns\n",
    "columns_to_select = [\"date\", \"symbol\", \"open\", \"high\", \"low\", \"close\", \"volume\"]\n",
    "hist_data = hist_data[columns_to_select]\n",
    "\n",
    "# Add a new field for the current datetime\n",
    "hist_data[\"import_time\"] = datetime.now()\n",
    "\n",
    "# Display the result\n",
    "print(hist_data)\n",
    "print(type(hist_data))\n",
    "# distinct_tickers = hist_data[\"symbol\"].unique()\n",
    "\n",
    "# # Display the result\n",
    "# print(\"Distinct Tickers:\", distinct_tickers)\n",
    "# distinct_ticker_count = hist_data[\"symbol\"].nunique()\n",
    "\n",
    "# print(distinct_ticker_count)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Convert pandas DataFrame to Spark DataFrame\n",
    "# spark_df = spark.createDataFrame(hist_data)\n",
    "\n",
    "# # Show the Spark DataFrame\n",
    "# spark_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0bdd9644-d892-4c7e-b0ba-25479c6ea284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+------------------+------------------+------------------+------------------+------+--------------------+\n",
      "|      date|symbol|              open|              high|               low|             close|volume|         import_time|\n",
      "+----------+------+------------------+------------------+------------------+------------------+------+--------------------+\n",
      "|2025-01-06|  WINV|  12.0600004196167|  12.0600004196167|  12.0600004196167|  12.0600004196167|     0|2025-01-09 01:23:...|\n",
      "|2025-01-07|  WINV|  12.0600004196167|  12.0600004196167|  12.0600004196167|  12.0600004196167|     0|2025-01-09 01:23:...|\n",
      "|2025-01-06|   RGA|218.49000549316406|221.61000061035156| 217.6199951171875| 218.6699981689453|623900|2025-01-09 01:23:...|\n",
      "|2025-01-07|   RGA|220.02999877929688|223.27999877929688| 218.2100067138672|221.74000549316406|646400|2025-01-09 01:23:...|\n",
      "|2025-01-06|  POLA| 3.180000066757202|3.6700000762939453| 3.119999885559082|3.2699999809265137| 58800|2025-01-09 01:23:...|\n",
      "|2025-01-07|  POLA|3.2799999713897705|3.5899999141693115| 3.200000047683716| 3.319999933242798|114400|2025-01-09 01:23:...|\n",
      "|2025-01-06|  SNEX|102.43000030517578|104.73999786376953| 102.1500015258789| 103.2699966430664|213000|2025-01-09 01:23:...|\n",
      "|2025-01-07|  SNEX|103.30999755859375|103.30999755859375|  99.6500015258789|100.05999755859375|235200|2025-01-09 01:23:...|\n",
      "|2025-01-06|  MKFG|               3.5|3.5869998931884766|3.4600000381469727| 3.490000009536743| 63900|2025-01-09 01:23:...|\n",
      "|2025-01-07|  MKFG|               3.5|3.5450000762939453|3.4100000858306885|3.4100000858306885| 48600|2025-01-09 01:23:...|\n",
      "|2025-01-06|  VYGR| 6.099999904632568| 6.150000095367432|  5.96999979019165| 6.019999980926514|481800|2025-01-09 01:23:...|\n",
      "|2025-01-07|  VYGR| 6.010000228881836| 6.269999980926514| 5.889999866485596| 5.920000076293945|248700|2025-01-09 01:23:...|\n",
      "|2025-01-06|   RMR|  20.3700008392334|20.440000534057617|19.899999618530273| 19.90999984741211|153800|2025-01-09 01:23:...|\n",
      "|2025-01-07|   RMR|19.899999618530273|20.059999465942383|  19.3799991607666|19.389999389648438|162700|2025-01-09 01:23:...|\n",
      "|2025-01-06|  LIVE| 9.229999542236328| 9.229999542236328| 8.760000228881836| 9.140000343322754|  3100|2025-01-09 01:23:...|\n",
      "|2025-01-07|  LIVE|               9.0| 9.390000343322754|              8.75| 9.180000305175781| 11300|2025-01-09 01:23:...|\n",
      "|2025-01-06|  AMPL|11.760000228881836|11.829999923706055|11.470000267028809|11.510000228881836|405100|2025-01-09 01:23:...|\n",
      "|2025-01-07|  AMPL|11.510000228881836|11.630000114440918|10.479999542236328|10.600000381469727|398800|2025-01-09 01:23:...|\n",
      "|2025-01-06|  CYBN|10.279999732971191|10.649999618530273|            10.125|10.649999618530273|206200|2025-01-09 01:23:...|\n",
      "|2025-01-07|  CYBN|10.699999809265137|10.729999542236328| 9.859999656677246|10.029999732971191|252200|2025-01-09 01:23:...|\n",
      "+----------+------+------------------+------------------+------------------+------------------+------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import yaml\n",
    "import nbimporter\n",
    "from datetime import datetime, date\n",
    "import yfinance as yf\n",
    "import time\n",
    "import random\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType,  DateType, TimestampType\n",
    "from pyspark.sql.functions import to_date, to_timestamp\n",
    "from lab_database_manager import PgDBManager\n",
    "from lab_spark import create_spark_session\n",
    "from lab_schema_manager import SchemaManager\n",
    "\n",
    "# Create Spark Session\n",
    "connection_config_file=\"cfg_connections.yaml\"\n",
    "spark_app_name=\"raw_yfinance\"\n",
    "spark=create_spark_session(connection_config_file, spark_app_name)\n",
    "# spark.sql(\"CREATE NAMESPACE IF NOT EXISTS nessie.raw;\")\n",
    "# Initialize Spark session\n",
    "# Set log level to WARN (you can also use \"ERROR\" to suppress even more)\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "# Convert pandas DataFrame to Spark DataFrame\n",
    "spark_df = spark.createDataFrame(hist_data)\n",
    "\n",
    "# Show the Spark DataFrame\n",
    "spark_df.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "eb3aca44-9d58-4686-8bc3-3f873dc70397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: date (nullable = true)\n",
      " |-- symbol: string (nullable = true)\n",
      " |-- open: double (nullable = true)\n",
      " |-- high: double (nullable = true)\n",
      " |-- low: double (nullable = true)\n",
      " |-- close: double (nullable = true)\n",
      " |-- volume: long (nullable = true)\n",
      " |-- import_time: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b0e9457c-7eaf-4b52-be1f-115f9dcad106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct Tickers: ['INKT' 'INNV' 'INTE' 'INTEU' 'INVZ' 'IOBT' 'IONM' 'IONQ' 'IPSC' 'IREN'\n",
      " 'IRRX' 'INSI' 'IVT' 'IXAQ' 'IXAQU' 'JANX' 'JCE' 'JFR' 'JGH' 'JLS' 'JMM'\n",
      " 'JOBY' 'JPC' 'JPI' 'JQC' 'JRI' 'JRS' 'JSPR' 'JWEL' 'JXN' 'JZXN' 'KACLU'\n",
      " 'KARO' 'KAVL' 'KD' 'KIND' 'KIO' 'FBRX' 'KORE' 'KPLT' 'KPRX' 'KYMR' 'KRT'\n",
      " 'FHTX' 'SWBI' 'TMQ' 'DOLE' 'RLYB' 'SGML']\n",
      "49\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22168854-440f-47aa-9be7-f0c13adef044",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Poetry",
   "language": "python",
   "name": "poetry-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
