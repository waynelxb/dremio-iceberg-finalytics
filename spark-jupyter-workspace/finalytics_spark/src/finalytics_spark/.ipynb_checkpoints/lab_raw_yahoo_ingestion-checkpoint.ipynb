{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "639ab335-35ed-4d00-9ce9-5d708b21672e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import nbimporter\n",
    "from datetime import datetime, date\n",
    "import time\n",
    "import random\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType,  DateType, TimestampType\n",
    "from pyspark.sql.functions import to_date, to_timestamp\n",
    "from lab_database_manager import PgDBManager\n",
    "from lab_spark import create_spark_session\n",
    "from lab_schema_manager import SchemaManager\n",
    "from lab_raw_yahooquery import get_raw_yahooquery, get_raw_yfinance\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d1248e-f920-4fe2-85b6-110c03fd84d5",
   "metadata": {},
   "source": [
    "# Create Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "320989cb-efd1-46ef-89b3-745acc98183f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Spark Session\n",
    "connection_config_file=\"cfg_connections.yaml\"\n",
    "spark_app_name=\"raw_yfinance\"\n",
    "spark=create_spark_session(connection_config_file, spark_app_name)\n",
    "\n",
    "# Set logging level to ERROR to reduce verbosity\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "# Optional: You can also adjust Python logging for third-party libraries\n",
    "logging.getLogger(\"py4j\").setLevel(logging.ERROR)\n",
    "spark.sql(\"CREATE NAMESPACE IF NOT EXISTS nessie.raw;\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53196d95-5457-4ce4-a0a4-af774cad7b7b",
   "metadata": {},
   "source": [
    "# Function: Load Iceberg Table Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "9cae414a-d7b5-41d3-af8f-861d61f07964",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_into_iceberg_table(schema_config_file, spark_source_df, iceberg_sink_table):\n",
    "    try: \n",
    "        schema_manager=SchemaManager(schema_config_file)\n",
    "        schema_struct_type=schema_manager.get_struct_type(\"tables\", iceberg_sink_table)  \n",
    "        \n",
    "        create_table_script = schema_manager.get_create_table_query(\"tables\", iceberg_sink_table)\n",
    "        spark.sql(create_table_script)\n",
    "     \n",
    "        spark_source_df.writeTo(iceberg_sink_table).append()\n",
    "        # source_spark_df.write.mode(\"overwrite\").saveAsTable(iceberg_sink_table) \n",
    "\n",
    "        incremental_count=spark_source_df.count()\n",
    "        total_count=spark.table(iceberg_sink_table).count()\n",
    "\n",
    "        print(f\"{iceberg_sink_table} was loaded with {incremental_count} records, totally {total_count} records.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading lceberg raw table: {e}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28104eba-f0d5-4cc6-a5a5-5a8615cf2359",
   "metadata": {},
   "source": [
    "# Function: Insert Data into PG Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "22168854-440f-47aa-9be7-f0c13adef044",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_iceberg_data_into_pg(conn_config_file, iceberg_source_table, pg_database, pg_sink_table, is_pg_truncate_enabled, is_pg_merge_enabled):   \n",
    "    try:    \n",
    "        df_source=spark.read.table(iceberg_source_table)          \n",
    "\n",
    "        pg_db_mgr=PgDBManager(conn_config_file, pg_database)\n",
    "        pg_url=pg_db_mgr.jdbc_url\n",
    "        pg_driver=pg_db_mgr.driver\n",
    "\n",
    "        if is_pg_truncate_enabled == True:\n",
    "            pg_truncate_script=f\"TRUNCATE TABLE {pg_sink_table}\"\n",
    "            pg_db_mgr.execute_sql_script(pg_truncate_script)\n",
    "        \n",
    "        # Write DataFrame to PostgreSQL\n",
    "        df_source.write.jdbc(url=pg_url, table=pg_sink_table, mode=\"append\", properties={\"driver\": pg_driver}) \n",
    "\n",
    "        if is_pg_merge_enabled == True:\n",
    "            pg_merge_script = \"call fin.usp_load_stock_eod();\"\n",
    "            pg_db_mgr.execute_sql_script(pg_merge_script)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading pg finalytics: {e}\")       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc5b821-f483-4e5d-9766-becd93dd3dfb",
   "metadata": {},
   "source": [
    "# Truncate Iceberg Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "f190b4f3-4dfd-491d-8a45-ec17875a98cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iceberg table nessie.raw.stock_eod_yahooquery truncated successfully.\n"
     ]
    }
   ],
   "source": [
    "# Get iceberg table config info\n",
    "schema_config_file='cfg_schemas.yaml'\n",
    "iceberg_raw_stock_eod_table='nessie.raw.stock_eod_yahooquery'\n",
    "\n",
    "# Check if the Iceberg table exists and truncate it if it does\n",
    "if spark.catalog.tableExists(iceberg_raw_stock_eod_table):\n",
    "    spark.sql(f\"TRUNCATE TABLE {iceberg_raw_stock_eod_table}\")\n",
    "    print(f\"Iceberg table {iceberg_raw_stock_eod_table} truncated successfully.\")\n",
    "else:\n",
    "    print(f\"Iceberg table {iceberg_raw_stock_eod_table} does not exist.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1f1bda-a63e-4410-92b8-b3a8be099844",
   "metadata": {},
   "source": [
    "# Get Symbol Group Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "123067ab-f994-4de4-9f63-6cce3afca0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get finalytics connetion info\n",
    "conn_config_file='cfg_connections.yaml'\n",
    "pg_db=\"finalytics\"\n",
    "pg_db_mgr=PgDBManager(conn_config_file, pg_db)\n",
    "# pg_url=pg_db_mgr.jdbc_url\n",
    "# pg_driver=pg_db_mgr.driver\n",
    "import_time = datetime.now()\n",
    "\n",
    "\n",
    "# Get symbol_start_date_pairs from finalytics\n",
    "query=\"SELECT group_number, group_start_date, symbol from fin.vw_etl_stock_eod_start_date_grouped  WHERE group_number <3 Limit 50;\"\n",
    "query_result=pg_db_mgr.get_sql_script_result_list(query)\n",
    "\n",
    "# Initialize a defaultdict to store the symbols for each (group_date, group_number)\n",
    "grouped_symbols = defaultdict(list)\n",
    "\n",
    "# Iterate over the data to group symbols by (group_date, group_number)\n",
    "for group_number, group_start_date, symbol in query_result:\n",
    "    # Use a tuple of (group_date, group_number) as the key and append the symbol to the list\n",
    "    grouped_symbols[(group_number, group_start_date)].append(symbol)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd33642-894e-486b-81a3-5461477d837c",
   "metadata": {},
   "source": [
    "# Loop Groups and Insert Data into Iceberg Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "47f50bb6-b8ab-401c-b268-5567c62e71b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: Cannot mix tz-aware with tz-naive values\n"
     ]
    },
    {
     "ename": "PySparkValueError",
     "evalue": "[CANNOT_INFER_EMPTY_SCHEMA] Can not infer schema from empty dataset.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPySparkValueError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[99], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# print(f\"Group Date: {group_start_date}, Group Number: {group_number}, Symbols: {group_symbols}\")\u001b[39;00m\n\u001b[1;32m      7\u001b[0m hist_data\u001b[38;5;241m=\u001b[39mget_raw_yahooquery(group_symbols, group_start_date, import_time)\n\u001b[0;32m----> 8\u001b[0m hist_df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhist_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m insert_into_iceberg_table(schema_config_file, hist_df, iceberg_raw_stock_eod_table)\n\u001b[1;32m     10\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m5\u001b[39m)\n",
      "File \u001b[0;32m/workspace/finalytics_spark/.venv/lib/python3.10/site-packages/pyspark/sql/session.py:1440\u001b[0m, in \u001b[0;36mSparkSession.createDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m   1436\u001b[0m     data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(data, columns\u001b[38;5;241m=\u001b[39mcolumn_names)\n\u001b[1;32m   1438\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_pandas \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, pd\u001b[38;5;241m.\u001b[39mDataFrame):\n\u001b[1;32m   1439\u001b[0m     \u001b[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001b[39;00m\n\u001b[0;32m-> 1440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mSparkSession\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[1;32m   1441\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverifySchema\u001b[49m\n\u001b[1;32m   1442\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1443\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_dataframe(\n\u001b[1;32m   1444\u001b[0m     data, schema, samplingRatio, verifySchema  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m   1445\u001b[0m )\n",
      "File \u001b[0;32m/workspace/finalytics_spark/.venv/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:363\u001b[0m, in \u001b[0;36mSparkConversionMixin.createDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    361\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m    362\u001b[0m converted_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_from_pandas(data, schema, timezone)\n\u001b[0;32m--> 363\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconverted_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverifySchema\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/finalytics_spark/.venv/lib/python3.10/site-packages/pyspark/sql/session.py:1485\u001b[0m, in \u001b[0;36mSparkSession._create_dataframe\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m   1483\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_createFromRDD(data\u001b[38;5;241m.\u001b[39mmap(prepare), schema, samplingRatio)\n\u001b[1;32m   1484\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1485\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_createFromLocal\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprepare\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1486\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1487\u001b[0m jrdd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mSerDeUtil\u001b[38;5;241m.\u001b[39mtoJavaArray(rdd\u001b[38;5;241m.\u001b[39m_to_java_object_rdd())\n",
      "File \u001b[0;32m/workspace/finalytics_spark/.venv/lib/python3.10/site-packages/pyspark/sql/session.py:1093\u001b[0m, in \u001b[0;36mSparkSession._createFromLocal\u001b[0;34m(self, data, schema)\u001b[0m\n\u001b[1;32m   1090\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(data)\n\u001b[1;32m   1092\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(schema, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m-> 1093\u001b[0m     struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inferSchemaFromList\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1094\u001b[0m     converter \u001b[38;5;241m=\u001b[39m _create_converter(struct)\n\u001b[1;32m   1095\u001b[0m     tupled_data: Iterable[Tuple] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(converter, data)\n",
      "File \u001b[0;32m/workspace/finalytics_spark/.venv/lib/python3.10/site-packages/pyspark/sql/session.py:948\u001b[0m, in \u001b[0;36mSparkSession._inferSchemaFromList\u001b[0;34m(self, data, names)\u001b[0m\n\u001b[1;32m    933\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    934\u001b[0m \u001b[38;5;124;03mInfer schema from list of Row, dict, or tuple.\u001b[39;00m\n\u001b[1;32m    935\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    945\u001b[0m \u001b[38;5;124;03m:class:`pyspark.sql.types.StructType`\u001b[39;00m\n\u001b[1;32m    946\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    947\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[0;32m--> 948\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkValueError(\n\u001b[1;32m    949\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCANNOT_INFER_EMPTY_SCHEMA\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    950\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{},\n\u001b[1;32m    951\u001b[0m     )\n\u001b[1;32m    952\u001b[0m infer_dict_as_struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jconf\u001b[38;5;241m.\u001b[39minferDictAsStruct()\n\u001b[1;32m    953\u001b[0m infer_array_from_first_element \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jconf\u001b[38;5;241m.\u001b[39mlegacyInferArrayTypeFromFirstElement()\n",
      "\u001b[0;31mPySparkValueError\u001b[0m: [CANNOT_INFER_EMPTY_SCHEMA] Can not infer schema from empty dataset."
     ]
    }
   ],
   "source": [
    "# warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"yahooquery\")\n",
    "\n",
    "# Display the results\n",
    "for group, group_symbols in grouped_symbols.items():\n",
    "    group_number, group_start_date = group\n",
    "    # print(f\"Group Date: {group_start_date}, Group Number: {group_number}, Symbols: {group_symbols}\")\n",
    "    hist_data=get_raw_yahooquery(group_symbols, group_start_date, import_time)\n",
    "    hist_df = spark.createDataFrame(hist_data)\n",
    "    insert_into_iceberg_table(schema_config_file, hist_df, iceberg_raw_stock_eod_table)\n",
    "    time.sleep(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1227020-d4a1-48d5-a5f4-0575f61ba26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hist_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a558a80-e6c2-497c-875e-69cc8a6921a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pg_table='stage.stock_eod_quote_yahoo'\n",
    "is_pg_truncate_enabled=True\n",
    "is_pg_merge_enabled=True\n",
    "insert_iceberg_data_into_pg(conn_config_file, iceberg_raw_stock_eod_table, pg_db, pg_table, is_pg_truncate_enabled, is_pg_merge_enabled)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5edf5c-d96d-4299-8e32-5556cfb93c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('select * from nessie.raw.stock_eod_yahooquery').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70235ec4-8a46-45be-ba7f-3bad304cdc3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Poetry",
   "language": "python",
   "name": "poetry-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
