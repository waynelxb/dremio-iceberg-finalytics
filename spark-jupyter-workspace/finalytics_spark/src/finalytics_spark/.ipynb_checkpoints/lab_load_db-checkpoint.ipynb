{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc71f1ce-97ce-4e88-a374-747acfc540fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import nbimporter\n",
    "from datetime import datetime, date\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType\n",
    "from raw_yfinance_ingestion import RawYFIngestion  \n",
    "from lab_table_manager import TableManager\n",
    "import yfinance as yf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8e29c97-3bf7-4c3f-8608-65c8a39b910f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.postgresql#postgresql added as a dependency\n",
      "org.apache.iceberg#iceberg-spark-runtime-3.5_2.12 added as a dependency\n",
      "org.projectnessie.nessie-integrations#nessie-spark-extensions-3.5_2.12 added as a dependency\n",
      "software.amazon.awssdk#bundle added as a dependency\n",
      "software.amazon.awssdk#url-connection-client added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-3008ca48-945a-4663-bc74-f24cdd46f6df;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.postgresql#postgresql;42.7.3 in central\n",
      "\tfound org.checkerframework#checker-qual;3.42.0 in central\n",
      "\tfound org.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.5.0 in central\n",
      "\tfound org.projectnessie.nessie-integrations#nessie-spark-extensions-3.5_2.12;0.77.1 in central\n",
      "\tfound software.amazon.awssdk#bundle;2.24.8 in central\n",
      "\tfound software.amazon.awssdk#url-connection-client;2.24.8 in central\n",
      "\tfound software.amazon.awssdk#utils;2.24.8 in central\n",
      "\tfound org.reactivestreams#reactive-streams;1.0.4 in central\n",
      "\tfound software.amazon.awssdk#annotations;2.24.8 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      "\tfound software.amazon.awssdk#http-client-spi;2.24.8 in central\n",
      "\tfound software.amazon.awssdk#metrics-spi;2.24.8 in central\n",
      ":: resolution report :: resolve 394ms :: artifacts dl 12ms\n",
      "\t:: modules in use:\n",
      "\torg.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.5.0 from central in [default]\n",
      "\torg.checkerframework#checker-qual;3.42.0 from central in [default]\n",
      "\torg.postgresql#postgresql;42.7.3 from central in [default]\n",
      "\torg.projectnessie.nessie-integrations#nessie-spark-extensions-3.5_2.12;0.77.1 from central in [default]\n",
      "\torg.reactivestreams#reactive-streams;1.0.4 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\tsoftware.amazon.awssdk#annotations;2.24.8 from central in [default]\n",
      "\tsoftware.amazon.awssdk#bundle;2.24.8 from central in [default]\n",
      "\tsoftware.amazon.awssdk#http-client-spi;2.24.8 from central in [default]\n",
      "\tsoftware.amazon.awssdk#metrics-spi;2.24.8 from central in [default]\n",
      "\tsoftware.amazon.awssdk#url-connection-client;2.24.8 from central in [default]\n",
      "\tsoftware.amazon.awssdk#utils;2.24.8 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   12  |   0   |   0   |   0   ||   12  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-3008ca48-945a-4663-bc74-f24cdd46f6df\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 12 already retrieved (0kB/8ms)\n",
      "24/12/24 15:38:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/12/24 15:38:34 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"config_connections.yaml\",\"r\") as file:\n",
    "    config=yaml.safe_load(file)\n",
    "    catalog_uri = config['docker_env']['catalog_uri'] \n",
    "    warehouse = config['docker_env']['warehouse']     # Minio Address to Write to\n",
    "    storage_uri = config['docker_env']['storage_uri'] # Minio IP address from docker inspec\n",
    "\n",
    "# Configure Spark with necessary packages and Iceberg/Nessie settings\n",
    "conf = (\n",
    "    pyspark.SparkConf()\n",
    "        .setAppName('finalytics_app')\n",
    "        # Include necessary packages\n",
    "        .set('spark.jars.packages',\n",
    "             'org.postgresql:postgresql:42.7.3,'\n",
    "             'org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.0,'\n",
    "             'org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.12:0.77.1,'\n",
    "             'software.amazon.awssdk:bundle:2.24.8,'\n",
    "             'software.amazon.awssdk:url-connection-client:2.24.8')\n",
    "        # Enable Iceberg and Nessie extensions\n",
    "        .set('spark.sql.extensions', \n",
    "             'org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,'\n",
    "             'org.projectnessie.spark.extensions.NessieSparkSessionExtensions')\n",
    "        # Configure Nessie catalog\n",
    "        .set('spark.sql.catalog.nessie', 'org.apache.iceberg.spark.SparkCatalog')\n",
    "        .set('spark.sql.catalog.nessie.uri', catalog_uri)\n",
    "        .set('spark.sql.catalog.nessie.ref', 'main')\n",
    "        .set('spark.sql.catalog.nessie.authentication.type', 'NONE')\n",
    "        .set('spark.sql.catalog.nessie.catalog-impl', 'org.apache.iceberg.nessie.NessieCatalog')\n",
    "        # Set Minio as the S3 endpoint for Iceberg storage\n",
    "        .set('spark.sql.catalog.nessie.s3.endpoint', storage_uri)\n",
    "        .set('spark.sql.catalog.nessie.warehouse', warehouse)\n",
    "        .set('spark.sql.catalog.nessie.io-impl', 'org.apache.iceberg.aws.s3.S3FileIO')       \n",
    ")   \n",
    "\n",
    "# Start Spark session\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()  \n",
    "# Create the \"sales\" namespace\n",
    "spark.sql(\"CREATE NAMESPACE IF NOT EXISTS nessie.raw;\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43f1668e-6c02-40a4-8d8b-4c1e522420fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fetch_yfinance_record(multi_param_pairs):\n",
    "    try:\n",
    "        symbol, start_date = multi_param_pairs\n",
    "        # Fetch stock data using yfinance\n",
    "        quote = yf.Ticker(symbol)\n",
    "        current_date = date.today()\n",
    "        hist = quote.history(start=start_date, end=current_date)\n",
    "\n",
    "        # Reset index to include Date as a column and format it\n",
    "        hist.reset_index(inplace=True)\n",
    "        hist['Date'] = hist['Date'].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "        # Add symbol and import_time to each row\n",
    "        record_list = [\n",
    "            tuple(row) + (symbol, import_time) for row in hist.itertuples(index=False)\n",
    "        ]\n",
    "\n",
    "        return record_list\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data for {symbol}: {e}\")\n",
    "        return []  # Return an empty list on error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7a8628-9f57-478a-9ca5-5e500fceb7a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afab4b13-8598-414f-8e80-14ec3828ed3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to process the records (pass through parameters)\n",
    "def process_yfinance_record(single_param_pair):\n",
    "    # print(f\"Processing {single_param_pair}\")\n",
    "    return fetch_yfinance_record(single_param_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc24f51b-ca74-4232-ae7c-eda5bafa83ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Parallel fetch function\n",
    "def parallel_fetch(multi_param_pairs, column_list):\n",
    "    # Create RDD from the input parameter pairs\n",
    "    record_rdd = spark.sparkContext.parallelize(multi_param_pairs)\n",
    "\n",
    "    # Use flatMap to return a flattened list of records\n",
    "    results_rdd = record_rdd.flatMap(process_yfinance_record)\n",
    "\n",
    "    # Collect the results from the RDD and convert to a list of tuples\n",
    "    # results = results_rdd.collect()        \n",
    "    df = spark.createDataFrame(results_rdd, column_list)   \n",
    "    \n",
    "    return df\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "78b02da3-a584-4dbd-b44b-ccd3d5bbed1a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------+------+------+------+---------+---------+------------+------+--------------------+\n",
      "|               date|  open|  high|   low| close|   volume|dividends|stock_splits|symbol|         import_time|\n",
      "+-------------------+------+------+------+------+---------+---------+------------+------+--------------------+\n",
      "|2024-12-10 00:00:00|246.89|248.21|245.34|247.77| 36914800|      0.0|         0.0|  AAPL|2024-12-24 15:23:...|\n",
      "|2024-12-11 00:00:00|247.96| 250.8|246.26|246.49| 45205800|      0.0|         0.0|  AAPL|2024-12-24 15:23:...|\n",
      "|2024-12-12 00:00:00|246.89|248.74|245.68|247.96| 32777500|      0.0|         0.0|  AAPL|2024-12-24 15:23:...|\n",
      "|2024-12-13 00:00:00|247.82|249.29|246.24|248.13| 33155300|      0.0|         0.0|  AAPL|2024-12-24 15:23:...|\n",
      "|2024-12-16 00:00:00|247.99|251.38|247.65|251.04| 51694800|      0.0|         0.0|  AAPL|2024-12-24 15:23:...|\n",
      "|2024-12-17 00:00:00|250.08|253.83|249.78|253.48| 51356400|      0.0|         0.0|  AAPL|2024-12-24 15:23:...|\n",
      "|2024-12-18 00:00:00|252.16|254.28|247.74|248.05| 56774100|      0.0|         0.0|  AAPL|2024-12-24 15:23:...|\n",
      "|2024-12-19 00:00:00| 247.5| 252.0|247.09|249.79| 60882300|      0.0|         0.0|  AAPL|2024-12-24 15:23:...|\n",
      "|2024-12-20 00:00:00|248.04| 255.0|245.69|254.49|147495300|      0.0|         0.0|  AAPL|2024-12-24 15:23:...|\n",
      "|2024-12-23 00:00:00|254.77|255.65|253.45|255.27| 40828600|      0.0|         0.0|  AAPL|2024-12-24 15:23:...|\n",
      "|2024-12-10 00:00:00|444.39|449.62| 441.6|443.33| 18469500|      0.0|         0.0|  MSFT|2024-12-24 15:23:...|\n",
      "|2024-12-11 00:00:00|444.05|450.35|444.05|448.99| 19200200|      0.0|         0.0|  MSFT|2024-12-24 15:23:...|\n",
      "|2024-12-12 00:00:00|449.11|456.16|449.11|449.56| 20834800|      0.0|         0.0|  MSFT|2024-12-24 15:23:...|\n",
      "|2024-12-13 00:00:00|448.44|451.43|445.58|447.27| 20177800|      0.0|         0.0|  MSFT|2024-12-24 15:23:...|\n",
      "|2024-12-16 00:00:00|447.27|452.18|445.28|451.59| 23598800|      0.0|         0.0|  MSFT|2024-12-24 15:23:...|\n",
      "|2024-12-17 00:00:00|451.01|455.29|449.57|454.46| 22733500|      0.0|         0.0|  MSFT|2024-12-24 15:23:...|\n",
      "|2024-12-18 00:00:00|451.32|452.65|437.02|437.39| 24444500|      0.0|         0.0|  MSFT|2024-12-24 15:23:...|\n",
      "|2024-12-19 00:00:00|441.62|443.18|436.32|437.03| 22963700|      0.0|         0.0|  MSFT|2024-12-24 15:23:...|\n",
      "|2024-12-20 00:00:00|433.11|443.74|428.63| 436.6| 64263700|      0.0|         0.0|  MSFT|2024-12-24 15:23:...|\n",
      "|2024-12-23 00:00:00|436.74|437.65|432.83|435.25| 19127000|      0.0|         0.0|  MSFT|2024-12-24 15:23:...|\n",
      "+-------------------+------+------+------+------+---------+---------+------------+------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# List of stock symbols and start dates\n",
    "yf_param_pairs = [\n",
    "    ('AAPL', '2024-12-10'),\n",
    "    ('MSFT', '2024-12-10'),\n",
    "    ('GOOGL', '2024-12-10'),\n",
    "]\n",
    "\n",
    "import_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S.\") + str(datetime.now().microsecond)[:3]\n",
    "zone='raw'\n",
    "sink_table='nessie.raw.stock_eod_yfinance'\n",
    "config_file_path='cfg_registered_table_schemas.yaml'\n",
    "\n",
    "table_manager=TableManager(config_file_path)\n",
    "registered_column_list = table_manager.get_column_list(sink_table)\n",
    "create_table_query = table_manager.get_create_table_query(sink_table)\n",
    "\n",
    "# Fetch data in parallel\n",
    "df_raw_eod_yfinance = parallel_fetch(yf_param_pairs, registered_column_list)\n",
    "# df_raw_eod_yfinance.show()\n",
    "\n",
    "# print(create_table_query)\n",
    "spark.sql(create_table_query)\n",
    "\n",
    "\n",
    "# df_raw_eod_yfinance.writeTo(sink_table).append()\n",
    "spark.sql(f\"select * from {sink_table}\").show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242a2f86-e3f0-40d6-9670-5b6fa2cd6280",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Poetry",
   "language": "python",
   "name": "poetry-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
