{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639ab335-35ed-4d00-9ce9-5d708b21672e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import nbimporter\n",
    "from datetime import datetime, date\n",
    "import time\n",
    "import random\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType,  DateType, TimestampType\n",
    "from pyspark.sql.functions import to_date, to_timestamp\n",
    "from lab_database_manager import PgDBManager\n",
    "from lab_spark import create_spark_session\n",
    "from lab_schema_manager import SchemaManager\n",
    "from lab_raw_yahoo import get_raw_yahooquery, get_raw_yfinance\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "import logging\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d1248e-f920-4fe2-85b6-110c03fd84d5",
   "metadata": {},
   "source": [
    "# Create Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320989cb-efd1-46ef-89b3-745acc98183f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Spark Session\n",
    "connection_config_file=\"cfg_connections.yaml\"\n",
    "spark_app_name=\"raw_yfinance\"\n",
    "spark=create_spark_session(connection_config_file, spark_app_name)\n",
    "\n",
    "# Set logging level to ERROR to reduce verbosity\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "# Optional: You can also adjust Python logging for third-party libraries\n",
    "logging.getLogger(\"py4j\").setLevel(logging.ERROR)\n",
    "spark.sql(\"CREATE NAMESPACE IF NOT EXISTS nessie.raw;\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53196d95-5457-4ce4-a0a4-af774cad7b7b",
   "metadata": {},
   "source": [
    "# Function: Load Iceberg Table Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cae414a-d7b5-41d3-af8f-861d61f07964",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_into_iceberg_table(schema_config_file, spark_source_df, iceberg_sink_table):\n",
    "    try: \n",
    "        schema_manager=SchemaManager(schema_config_file)\n",
    "        schema_struct_type=schema_manager.get_struct_type(\"tables\", iceberg_sink_table)  \n",
    "        \n",
    "        create_table_script = schema_manager.get_create_table_query(\"tables\", iceberg_sink_table)\n",
    "        spark.sql(create_table_script)\n",
    "     \n",
    "        spark_source_df.writeTo(iceberg_sink_table).append()\n",
    "        # source_spark_df.write.mode(\"overwrite\").saveAsTable(iceberg_sink_table) \n",
    "\n",
    "        incremental_count=spark_source_df.count()\n",
    "        total_count=spark.table(iceberg_sink_table).count()\n",
    "\n",
    "        print(f\"{iceberg_sink_table} was loaded with {incremental_count} records, totally {total_count} records.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading lceberg raw table: {e}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28104eba-f0d5-4cc6-a5a5-5a8615cf2359",
   "metadata": {},
   "source": [
    "# Function: Insert Data into PG Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22168854-440f-47aa-9be7-f0c13adef044",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_iceberg_data_into_pg(conn_config_file, iceberg_source_table, pg_database, pg_sink_table, is_pg_truncate_enabled, is_pg_merge_enabled):   \n",
    "    try:    \n",
    "        df_source=spark.read.table(iceberg_source_table)          \n",
    "\n",
    "        pg_db_mgr=PgDBManager(conn_config_file, pg_database)\n",
    "        pg_url=pg_db_mgr.jdbc_url\n",
    "        pg_driver=pg_db_mgr.driver\n",
    "\n",
    "        if is_pg_truncate_enabled == True:\n",
    "            pg_truncate_script=f\"TRUNCATE TABLE {pg_sink_table}\"\n",
    "            pg_db_mgr.execute_sql_script(pg_truncate_script)\n",
    "        \n",
    "        # Write DataFrame to PostgreSQL\n",
    "        df_source.write.jdbc(url=pg_url, table=pg_sink_table, mode=\"append\", properties={\"driver\": pg_driver}) \n",
    "\n",
    "        if is_pg_merge_enabled == True:\n",
    "            pg_merge_script = \"call fin.usp_load_stock_eod();\"\n",
    "            pg_db_mgr.execute_sql_script(pg_merge_script)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading pg finalytics: {e}\")       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc5b821-f483-4e5d-9766-becd93dd3dfb",
   "metadata": {},
   "source": [
    "# Truncate Iceberg Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f190b4f3-4dfd-491d-8a45-ec17875a98cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get iceberg table config info\n",
    "schema_config_file='cfg_schemas.yaml'\n",
    "iceberg_raw_stock_eod_table='nessie.raw.stock_eod_yahooquery'\n",
    "\n",
    "# Check if the Iceberg table exists and truncate it if it does\n",
    "if spark.catalog.tableExists(iceberg_raw_stock_eod_table):\n",
    "    spark.sql(f\"TRUNCATE TABLE {iceberg_raw_stock_eod_table}\")\n",
    "    print(f\"Iceberg table {iceberg_raw_stock_eod_table} truncated successfully.\")\n",
    "else:\n",
    "    print(f\"Iceberg table {iceberg_raw_stock_eod_table} does not exist.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1f1bda-a63e-4410-92b8-b3a8be099844",
   "metadata": {},
   "source": [
    "# Get Symbol Group Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123067ab-f994-4de4-9f63-6cce3afca0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get finalytics connetion info\n",
    "conn_config_file='cfg_connections.yaml'\n",
    "pg_db=\"finalytics\"\n",
    "pg_db_mgr=PgDBManager(conn_config_file, pg_db)\n",
    "# pg_url=pg_db_mgr.jdbc_url\n",
    "# pg_driver=pg_db_mgr.driver\n",
    "import_time = datetime.now()\n",
    "\n",
    "\n",
    "# Get symbol_start_date_pairs from finalytics\n",
    "query=\"SELECT group_id, group_start_date, symbol from fin.vw_etl_stock_eod_start_date_grouped  WHERE group_start_date <'2025-1-9' Limit 50;\"\n",
    "query_result=pg_db_mgr.get_sql_script_result_list(query)\n",
    "\n",
    "# Initialize a defaultdict to store the symbols for each (group_date, group_id)\n",
    "grouped_symbols = defaultdict(list)\n",
    "\n",
    "# Iterate over the data to group symbols by (group_date, group_id)\n",
    "for group_id, group_start_date, symbol in query_result:\n",
    "    # Use a tuple of (group_date, group_id) as the key and append the symbol to the list\n",
    "    grouped_symbols[(group_id, group_start_date)].append(symbol)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd33642-894e-486b-81a3-5461477d837c",
   "metadata": {},
   "source": [
    "# Get History Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f50bb6-b8ab-401c-b268-5567c62e71b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"yahooquery\")\n",
    "\n",
    "hist_data_frames=[]\n",
    "for group, group_symbols in grouped_symbols.items():\n",
    "    group_id, group_start_date = group\n",
    "    print(f\"Group Date: {group_start_date}, Group Number: {group_id}, Symbols: {group_symbols}\")\n",
    "    # hist_data=get_raw_yfinance(group_symbols, group_start_date, import_time)\n",
    "    hist_group_data_frame=get_raw_yahooquery(group_symbols, group_start_date, import_time)    \n",
    "    hist_data_frames.append(hist_group_data_frame)\n",
    "    time.sleep(5)\n",
    "combined_hist_data = pd.concat(hist_data_frames, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7c628a-867e-49e4-a490-83f704d8de57",
   "metadata": {},
   "source": [
    "# Load History Data into Iceberg Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0727461a-6c69-4ed0-b458-eceb2fc68022",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_df = spark.createDataFrame(combined_hist_data)    \n",
    "insert_into_iceberg_table(schema_config_file, hist_df, iceberg_raw_stock_eod_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a558a80-e6c2-497c-875e-69cc8a6921a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pg_table='stage.stock_eod_quote_yahoo'\n",
    "# is_pg_truncate_enabled=True\n",
    "# is_pg_merge_enabled=True\n",
    "# insert_iceberg_data_into_pg(conn_config_file, iceberg_raw_stock_eod_table, pg_db, pg_table, is_pg_truncate_enabled, is_pg_merge_enabled)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5edf5c-d96d-4299-8e32-5556cfb93c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.sql('select * from nessie.raw.stock_eod_yahooquery').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70235ec4-8a46-45be-ba7f-3bad304cdc3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Poetry",
   "language": "python",
   "name": "poetry-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
