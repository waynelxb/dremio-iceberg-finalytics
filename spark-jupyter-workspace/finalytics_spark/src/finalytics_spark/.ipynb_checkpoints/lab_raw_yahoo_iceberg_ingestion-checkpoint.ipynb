{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "639ab335-35ed-4d00-9ce9-5d708b21672e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import nbimporter\n",
    "from datetime import datetime, date\n",
    "import time\n",
    "import random\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType,  DateType, TimestampType\n",
    "from pyspark.sql.functions import to_date, to_timestamp\n",
    "from lab_database_manager import PgDBManager\n",
    "from lab_spark import create_spark_session\n",
    "from lab_schema_manager import SchemaManager\n",
    "from lab_raw_yahoo import get_raw_yahooquery, get_raw_yfinance\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "import logging\n",
    "import pandas as pd\n",
    "from lab_iceberg_manager import IcebergManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "363ae362-2a49-4cc1-bacb-e96a3ffabac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.postgresql#postgresql added as a dependency\n",
      "org.apache.iceberg#iceberg-spark-runtime-3.5_2.12 added as a dependency\n",
      "org.projectnessie.nessie-integrations#nessie-spark-extensions-3.5_2.12 added as a dependency\n",
      "software.amazon.awssdk#bundle added as a dependency\n",
      "software.amazon.awssdk#url-connection-client added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-70a20f2f-99c9-41f1-9b98-4b4c027b60d0;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.postgresql#postgresql;42.7.3 in central\n",
      "\tfound org.checkerframework#checker-qual;3.42.0 in central\n",
      "\tfound org.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.5.0 in central\n",
      "\tfound org.projectnessie.nessie-integrations#nessie-spark-extensions-3.5_2.12;0.77.1 in central\n",
      "\tfound software.amazon.awssdk#bundle;2.24.8 in central\n",
      "\tfound software.amazon.awssdk#url-connection-client;2.24.8 in central\n",
      "\tfound software.amazon.awssdk#utils;2.24.8 in central\n",
      "\tfound org.reactivestreams#reactive-streams;1.0.4 in central\n",
      "\tfound software.amazon.awssdk#annotations;2.24.8 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      "\tfound software.amazon.awssdk#http-client-spi;2.24.8 in central\n",
      "\tfound software.amazon.awssdk#metrics-spi;2.24.8 in central\n",
      ":: resolution report :: resolve 379ms :: artifacts dl 15ms\n",
      "\t:: modules in use:\n",
      "\torg.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.5.0 from central in [default]\n",
      "\torg.checkerframework#checker-qual;3.42.0 from central in [default]\n",
      "\torg.postgresql#postgresql;42.7.3 from central in [default]\n",
      "\torg.projectnessie.nessie-integrations#nessie-spark-extensions-3.5_2.12;0.77.1 from central in [default]\n",
      "\torg.reactivestreams#reactive-streams;1.0.4 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\tsoftware.amazon.awssdk#annotations;2.24.8 from central in [default]\n",
      "\tsoftware.amazon.awssdk#bundle;2.24.8 from central in [default]\n",
      "\tsoftware.amazon.awssdk#http-client-spi;2.24.8 from central in [default]\n",
      "\tsoftware.amazon.awssdk#metrics-spi;2.24.8 from central in [default]\n",
      "\tsoftware.amazon.awssdk#url-connection-client;2.24.8 from central in [default]\n",
      "\tsoftware.amazon.awssdk#utils;2.24.8 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   12  |   0   |   0   |   0   ||   12  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-70a20f2f-99c9-41f1-9b98-4b4c027b60d0\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 12 already retrieved (0kB/8ms)\n",
      "25/01/10 16:36:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/01/10 16:36:16 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/01/10 16:36:16 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "25/01/10 16:36:16 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\n",
      "SLF4J: Defaulting to no-operation (NOP) logger implementation\n",
      "SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iceberg table nessie.raw.stock_eod_yahooquery truncated successfully.\n"
     ]
    }
   ],
   "source": [
    "my_iceberg_manager=IcebergManager('cfg_connections.yaml', 'cfg_schemas.yaml', 'fin_spark_app') \n",
    "my_iceberg_manager.truncate_iceberg_table('nessie.raw.stock_eod_yahooquery')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d1248e-f920-4fe2-85b6-110c03fd84d5",
   "metadata": {},
   "source": [
    "# Create Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320989cb-efd1-46ef-89b3-745acc98183f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Spark Session\n",
    "connection_config_file=\"cfg_connections.yaml\"\n",
    "spark_app_name=\"raw_yfinance\"\n",
    "spark=create_spark_session(connection_config_file, spark_app_name)\n",
    "\n",
    "# Set logging level to ERROR to reduce verbosity\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "# Optional: You can also adjust Python logging for third-party libraries\n",
    "logging.getLogger(\"py4j\").setLevel(logging.ERROR)\n",
    "spark.sql(\"CREATE NAMESPACE IF NOT EXISTS nessie.raw;\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53196d95-5457-4ce4-a0a4-af774cad7b7b",
   "metadata": {},
   "source": [
    "# Function: Load Iceberg Table Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cae414a-d7b5-41d3-af8f-861d61f07964",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_into_iceberg_table(schema_config_file, spark_source_df, iceberg_sink_table):\n",
    "    try: \n",
    "        schema_manager=SchemaManager(schema_config_file)\n",
    "        schema_struct_type=schema_manager.get_struct_type(\"tables\", iceberg_sink_table)  \n",
    "        \n",
    "        create_table_script = schema_manager.get_create_table_query(\"tables\", iceberg_sink_table)\n",
    "        spark.sql(create_table_script)\n",
    "     \n",
    "        spark_source_df.writeTo(iceberg_sink_table).append()\n",
    "        # source_spark_df.write.mode(\"overwrite\").saveAsTable(iceberg_sink_table) \n",
    "\n",
    "        incremental_count=spark_source_df.count()\n",
    "        total_count=spark.table(iceberg_sink_table).count()\n",
    "\n",
    "        print(f\"{iceberg_sink_table} was loaded with {incremental_count} records, totally {total_count} records.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading lceberg raw table: {e}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28104eba-f0d5-4cc6-a5a5-5a8615cf2359",
   "metadata": {},
   "source": [
    "# Function: Insert Data into PG Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22168854-440f-47aa-9be7-f0c13adef044",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_iceberg_data_into_pg(conn_config_file, iceberg_source_table, pg_database, pg_sink_table, is_pg_truncate_enabled, is_pg_merge_enabled):   \n",
    "    try:    \n",
    "        df_source=spark.read.table(iceberg_source_table)          \n",
    "\n",
    "        pg_db_mgr=PgDBManager(conn_config_file, pg_database)\n",
    "        pg_url=pg_db_mgr.jdbc_url\n",
    "        pg_driver=pg_db_mgr.driver\n",
    "\n",
    "        if is_pg_truncate_enabled == True:\n",
    "            pg_truncate_script=f\"TRUNCATE TABLE {pg_sink_table}\"\n",
    "            pg_db_mgr.execute_sql_script(pg_truncate_script)\n",
    "        \n",
    "        # Write DataFrame to PostgreSQL\n",
    "        df_source.write.jdbc(url=pg_url, table=pg_sink_table, mode=\"append\", properties={\"driver\": pg_driver}) \n",
    "\n",
    "        if is_pg_merge_enabled == True:\n",
    "            pg_merge_script = \"call fin.usp_load_stock_eod();\"\n",
    "            pg_db_mgr.execute_sql_script(pg_merge_script)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading pg finalytics: {e}\")       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc5b821-f483-4e5d-9766-becd93dd3dfb",
   "metadata": {},
   "source": [
    "# Truncate Iceberg Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f190b4f3-4dfd-491d-8a45-ec17875a98cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get iceberg table config info\n",
    "schema_config_file='cfg_schemas.yaml'\n",
    "iceberg_raw_stock_eod_table='nessie.raw.stock_eod_yahooquery'\n",
    "\n",
    "# Check if the Iceberg table exists and truncate it if it does\n",
    "if spark.catalog.tableExists(iceberg_raw_stock_eod_table):\n",
    "    spark.sql(f\"TRUNCATE TABLE {iceberg_raw_stock_eod_table}\")\n",
    "    print(f\"Iceberg table {iceberg_raw_stock_eod_table} truncated successfully.\")\n",
    "else:\n",
    "    print(f\"Iceberg table {iceberg_raw_stock_eod_table} does not exist.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1f1bda-a63e-4410-92b8-b3a8be099844",
   "metadata": {},
   "source": [
    "# Get Symbol Group Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123067ab-f994-4de4-9f63-6cce3afca0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get finalytics connetion info\n",
    "conn_config_file='cfg_connections.yaml'\n",
    "pg_db=\"finalytics\"\n",
    "pg_db_mgr=PgDBManager(conn_config_file, pg_db)\n",
    "# pg_url=pg_db_mgr.jdbc_url\n",
    "# pg_driver=pg_db_mgr.driver\n",
    "import_time = datetime.now()\n",
    "\n",
    "\n",
    "# Get symbol_start_date_pairs from finalytics\n",
    "query=\"SELECT group_id, group_start_date, symbol from fin.vw_etl_stock_eod_start_date_grouped  WHERE group_start_date <'2025-1-9' Limit 50;\"\n",
    "query_result=pg_db_mgr.get_sql_script_result_list(query)\n",
    "\n",
    "# Initialize a defaultdict to store the symbols for each (group_date, group_id)\n",
    "grouped_symbols = defaultdict(list)\n",
    "\n",
    "# Iterate over the data to group symbols by (group_date, group_id)\n",
    "for group_id, group_start_date, symbol in query_result:\n",
    "    # Use a tuple of (group_date, group_id) as the key and append the symbol to the list\n",
    "    grouped_symbols[(group_id, group_start_date)].append(symbol)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd33642-894e-486b-81a3-5461477d837c",
   "metadata": {},
   "source": [
    "# Get History Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f50bb6-b8ab-401c-b268-5567c62e71b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"yahooquery\")\n",
    "\n",
    "hist_data_frames=[]\n",
    "for group, group_symbols in grouped_symbols.items():\n",
    "    group_id, group_start_date = group\n",
    "    print(f\"Group Date: {group_start_date}, Group Number: {group_id}, Symbols: {group_symbols}\")\n",
    "    # hist_data=get_raw_yfinance(group_symbols, group_start_date, import_time)\n",
    "    hist_group_data_frame=get_raw_yahooquery(group_symbols, group_start_date, import_time)    \n",
    "    hist_data_frames.append(hist_group_data_frame)\n",
    "    time.sleep(5)\n",
    "combined_hist_data = pd.concat(hist_data_frames, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7c628a-867e-49e4-a490-83f704d8de57",
   "metadata": {},
   "source": [
    "# Load History Data into Iceberg Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0727461a-6c69-4ed0-b458-eceb2fc68022",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_df = spark.createDataFrame(combined_hist_data)    \n",
    "insert_into_iceberg_table(schema_config_file, hist_df, iceberg_raw_stock_eod_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a558a80-e6c2-497c-875e-69cc8a6921a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pg_table='stage.stock_eod_quote_yahoo'\n",
    "# is_pg_truncate_enabled=True\n",
    "# is_pg_merge_enabled=True\n",
    "# insert_iceberg_data_into_pg(conn_config_file, iceberg_raw_stock_eod_table, pg_db, pg_table, is_pg_truncate_enabled, is_pg_merge_enabled)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5edf5c-d96d-4299-8e32-5556cfb93c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.sql('select * from nessie.raw.stock_eod_yahooquery').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70235ec4-8a46-45be-ba7f-3bad304cdc3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Poetry",
   "language": "python",
   "name": "poetry-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
